{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Apšvalka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) \n",
    "- **Date:** August 2024  \n",
    "- **conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts.\n",
    "\n",
    "**conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts.\n",
    "\n",
    "# fMRI Data Management\n",
    "\n",
    "Effective data management is essential in fMRI research, ensuring that datasets are well-organised, reproducible, and easily accessible for analysis and sharing. Standardised formats like BIDS not only streamlines workflows but also enhances collaboration and supports long-term data integrity.\n",
    "\n",
    "In this tutorial, we will learn how to convert fMRI data into BIDS format using [HeuDiConv](https://heudiconv.readthedocs.io/en/latest/index.html) and how to query the BIDS dataset using [PyBIDS](https://bids-standard.github.io/pybids/).\n",
    "\n",
    "Here are two recommended short videos to help you better understand the basics of fMRI terminology and data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"width: 1200px; margin: 0; display: flex; justify-content: space-around;\">\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>fMRI Data Structure & Terminology (7 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/OuRdQJMU5ro\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>Brain imaging data structure (11 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/5H6XaJLp2V8\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('''\n",
    "<div style=\"width: 1200px; margin: 0; display: flex; justify-content: space-around;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>fMRI Data Structure & Terminology (7 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/OuRdQJMU5ro\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>Brain imaging data structure (11 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/5H6XaJLp2V8\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "</div>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "**Table of contents**  \n",
    "1. Create a project folder  \n",
    "2. Retrieving the DICOM files    \n",
    "3. Brain Imaging Data Structure (BIDS)   \n",
    "4. HeuDiConv   \n",
    "4.1. Step 1: Discovering your scans    \n",
    "4.2. Step 2: Creating a heuristic file   \n",
    "4.3. Step 3: Converting the data   \n",
    "4.4. 'To Do' - additional information to check and add   \n",
    "5. Validate BIDS structure   \n",
    "6. PyBIDS\n",
    "6.1. Querying the BIDSLayout  \n",
    "6.2. Filtering files by entities   \n",
    "6.3. Filtering by metadata  \n",
    "6.4. Other `return_type` values   \n",
    "6.5. The `BIDSFile`    \n",
    "6.6. `.tsv` files  \n",
    "6.7. Filename parsing   \n",
    "6.8. Report generation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "## Create a project folder\n",
    "\n",
    "There is no universally agreed standard, but here is a recommended folder structure for your fMRI project.\n",
    "\n",
    "```bash\n",
    "# ======================================================================\n",
    "# Recommended directory structure for an fMRI project\n",
    "# ======================================================================\n",
    "\n",
    "# project_name                     \n",
    "#    └── code\n",
    "#        └── task\n",
    "#        └── preprocessing\n",
    "#        └── analysis\n",
    "#    └── data\n",
    "#    └── documents      # Protocols, reports, and other documentation\n",
    "#    └── results        # Analysis results, figures, and summary outputs\n",
    "#    └── scratch        # Temporary files and intermediate results\n",
    "#    └── logs           # Log files, error reports, and processing records\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create the folder structure manually or use a simple `command-line` command like this:\n",
    "```bash\n",
    "mkdir -p My_fMRI_study/{code/{task,preprocessing,analysis},data,documents,results,scratch,logs}\n",
    "```\n",
    "\n",
    "For this workshop, we have already created it for you as the `FaceProcessing` directory.\n",
    "\n",
    "Let's take a look at the directory's contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree FaceProcessing -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the DICOM files\n",
    "\n",
    "`DICOM` files are the raw imaging files generated by the MRI scanner. Typically, they are stored on an MRI data server. At the CBU, each imaging project is assigned a unique code. By knowing the project's code, we can locate the raw DICOM files on our server.\n",
    "\n",
    "For this tutorial, we have copied one subject's DICOM data from our server to the `mridata` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree mridata -L 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Series###` folder contains DICOM files of the particular scan. The name of the folder is the same as what a radiographer named the scan in the MRI console. Usually, the name is very indicative of what type of scan it is. In the example above, we acquired a T1w anatomical/structural scan (MPRAGE), nine functional scans (BOLD), and two field maps. The *Series_001_CBU_Localiser* scan is a positional scan for the MRI and can be ignored.\n",
    "\n",
    "Each of these folders contains DICOM (`.dcm`) files, typically, one file per slice for structural scans or one file per volume for functional scans. These files contain a header with the metadata (e.g., acquisition parameters) and the actual image itself. DICOM is a standard format for any medical image, not just the brain. To work with the brain images, we need to convert the DICOM files to NIfTI format which is a cross-platform and cross-software standard for brain images. Along with having the files in NIfTI format, we need to name and organise them according to BIDS standard.\n",
    "\n",
    "\n",
    "Each `Series###` folder contains the DICOM files for a particular scan. The folder name corresponds to what the radiographer named the scan in the MRI console. Typically, the name is indicative of the scan type. In the example above, we acquired a T1-weighted anatomical/structural scan (MPRAGE), nine functional scans (BOLD), and two field maps. The Series_001_CBU_Localiser scan is a positional scan for the MRI system and can be ignored.\n",
    "\n",
    "Each of these folders contains DICOM (.dcm) files, usually one file per slice for structural scans or one file per volume for functional scans. These files include a header with metadata (e.g., acquisition parameters) and the image itself. DICOM is a standard format for any medical image, not just brain scans. However, to work with brain images efficiently, we need to convert the DICOM files into NIfTI format—a cross-platform and cross-software standard for brain imaging. In addition to converting the files, we must name and organise them according to the BIDS standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain Imaging Data Structure (BIDS)\n",
    "\n",
    "(***For a more detailed tutorial on BODS conversion see: [https://github.com/MRC-CBU/BIDS_conversion/tree/main/MRI](https://github.com/MRC-CBU/BIDS_conversion/tree/main/MRI).***)\n",
    "\n",
    "To proceed with analysis, we need to convert the `DICOMs` to `NIfTI` format and then organise all these files in a 'nice' way.\n",
    "\n",
    "[Brain Imaging Data Structure (**BIDS**)](https://bids-specification.readthedocs.io/en/stable/) is a a standard for organizing and describing neuroimaging (and behavioural) datasets. See [BIDS paper](https://doi.org/10.1038/sdata.2016.44) and http://bids.neuroimaging.io website for more information.\n",
    "\n",
    "How to get your DICOMs into NIfTI and into BIDS?\n",
    "\n",
    "Several tools exist (see a full list [here](https://bids.neuroimaging.io/benefits#converters)). We will use [HeuDiConv](https://joss.theoj.org/papers/10.21105/joss.05839). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HeuDiConv\n",
    "\n",
    "HeudiConv is a comand-line tool. To use it, you would either install heudiconv and dcm2niix packages locally: \n",
    "```\n",
    "pip install heudiconv dcm2niix\n",
    "\n",
    "```\n",
    "\n",
    "or use Docker (or Apptainer/Singularity) container image\n",
    "```\n",
    "docker pull nipy/heudiconv\n",
    "```\n",
    "\n",
    "Here we have `heudiconv` included in the `mri` conda environment that we are using for this workshop. \n",
    "\n",
    "HeuDiConv does the following:\n",
    "\n",
    "- converts DICOM (.dcm) files to NIfTI format (`.nii` or `.nii.gz`);\n",
    "- generates their corresponding metadata files (`.json`);\n",
    "- renames the files and organises them in folders following BIDS specification;\n",
    "- generates several other `.json` and `.tsv` files required by BIDS.\n",
    "\n",
    "The final result of DICOM Series being converted into BIDS for our example subject above would be this:\n",
    "\n",
    "![dicom_bids.png](attachment:3d6750f6-d21c-4f68-9111-48603b4c7563.png)\n",
    "\n",
    "*(Why are we working with sub-04 instead of sub-01? The first three subjects in this study did not consent to having their raw data made public.)*\n",
    "\n",
    "HeuDiConv needs information on how to translate your specific DICOMs into BIDS. This information is provided in a [heuristic file](https://heudiconv.readthedocs.io/en/latest/heuristics.html) that the user creates.\n",
    "\n",
    "To create the heuristic file, you need to know what scans you have, which ones you want to convert (you don't have to convert all scans, only the ones you need for your project), and how to uniquely identify each scan based on its metadata.\n",
    "\n",
    "As such, **converting DICOM data to BIDS using HeuDiConv involves 3 main steps**:\n",
    "\n",
    " 1. Discovering what DICOM series (scans) there are in your data\n",
    " 2. Creating a heuristic file specifying how to translate the DICOMs into BIDS\n",
    " 3. Converting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Discovering your scans\n",
    "\n",
    "First, you need to know what scans are available and how to uniquely identify them using their metadata. While you could manually check the metadata of each DICOM file, this is not very convenient. Instead, you can 'ask' HeuDiConv to perform the scan discovery for you. By running HeuDiConv without specifying NIfTI conversion or a heuristic file, it will generate a DICOM info table that lists all scans and their metadata. It will look something like this:\n",
    "\n",
    "![dicom_info.png](attachment:12f790cf-9e68-42e6-a5df-47a237734eea.png)\n",
    "\n",
    "The column names are metadata fields and rows contain their corresponding values.\n",
    "\n",
    "To generate such a table, you would run the following command in the terminal:\n",
    "\n",
    "`heudiconv --files mridata/CBU090928_MR09029/*/*/*.dcm --outdir FaceProcessing/scratch/dicom_discovery --heuristic convertall --subjects '04' --converter none --bids --overwrite`\n",
    "\n",
    "However, this approach is error-prone and not easily reproducible. Instead, we recommend writing a simple bash script like this [code-examples/step01_dicom_discover.sh](code-examples/step01_dicom_discover.sh) \n",
    "\n",
    "**Example script:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Path to the raw DICOM files\n",
    "DICOM_PATH='/home/cognestic/COGNESTIC/05_fMRI/mridata/CBU090928_MR09029'\n",
    "\n",
    "# Location of the output data (it will be created if it doesn't exist)\n",
    "OUTPUT_PATH='/home/cognestic/COGNESTIC/05_fMRI/FaceProcessing/scratch/dicom_discovery'\n",
    "\n",
    "# Subject ID\n",
    "SUBJECT_ID='04'\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Activate the mri environment (or any other environment with heudiconv installed)\n",
    "# ------------------------------------------------------------\n",
    "#conda activate mri # This doesn't work on all systems. Then you need to activate the environment manually.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run the heudiconv\n",
    "# ------------------------------------------------------------\n",
    "heudiconv \\\n",
    "    --files \"${DICOM_PATH}\"/*/*/*.dcm \\\n",
    "    --outdir \"${OUTPUT_PATH}\" \\\n",
    "    --heuristic convertall \\\n",
    "    --subjects \"${SUBJECT_ID}\" \\\n",
    "    --converter none \\\n",
    "    --bids \\\n",
    "    --overwrite\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Deactivate the conda environment\n",
    "# conda deactivate\n",
    "\n",
    "cp \"${OUTPUT_PATH}\"/.heudiconv/\"${SUBJECT_ID}\"/info/dicominfo.tsv \"${OUTPUT_PATH}\"\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# HeudiConv parameters:\n",
    "# --files: Files or directories containing files to process\n",
    "# --outdir: Output directory\n",
    "# --heuristic: Name of a known heuristic or path to the Python script containing heuristic\n",
    "# --subjects: Subject ID\n",
    "# --converter : dicom to nii converter (dcm2niix or none)\n",
    "# --bids: Flag for output into BIDS structure\n",
    "# --overwrite: Flag to overwrite existing files\n",
    "# \n",
    "# For a full list of parameters, see: https://heudiconv.readthedocs.io/en/latest/usage.html \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this script to discover which DICOMs our example subject has.\n",
    "\n",
    "Normally, we would execute bash scripts from the terminal. However, we can also run command-line commands directly within the notebook by adding an '!' sign at the beginning. After running it, it's a good idea to comment it out to avoid accidentally running it again when it's not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !./code-examples/step01_dicom_discover.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the script, the table that we are interested in will be located at *`OUTPUT_PATH/.heudiconv/[subject ID]/info/dicominfo.tsv`*. The .heudiconv directory is a hidden directory and you might not be able to see it in your file system unless you copy it to an unhidden directory which we in fact did (notice the last line in the script). \n",
    "\n",
    "\n",
    "Now, you can open the file and keep it open for the next step - creating a heuristic file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Creating a heuristic file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heuristic file must be a `Python` file. You can create and edit Python files in any text editor. You can name the file anything you want. For example, `bids_heuristic.py` like we have named our file which is available here: [code-examples/bids_heuristic.py](code-examples/bids_heuristic.py).\n",
    "\n",
    "We need to specify the conversion template for each DICOM series. The template can be anything you want. In this particular case, we want it to be in BIDS format. Therefore for each of our scan types, we need to consult BIDS specification. A good starting point is to look at the BIDS starter kit [folders](https://bids-standard.github.io/bids-starter-kit/folders_and_files/folders.html) and [filenames](https://bids-standard.github.io/bids-starter-kit/folders_and_files/files.html). (See the full BIDS specification for MRI [here](https://bids-specification.readthedocs.io/en/stable/modality-specific-files/magnetic-resonance-imaging-data.html).)\n",
    "\n",
    "Following these guidelines, we define the template (folder structure and filenames) for our anatomical, fieldmap, and functional scans. (We don't need to worry about the BIDS-required metadata, HeuDiConv will generate the required metadata for us.)\n",
    "\n",
    "```python\n",
    "    # The structural/anatomical scan\n",
    "    anat = create_key('sub-{subject}/anat/sub-{subject}_T1w')\n",
    "    \n",
    "    # The fieldmap scans\n",
    "    fmap_mag = create_key('sub-{subject}/fmap/sub-{subject}_acq-func_magnitude')\n",
    "    fmap_phase = create_key('sub-{subject}/fmap/sub-{subject}_acq-func_phasediff')\n",
    "    \n",
    "    # The functional scans\n",
    "    # You need to specify the task name in the filename. It must be a single string of letters WITHOUT spaces, underscores, or dashes!\n",
    "    func_task = create_key('sub-{subject}/func/sub-{subject}_task-faceprocessing_run-{item:02d}_bold')\n",
    "```\n",
    "\n",
    "A couple of important points to note:\n",
    "\n",
    "- For the fielmaps, we add a key-value pair `acq-func` to specify that these fieldmaps are intended for the functional images - to correct the functional images for susceptibility distortions. You will see how this key-value is relevant later.\n",
    "\n",
    "- For the functional scans, you have to specify the task name. In our case, the task was *Face Processing*. The task name must be a **single string** of letters without spaces, underscores, or dashes!\n",
    "\n",
    "- In this example project, we had multiple functional scans with identical parameters and task names. They correspond to multiple runs of our task. Therefore we add a `run` key and its value will be added automatically as a 2-digit integer (*run-01*, *run-02* etc.).\n",
    "\n",
    "Next, we create a dictionary `info` with our template names as keys and empty lists for their values. The values will be filled in in the next step with the `for loop`.\n",
    "\n",
    "```python\n",
    "    info = {\n",
    "        anat: [],\n",
    "        fmap_mag: [],\n",
    "        fmap_phase: [],\n",
    "        func_task: []\n",
    "        }\n",
    "```\n",
    "The `for loop` loops through all our DICOM series that are found in the `RAW_PATH` that we specify in our HeuDiConv parameters. Here we need to specify the criteria for associating DICOM series with their respective outputs. Now you need to look at the *dicominfo.tsv* table that we generated in the previous step.\n",
    "\n",
    "- We only have one DICOM series with *MPRAGE* in its protocol name. Therefore for the anatomical scan, we don't need to specify any additional cireteria.\n",
    "\n",
    "- We have two fieldmaps. One of them should be a *magnitude* image and the other one a *phase* image. Both have *FieldMapping* in their protocol name therefore we need to define additional criteria to distinguish between them. They differ in the `dim3` parameter. If in doubt, ask your radiographer, but typically, the *magnitude* image has more slices (higher `dim3`).\n",
    "\n",
    "- Our 9 functional scans are the only ones with more than one volume (`dim4`). They all are different runs of the same task, therefore we don't need any other distinguishing criteria for them, just the `dim4`. We specify here `dim4 > 100` because it could happen that not all participants and not all runs had exactly 210 volumes collected. In addition, sometimes it happens that a run is cancelled due to some problem and we want to discard any run with less than 100 volumes.\n",
    "\n",
    "```python\n",
    "# Loop through all the DICOM series and assign them to the appropriate conversion key.\n",
    "    for s in seqinfo:\n",
    "        # Uniquelly identify each series\n",
    "        \n",
    "        # Structural\n",
    "        if \"MPRAGE\" in s.series_id:\n",
    "            info[anat].append(s.series_id)\n",
    "            \n",
    "        # Field map Magnitude (the fieldmap with the largest dim3 is the magnitude, the other is the phase)\n",
    "        if 'FieldMapping' in s.series_id and s.series_files == 66:\n",
    "            info[fmap_mag].append(s.series_id)\n",
    "            \n",
    "        # Field map PhaseDiff\n",
    "        if 'FieldMapping' in s.series_id and s.series_files == 33:\n",
    "            info[fmap_phase].append(s.series_id)\n",
    "\n",
    "        # Functional Bold\n",
    "        if s.dim4 > 100:\n",
    "           info[func_task].append(s.series_id)\n",
    "            \n",
    "    # Return the dictionary\n",
    "    return info\n",
    "```\n",
    "Finally, we add `POPULATE_INTENDED_FOR_OPTS` dictionary to our heuristic file. This will automatically add an `IntendedFor` field in the fieldmap .json files telling for which images the fieldmaps should be used to correct for susceptibility distortion. Typically, these are the functional images. There are several options how you can specify for which images the fieldmaps are intended for. See full information [here](https://heudiconv.readthedocs.io/en/latest/heuristics.html#populate-intended-for-opts). In our example, we specify to look for a `modality acquisition label`. It checks for what modality (e.g., *anat*, *func*, *dwi*) each fieldmap is intended by checking the `acq-` label in the fieldmap filename and finding corresponding modalities. For example, `acq-func` will be matched with the `func` modality. That's why when defining the template for the `fmap_mag` and `fmap_phase` we added `acq-func` in their filenames.\n",
    "\n",
    "```python\n",
    "POPULATE_INTENDED_FOR_OPTS = {\n",
    "    'matching_parameters': ['ModalityAcquisitionLabel'],\n",
    "    'criterion': 'Closest'\n",
    "}\n",
    "```\n",
    "\n",
    "Other, more advanced functions can be added to the heuristic file as well, such as input file filtering. For full information, see the [documentation](https://heudiconv.readthedocs.io/en/latest/heuristics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Converting the data\n",
    "\n",
    "Once our heuristic file is created, we are ready to convert our raw DICOM data to BIDS.\n",
    "\n",
    "To do that, we need to make three changes in our `code-examples/step01_dicom_discover.sh` bash script that we wrote in the first step. We named the edited script [code-examples/step02_dicom_to_bids.sh](code-examples/step02_dicom_to_bids.sh).\n",
    "\n",
    "- We need to change the output path because we don't want to put this result in the *scratch* directory but in the final *data* directory\n",
    "\n",
    "- We need to change the `--heuristic` parameter specifying the path to our newly created heuristic file\n",
    "\n",
    "- We need to change the `--converter` parameter to `dcm2niix` because now we want the DICOM files to be converted to NIfTI format.\n",
    "\n",
    "Once these changes are saved, you can run the script from the terminal:\n",
    "\n",
    "\n",
    "`./code-examples/step02_dicom_to_bids.sh`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!./code-examples/step02_dicom_to_bids.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'To Do' - additional information to check and add\n",
    "\n",
    "Once you have converted the DICOMs to BIDS, there are some details you'll need to fill in yourself to ensure the dataset is fully BIDS-compliant. HeuDiConv marks any 'missing' information with `'To Do'` placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Events\n",
    "\n",
    "HeuDiConv generates the `events.tsv` file for each functional run. The files are just a template that you need to fill with the actual data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "events_file_empty = pd.read_csv('FaceProcessing/data/sub-04/func/sub-04_task-faceprocessing_run-01_events.tsv', sep='\\t')\n",
    "events_file_empty .head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For functional images, if they are not resting-state scans but involve participants performing a task, you will need to provide details such as trial type, onset, and duration. This data should come from the outputs of your experimental script, so make sure your task is programmed in a way that allows you to easily retrieve the necessary trial and timing information.\n",
    "\n",
    "**For our example subject, we have prepared the final events files, and you will need to copy them into the subject's 'func' folder.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the prepared events\n",
    "!rm -f FaceProcessing/data/sub-04/func/sub-04_task-faceprocessing_run-*.tsv\n",
    "!cp -r sub-04_task-files/* FaceProcessing/data/sub-04/func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_file_filled = pd.read_csv('FaceProcessing/data/sub-04/func/sub-04_task-faceprocessing_run-01_events.tsv', sep='\\t')\n",
    "\n",
    "events_file_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested how we obtained the events information from the OpenNeuro dataset, have a look at these two scripts: [code-examples/step03_events_to_bids.py](code-examples/step03_events_to_bids.py) and [code-examples/step04_transform_events.py](code-examples/step04_transform_events.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset description\n",
    "\n",
    "`dataset_description.json`\n",
    "\n",
    "A brief description of your dataset. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Acknowledgements\": \"TODO: whom you want to acknowledge\",\n",
    "  \"Authors\": [\n",
    "    \"TODO:\",\n",
    "    \"First1 Last1\",\n",
    "    \"First2 Last2\",\n",
    "    \"...\"\n",
    "  ],\n",
    "  \"BIDSVersion\": \"1.8.0\",\n",
    "  \"DatasetDOI\": \"TODO: eventually a DOI for the dataset\",\n",
    "  \"Funding\": [\n",
    "    \"TODO\",\n",
    "    \"GRANT #1\",\n",
    "    \"GRANT #2\"\n",
    "  ],\n",
    "  \"HowToAcknowledge\": \"TODO: describe how to acknowledge -- either cite a corresponding paper, or just in acknowledgement section\",\n",
    "  \"License\": \"TODO: choose a license, e.g. PDDL (http://opendatacommons.org/licenses/pddl/)\",\n",
    "  \"Name\": \"TODO: name of the dataset\",\n",
    "  \"ReferencesAndLinks\": [\n",
    "    \"TODO\",\n",
    "    \"List of papers or websites\"\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participants\n",
    "\n",
    "`participants.json`\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"participant_id\": {\n",
    "    \"Description\": \"Participant identifier\"\n",
    "  },\n",
    "  \"age\": {\n",
    "    \"Description\": \"Age in years (TODO - verify) as in the initial session, might not be correct for other sessions\"\n",
    "  },\n",
    "  \"sex\": {\n",
    "    \"Description\": \"self-rated by participant, M for male/F for female (TODO: verify)\"\n",
    "  },\n",
    "  \"group\": {\n",
    "    \"Description\": \"(TODO: adjust - by default everyone is in control group)\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task information\n",
    "\n",
    "`task-facerecognition_bold.json`\n",
    "\n",
    "Could add full task name and a Cognitive Atlas ID if it exists. \n",
    "```json\n",
    "{\n",
    " ...\n",
    "  \"TaskName\": \"TODO: full task name for facerecognition\",\n",
    " ...\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### README\n",
    "\n",
    "*\"TODO: Provide description for the dataset -- basic details about the study, possibly pointing to pre-registration (if public or embargoed)\"*\n",
    "\n",
    "See an example for the OpenNeuro version of this dataset https://openneuro.org/datasets/ds000117/versions/1.0.5/file-display/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate BIDS structure\n",
    "\n",
    "Once we have our BIDS dataset, we can use an [online BIDS validator](https://bids-standard.github.io/bids-validator/) to check if our dataset confirms with BIDS standard and what additional information we might need to include in your dataset's metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyBIDS\n",
    "\n",
    "`PyBids` is a Python module to interface with datasets conforming BIDS. See the [documentation](https://bids-standard.github.io/pybids/) and [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7409983/) for more info. \n",
    "\n",
    "**Let's explore some of the functionality of pybids.layout.** The material is adapted from https://github.com/bids-standard/pybids/tree/master/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bids.layout import BIDSLayout\n",
    "\n",
    "fmri_data_dir = 'FaceProcessing/data'\n",
    "\n",
    "# Initialize the layout\n",
    "layout = BIDSLayout(fmri_data_dir)\n",
    "\n",
    "# Print some basic information about the layout\n",
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the BIDSLayout\n",
    "The main method for querying `BIDSLayout` is `.get()`.\n",
    "\n",
    "If we call `.get()` with no additional arguments, we get back a list of all the BIDS files in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = layout.get()\n",
    "print(\"There are {} files in the layout.\".format(len(all_files)))\n",
    "print(\"\\nThe first 5 files are:\")\n",
    "all_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned object is a list of `BIDSFile` objects. \n",
    "\n",
    "We can also get just filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get(return_type='filename')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get such information as\n",
    "* all `subject` IDs\n",
    "* all `task` names\n",
    "* dataset `description`\n",
    "* the BOLD repetition time TR\n",
    "* how many `runs` there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_dataset_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_tr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding runs, it might be that there are varied number of runs accross participants. So, let's get runs for each participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sID in layout.get_subjects(): \n",
    "    print(layout.get_runs(subject = sID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering files by entities\n",
    "We can pass any BIDS-defined entities (keywords) to `.get()` method. For example, here's how we would retrieve all BOLD runs with `.nii.gz` extensions for subject `04`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve filenames of all BOLD runs for subject\n",
    "layout.get(subject='04', extension='nii.gz', suffix='bold', return_type='filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the entities are found in the names of BIDS files. For example `sub-01_task-facerecognition_run-01_bold.nii.gz` has entities: **subject**, **task**, **run**, **suffix**, **extension**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the list of all availabe entities by `layout.get_entities()`.\n",
    "\n",
    "Here are a few of the most common entities:\n",
    "\n",
    "* `suffix`: The part of a BIDS filename just before the extension (e.g., 'bold', 'events', 'T1w', etc.).\n",
    "* `subject`: The subject label\n",
    "* `session`: The session label\n",
    "* `run`: The run index\n",
    "* `task`: The task name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by metadata\n",
    "Sometimes we want to search for files not just by their names, but also based on metadata defined in JSON files. We can use any key that appears in a JSON file in our project as an argument to `.get()`. This can be combined with any number of core BIDS entities (like *subject*, *run*, etc.).\n",
    "\n",
    "For example, let's retrieve the subject's `SpacingBetweenSlices` (measured from the center of each slice to the next, in mm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_SpacingBetweenSlices(subject='04', suffix='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==================================================================================================**\n",
    "\n",
    "**EXCERCISE**\n",
    "\n",
    "We want to know the time of the day when the subject was scanned. The scanning started with T1 images, so we want to retrieve the `AcquisitionTime` of subject's `T1w` image. Adapt the script above to acquire this information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other `return_type` values\n",
    "We can also ask `get()` to return unique values (or IDs) of particular entities. For example, we want to know which subjects had a fieldmap acquired. We can request that information by setting `return_type='id'` - to get subject IDs. When using this option, we also need to specify a `target` entity for the ID (in this case, subject). This combination tells the `BIDSLayout` to return the unique values for the specified `target` entity. \n",
    "\n",
    "For example, in the next example, we ask for all of the unique subject IDs that have at least one file with a `phasediff` (fieldmap) suffix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask get() to return the ids of subjects that have phasediff (fieldmap_ files)\n",
    "\n",
    "layout.get(return_type='id', target='subject', suffix='phasediff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our `target` is a BIDS entity that corresponds to a particular directory in the BIDS specification (e.g., `subject` or `session`) we can also use `return_type='dir'` to get all matching subdirectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get(return_type='dir', target='subject')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `BIDSFile`\n",
    "When you call `.get()` on a `BIDSLayout`, the default returned values are objects of class `BIDSFile`. A `BIDSFile` is a lightweight container for individual files in a BIDS dataset. It provides easy access to a variety of useful attributes and methods. Let's take a closer look. First, let's pick a random file from our existing `layout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the 7th file in the dataset\n",
    "bf = layout.get(subject='04', extension='nii.gz', suffix='bold')[0]\n",
    "# Print it\n",
    "bf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes and methods available to us in a `BIDSFile` (note that some of these are only available for certain subclasses of `BIDSFile`; e.g., you can't call `get_image()` on a `BIDSFile` that doesn't correspond to an image file!):\n",
    "* `.path`: The full path of the associated file\n",
    "* `.filename`: The associated file's filename (without directory)\n",
    "* `.dirname`: The directory containing the file\n",
    "* `.get_entities()`: Returns information about entities associated with this `BIDSFile` (optionally including metadata)\n",
    "* `.get_image()`: Returns the file contents as a nibabel image (only works for image files)\n",
    "* `.get_df()`: Get file contents as a pandas DataFrame (only works for TSV files)\n",
    "* `.get_metadata()`: Returns a dictionary of all metadata found in associated JSON files\n",
    "* `.get_associations()`: Returns a list of all files associated with this one in some way\n",
    "\n",
    "Let's see some of these in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the entities associated with this file, and their values\n",
    "bf.get_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 30 metadata items associated with this file\n",
    "file_metadata = bf.get_metadata()\n",
    "\n",
    "{k: file_metadata[k] for k in list(file_metadata)[:30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.get_image()`: Returns the file contents as a `nibabel` image (only works for image files). We can then display the image, for example, using `OrthoSlicer3D`.   \n",
    "\n",
    "**Note:** When using `orthoview()` in notebook, don't forget to close figures afterward again or use %matplotlib inline again, otherwise, you cannot plot any other figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "bf.get_image().orthoview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.tsv` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases where a file has a `.tsv.gz` or `.tsv` extension, it will automatically be created as a `BIDSDataFile`, and we can easily grab the contents as a `DataFrame`.\n",
    "\n",
    "Let's look at the first `events` file from our layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first events file\n",
    "evfile = layout.get(suffix='events')[0]\n",
    "\n",
    "# Get contents as a DataFrame and show the first few rows\n",
    "df = evfile.get_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `participants` information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = layout.get(suffix='participants', extension='tsv')[0]\n",
    "\n",
    "df = participants.get_df()\n",
    "df.sort_values(by=['participant_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filename parsing\n",
    "Let’s say you have a filename and want to manually extract BIDS entities from it. . The `parse_file_entities` method provides the facility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.parse_file_entities('some_path_to_bids_file/sub-04_task-facerecognition_run-01_bold.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same for `BIDSFile` object that we defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.parse_file_entities(bf.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.parse_file_entities(bf.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report generation\n",
    "`PyBIDS` also allows you to automatically create data acquisition reports based on the available `image` and `meta-data` information. This enables a new level of standardisation and transparency. FAIR-ness, meta-analyses, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the BIDSReport function from the reports submodule\n",
    "from bids.reports import BIDSReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only need to apply the `BIDSReport` function to our `layout` and generate our report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a report for the dataset\n",
    "report = BIDSReport(layout)\n",
    "\n",
    "# Method generate returns a Counter of unique descriptions across subjects\n",
    "try:\n",
    "    descriptions = report.generate()\n",
    "    pub_description = descriptions.most_common()[0][0]\n",
    "    print(pub_description)\n",
    "except IndexError:\n",
    "    print('Sorry, it seems that the dataset is not complete and report cannot be generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
