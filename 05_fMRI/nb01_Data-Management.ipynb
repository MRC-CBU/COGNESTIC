{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Apšvalka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) \n",
    "- **Date:** August 2024  \n",
    "- **conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/c0dc3faa699e19187d5d5a8fb491a66baa27b9fb/mri_environment.yml) to run this notebook and any accompanied scripts.\n",
    "\n",
    "**conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts.\n",
    "\n",
    "# fMRI Data Management\n",
    "* Brief overview of the importance of data management in fMRI research.\n",
    "* Objectives of the notebook (e.g., understanding data organization, file formats, metadata, and version control).\n",
    "* Outline of the topics covered.\n",
    "\n",
    "test changes\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [File types and formats](#toc1_)    \n",
    "2. [Create a project folder](#toc2_)    \n",
    "3. [Retrieving the DICOM files](#toc3_)    \n",
    "4. [Brain Imaging Data Structure (BIDS)](#toc4_)    \n",
    "5. [HeuDiConv](#toc5_)    \n",
    "5.1. [Step 1: Discovering your scans](#toc5_1_)    \n",
    "5.2. [Step 2: Creating a heuristic file](#toc5_2_)    \n",
    "5.3. [Step 3: Converting the data](#toc5_3_)    \n",
    "5.4. ['To Do' - additional information to check and add](#toc5_4_)    \n",
    "6. [Validate BIDS structure](#toc6_)    \n",
    "7. [PyBIDS](#toc7_)    \n",
    "7.1. [Querying the BIDSLayout](#toc7_1_)    \n",
    "7.2. [Filtering files by entities](#toc7_2_)    \n",
    "7.3. [Filtering by metadata](#toc7_3_)    \n",
    "7.4. [Other `return_type` values](#toc7_4_)    \n",
    "7.5. [The `BIDSFile`](#toc7_5_)    \n",
    "7.6. [`.tsv` files](#toc7_6_)    \n",
    "7.7. [Filename parsing](#toc7_7_)    \n",
    "7.8. [Report generation](#toc7_8_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=2\n",
    "\tmaxLevel=3\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "## 1. <a id='toc1_'></a>[File types and formats](#toc0_)\n",
    "\n",
    "Brain images:\n",
    "* DICOM\n",
    "* NifTI\n",
    "\n",
    "Task data - stimuli onset times and duration\n",
    "\n",
    "Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a id='toc2_'></a>[Create a project folder](#toc0_)\n",
    "\n",
    "Here is a recommended folder structure for your fMRI project. \n",
    "\n",
    "```bash\n",
    "# ======================================================================\n",
    "# Recommended directory structure for an fMRI project\n",
    "# ======================================================================\n",
    "\n",
    "# project_name                     \n",
    "#    └── code\n",
    "#        └── task\n",
    "#        └── preprocessing\n",
    "#        └── analysis\n",
    "#    └── data\n",
    "#    └── documents      # Protocols, reports, and other documentation\n",
    "#    └── results        # Analysis results, figures, and summary outputs\n",
    "#    └── scratch        # Temporary files and intermediate results\n",
    "#    └── logs           # Log files, error reports, and processing records\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You can create it manually or use a simple `command line` command: \n",
    "```bash\n",
    "mkdir -p My_fMRI_study/{code/{task,preprocessing,analysis},data,documents,results,scratch,logs}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a id='toc3_'></a>[Retrieving the DICOM files](#toc0_)\n",
    "\n",
    "`DICOM` files are the raw imaging files that come from the MRI scanner. Usually they are stored on some MRI data server. At the CBU, each imaging project has a unique code. Knowing my project's code, I can locate the raw `DICOM` files on our server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -d mridata/*_MR09029/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a id='toc4_'></a>[Brain Imaging Data Structure (BIDS)](#toc0_)\n",
    "\n",
    "***!For a more detailed tutorial see: [https://github.com/MRC-CBU/BIDS_conversion/tree/main/MRI](https://github.com/MRC-CBU/BIDS_conversion/tree/main/MRI).***\n",
    "\n",
    "To proceed with analysis, we need to convert the `DICOMs` to `NIfTI` format and then organise all these files in a 'nice' way.\n",
    "\n",
    "[Brain Imaging Data Structure (**BIDS**)](https://bids-specification.readthedocs.io/en/stable/) is a a standard for organizing and describing neuroimaging (and behavioural) datasets. See [BIDS paper](https://doi.org/10.1038/sdata.2016.44) and http://bids.neuroimaging.io website for more information.\n",
    "\n",
    "How to get your DICOMs into NIfTI and into BIDS?\n",
    "\n",
    "Several tools exist (see a full list [here](https://bids.neuroimaging.io/benefits#converters)). I will here demonstrate a `Python`-based converter [HeuDiConv](https://heudiconv.readthedocs.io/en/latest/index.html). \n",
    "\n",
    "`heudiconv` is a flexible `DICOM` converter for organizing brain imaging data into structured directory layouts.\n",
    "* It allows flexible directory layouts and naming schemes through customizable heuristics implementations\n",
    "* It only converts the necessary DICOMs, not everything in a directory\n",
    "* You can keep links to DICOM files in the participant layout\n",
    "* Using `dcm2niix` under the hood, it’s fast\n",
    "* It provides assistance in converting to `BIDS`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id='toc5_'></a>[HeuDiConv](#toc0_)\n",
    "\n",
    "HeudiConv is a comand lime tool. To use it, you would either install heudiconv and dcm2niix packages locally: \n",
    "```\n",
    "pip install heudiconv dcm2niix\n",
    "\n",
    "```\n",
    "\n",
    "or use Docker (or Apptainer/Singularity) container image\n",
    "```\n",
    "docker pull nipy/heudiconv\n",
    "```\n",
    "\n",
    "`heidiconv` involves 3 main steps:\n",
    "1. Discovering what DICOM series (scans) there are in your data\n",
    "2. Creating a heuristic file specifying how to translate the DICOMs into BIDS\n",
    "3. Converting the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. <a id='toc5_1_'></a>[Step 1: Discovering your scans](#toc0_)\n",
    "\n",
    "First, you need to know what scans there are and how to uniquely identify them by their metadata. You could look in each scan's DICOM file metadata manually yourself, but that's not very convenient. Instead, you can 'ask' HeuDiConv to do the scan discovery for you. If you run HeuDiConv without NIfTI conversion and heuristic, it will generate a DICOM info table with all scans and their metadata. Like this: \n",
    "\n",
    "<img align=\"left\" padding = \"16px;\" src=\"dicom_info.png\">\n",
    "<br clear=\"left\"/>\n",
    "\n",
    "The column names are metadata fields and rows contain their corresponding values.\n",
    "\n",
    "**Example script:**\n",
    "To get such a table, you'd write a simple bash script, like this: [code-examples/step01_dicom_discover.sh](code-examples/step01_dicom_discover.sh) script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Path to the raw DICOM files\n",
    "DICOM_PATH='mridata/CBU090928_MR09029'\n",
    "\n",
    "# Location of the output data (it will be created if it doesn't exist)\n",
    "OUTPUT_PATH=\"FaceProcessing/scratch/dicom_discovery\"\n",
    "\n",
    "# Subject ID\n",
    "SUBJECT_ID='04'\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Activate the mri environment (or any other environment with heudiconv installed)\n",
    "# ------------------------------------------------------------\n",
    "conda activate mri\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run the heudiconv\n",
    "# ------------------------------------------------------------\n",
    "heudiconv \\\n",
    "    --files \"${DICOM_PATH}\"/*/*/*.dcm \\\n",
    "    --outdir \"${OUTPUT_PATH}\" \\\n",
    "    --heuristic convertall \\\n",
    "    --subjects \"${SUBJECT_ID}\" \\\n",
    "    --converter none \\\n",
    "    --bids \\\n",
    "    --overwrite\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Deactivate the conda environment\n",
    "conda deactivate\n",
    "\n",
    "cp \"${OUTPUT_PATH}\"/.heudiconv/\"${SUBJECT_ID}\"/info/dicominfo.tsv \"${OUTPUT_PATH}\"\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# HeudiConv parameters:\n",
    "# --files: Files or directories containing files to process\n",
    "# --outdir: Output directory\n",
    "# --heuristic: Name of a known heuristic or path to the Python script containing heuristic\n",
    "# --subjects: Subject ID\n",
    "# --converter : dicom to nii converter (dcm2niix or none)\n",
    "# --bids: Flag for output into BIDS structure\n",
    "# --overwrite: Flag to overwrite existing files\n",
    "# \n",
    "# For a full list of parameters, see: https://heudiconv.readthedocs.io/en/latest/usage.html \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the script, the table that we are interested in will be located at *`OUTPUT_PATH/.heudiconv/[subject ID]/info/dicominfo.tsv`*. The .heudiconv directory is a hidden directory and you might not be able to see it in your file system unless you copy it to an unhidden directory. \n",
    "\n",
    "\n",
    "Now, you can open the file and keep it open for the next step - creating a heuristic file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. <a id='toc5_2_'></a>[Step 2: Creating a heuristic file](#toc0_)\n",
    "\n",
    "The `heuristic` file is used to convert and organize the DICOM data into BIDS standard. You will need to define heuristic keys. Keys define type of scan. \n",
    "\n",
    "The key definitions must strictly follow BIDS standart! https://bids-specification.readthedocs.io/en/stable/02-common-principles.html\n",
    "\n",
    "In our example dataset, we have four types of scans: anatomical image, fieldmaps (magnitude and phase), and functional runs. We will need to define the keys for them all. Like this:\n",
    "\n",
    "```python\n",
    "    anat = create_key(\n",
    "        'sub-{subject}/anat/sub-{subject}_T1w'\n",
    "        )\n",
    "    fmap_mag = create_key(\n",
    "        'sub-{subject}/fmap/sub-{subject}_acq-func_magnitude'\n",
    "        )\n",
    "    fmap_phase = create_key(\n",
    "        'sub-{subject}/fmap/sub-{subject}_acq-func_phasediff'\n",
    "        )\n",
    "    func_task = create_key(\n",
    "        'sub-{subject}/func/sub-{subject}_task-facerecognition_run-{item:02d}_bold'\n",
    "        )\n",
    "```\n",
    "\n",
    "Next, we will need to specify unique criteria that only the particular scan will meet. This information we get from the `dicominfo.tsv` file that we 'discovered' in the previous step. For example, to uniquely identify the anatomical scan, we can specify that the `protocol_name` contains `MPRAGE`. We don't have any other scans with MPRAGE in protocol name, therefore for the anatomical scan, we don't need to specify any additional cireteria. Similarly, we would specify unique identifiers for the other three scans. \n",
    "\n",
    "Then we integrate the keys and specifications into a heuristic Python file.\n",
    "\n",
    "**Example heuristic file**: [code-examples/bids_heuristic.py](code-examples/bids_heuristic.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# create_key: A common helper function used to create the conversion key in infotodict. \n",
    "# But it is not used directly by HeuDiConv.\n",
    "# --------------------------------------------------------------------------------------\n",
    "def create_key(template, outtype=('nii.gz',), annotation_classes=None):\n",
    "    if template is None or not template:\n",
    "        raise ValueError('Template must be a valid format string')\n",
    "    return template, outtype, annotation_classes\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# infotodict: A function to assist in creating the dictionary, and to be used inside heudiconv.\n",
    "# This is a required function for heudiconv to run.\n",
    "#\n",
    "# seqinfo is a record of DICOM's passed in by heudiconv. Each item in seqinfo contains DICOM metadata \n",
    "# that can be used to isolate the series, and assign it to a conversion key.\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "def infotodict(seqinfo):\n",
    "\n",
    "    # Specify the conversion template for each series following the BIDS format.\n",
    "    # See https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html\n",
    "    \n",
    "    # The structural/anatomical scan\n",
    "    anat = create_key('sub-{subject}/anat/sub-{subject}_T1w')\n",
    "    \n",
    "    # The fieldmap scans\n",
    "    fmap_mag = create_key('sub-{subject}/fmap/sub-{subject}_acq-func_magnitude')\n",
    "    fmap_phase = create_key('sub-{subject}/fmap/sub-{subject}_acq-func_phasediff')\n",
    "    \n",
    "    # The functional scans\n",
    "    # You need to specify the task name in the filename. It must be a single string of letters WITHOUT spaces, underscores, or dashes!\n",
    "    func_task = create_key('sub-{subject}/func/sub-{subject}_task-faceprocessing_run-{item:02d}_bold')\n",
    "    \n",
    "    # Create the dictionary that will be returned by this function.\n",
    "    info = {\n",
    "        anat: [], \n",
    "        fmap_mag: [], \n",
    "        fmap_phase: [],  \n",
    "        func_task: []\n",
    "        }\n",
    "\n",
    "    # Loop through all the DICOM series and assign them to the appropriate conversion key.\n",
    "    for s in seqinfo:\n",
    "        # Uniquelly identify each series\n",
    "        \n",
    "        # Structural\n",
    "        if \"MPRAGE\" in s.series_id:\n",
    "            info[anat].append(s.series_id)\n",
    "            \n",
    "        # Field map Magnitude (the fieldmap with the largest dim3 is the magnitude, the other is the phase)\n",
    "        if 'FieldMapping' in s.series_id and s.series_files == 66:\n",
    "            info[fmap_mag].append(s.series_id)\n",
    "            \n",
    "        # Field map PhaseDiff\n",
    "        if 'FieldMapping' in s.series_id and s.series_files == 33:\n",
    "            info[fmap_phase].append(s.series_id)\n",
    "\n",
    "        # Functional Bold\n",
    "        if s.dim4 > 100:\n",
    "           info[func_task].append(s.series_id)\n",
    "            \n",
    "    # Return the dictionary\n",
    "    return info\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Dictionary to specify options to populate the 'IntendedFor' field of the fmap jsons.\n",
    "#\n",
    "# See https://heudiconv.readthedocs.io/en/latest/heuristics.html#populate-intended-for-opts\n",
    "#\n",
    "# If POPULATE_INTENDED_FOR_OPTS is not present in the heuristic file, IntendedFor will not be populated automatically.\n",
    "# --------------------------------------------------------------------------------------\n",
    "POPULATE_INTENDED_FOR_OPTS = {\n",
    "    'matching_parameters': ['ModalityAcquisitionLabel'],\n",
    "    'criterion': 'Closest'\n",
    "}\n",
    "# 'ModalityAcquisitionLabel': it checks for what modality (anat, func, dwi) each fmap is \n",
    "# intended by checking the _acq- label in the fmap filename and finding corresponding \n",
    "# modalities (e.g. _acq-fmri, _acq-bold and _acq-func will be matched with the func modality)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. <a id='toc5_3_'></a>[Step 3: Converting the data](#toc0_)\n",
    "\n",
    "#### Conversting a single-subject\n",
    "\n",
    "To convert a single subject, we only need to change 2 things in our previous 'DICOM discovery' script:\n",
    "* the output to be in PROJECT_PATH/data/, \n",
    "* location to the heuristic file that we created in the previous step, and \n",
    "* we specify the DICOM to NIfTI converter (*dcm2niix*), so that the files are actually converted. \n",
    "\n",
    "```bash\n",
    "# ------------------------------------------------------------\n",
    "# Define your paths\n",
    "# ------------------------------------------------------------\n",
    "# Your project's root directory\n",
    "PROJECT_PATH='/imaging/correia/da05/workshops/2024-CBU'\n",
    "# Path to the raw DICOM files\n",
    "DICOM_PATH='/mridata/cbu/CBU090942_MR09029'\n",
    "# Location of the output data (it will be created if it doesn't exist)\n",
    "OUTPUT_PATH=\"${PROJECT_PATH}/data/\"\n",
    "# Subject ID\n",
    "SUBJECT_ID='01'\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run the heudiconv\n",
    "# ------------------------------------------------------------\n",
    "conda activate fmri\n",
    "\n",
    "heudiconv \\\n",
    "    --files \"${DICOM_PATH}\"/*/*/*.dcm \\\n",
    "    --outdir \"${OUTPUT_PATH}\" \\\n",
    "    --heuristic $PROJECT_PATH/code/bids_heuristic.py \\\n",
    "    --subjects \"${SUBJECT_ID}\" \\\n",
    "    --converter dcm2niix \\\n",
    "    --bids \\\n",
    "    --overwrite\n",
    "\n",
    "conda deactivate\n",
    "# ------------------------------------------------------------\n",
    "```\n",
    "To convert other subjects as well, you'd need to change the raw DICOM path and subject ID accordingly. If you have multiple subjects, it's a good idea to process them all together using the scheduling system like SLURM (Simple Linux Utility for Resource Management).\n",
    "\n",
    "#### Converting multiple subjects in parallel using SLURM\n",
    "\n",
    "First, we need a generic script that runs HeuDiConv. It would be very similar to the one above where we converted a single subject. \n",
    "\n",
    "**Example of a generic heudiconv script**: [code-examples/heudiconv_script.sh](code-examples/heudiconv_script.sh)\n",
    "\n",
    "Second, you'd need a project-specific script where you define the paths and use the `sbatch` command to execute the generic script for each subject. You can either write your script in bash, or Python if you prefer a more 'user-friendly' syntax. I have written an example script in Python. \n",
    "\n",
    "**Example script to convert multiple subjects' DICOMs to BIDS**: [code-examples/step02_dicom_to_bids.py](code-examples/step02_dicom_to_bids.py)\n",
    "\n",
    "The script's main function is to generate a list of subject IDs alongside their corresponding DICOM paths, define the heuristic file's location, specify the output path, and then construct and execute an `sbatch`` command to run the *heudiconv_script.sh*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. <a id='toc5_4_'></a>['To Do' - additional information to check and add](#toc0_)\n",
    "\n",
    "Once you have converted the DICOMs to BIDS, there are some things that you need to fill in yourself to make the dataset fully BIDS compliant. HeuDiConv has marked such 'missing' information as 'To Do'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset description\n",
    "\n",
    "`dataset_description.json`\n",
    "\n",
    "A brief description of your dataset. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Acknowledgements\": \"TODO: whom you want to acknowledge\",\n",
    "  \"Authors\": [\n",
    "    \"TODO:\",\n",
    "    \"First1 Last1\",\n",
    "    \"First2 Last2\",\n",
    "    \"...\"\n",
    "  ],\n",
    "  \"BIDSVersion\": \"1.8.0\",\n",
    "  \"DatasetDOI\": \"TODO: eventually a DOI for the dataset\",\n",
    "  \"Funding\": [\n",
    "    \"TODO\",\n",
    "    \"GRANT #1\",\n",
    "    \"GRANT #2\"\n",
    "  ],\n",
    "  \"HowToAcknowledge\": \"TODO: describe how to acknowledge -- either cite a corresponding paper, or just in acknowledgement section\",\n",
    "  \"License\": \"TODO: choose a license, e.g. PDDL (http://opendatacommons.org/licenses/pddl/)\",\n",
    "  \"Name\": \"TODO: name of the dataset\",\n",
    "  \"ReferencesAndLinks\": [\n",
    "    \"TODO\",\n",
    "    \"List of papers or websites\"\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participants\n",
    "\n",
    "`participants.json`\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"participant_id\": {\n",
    "    \"Description\": \"Participant identifier\"\n",
    "  },\n",
    "  \"age\": {\n",
    "    \"Description\": \"Age in years (TODO - verify) as in the initial session, might not be correct for other sessions\"\n",
    "  },\n",
    "  \"sex\": {\n",
    "    \"Description\": \"self-rated by participant, M for male/F for female (TODO: verify)\"\n",
    "  },\n",
    "  \"group\": {\n",
    "    \"Description\": \"(TODO: adjust - by default everyone is in control group)\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task information\n",
    "\n",
    "`task-facerecognition_bold.json`\n",
    "\n",
    "Could add full task name and a Cognitive Atlas ID if it exists. \n",
    "```json\n",
    "{\n",
    " ...\n",
    "  \"TaskName\": \"TODO: full task name for facerecognition\",\n",
    " ...\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Events\n",
    "\n",
    "For the functional images, if they are not resting-state images, but participants performed a task, you need to provide the trial type, onset and duration details. HeuDiConv generates the `events.tsv` file for each functional run. The files are just a template that you need to fill with the actual data. You would get this data from tour experimental script outputs (make sure you have programmed your task to easily retrieve the needed trial and timing details). \n",
    "\n",
    "For the example dataset used in this tutorial, I retrieved the event timing information from the [OpenNeuro version of this dataset](https://openneuro.org/datasets/ds000117/versions/1.0.5)\n",
    "\n",
    "**See my script here:** [code-examples/step03_events_to_bids.py](code-examples/step03_events_to_bids.py).\n",
    "\n",
    "The event files in the OpenNeuro version of this dataset, do not fully comply with the current BIDS specification. According to `BIDS` specification for [Task Events](https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/05-task-events.html), a correct column name is *'trial_type'*, not *'stim_type'*. In my script, after downloading the files, I fixed this naming. In addition, I removed the no-name trial types (the 'rest' period) as we don't want to model rest as a separate event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the OpenNeuro version only had recorded three event types: FAMOUS, UNFAMILIAR, SCRAMBLED. But I wanted to also analyse the repetition suppression effects. Therefore, I split each condition into three: e.g., FAMOUS_1 (the initial presentation), FAMOUS_im (immediate repetition), FAMOUS_dl (delayed repetition).\n",
    "\n",
    "If interested, you can **see my script for this here**: [code-examples/step04_transform_events.py](code-examples/step04_transform_events.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy the prepared events\n",
    "!rm -f FaceProcessing/data/sub-04/func/sub-04_task-faceprocessing_run-*.tsv\n",
    "!cp -r sub-04_task-files/* FaceProcessing/data/sub-04/func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "events_file = 'FaceProcessing/data/sub-04/func/sub-04_task-faceprocessing_run-01_events.tsv'\n",
    "events = pd.read_csv(events_file, sep='\\t')\n",
    "\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### README\n",
    "\n",
    "*\"TODO: Provide description for the dataset -- basic details about the study, possibly pointing to pre-registration (if public or embargoed)\"*\n",
    "\n",
    "See an example for the OpenNeuro version of this dataset https://openneuro.org/datasets/ds000117/versions/1.0.5/file-display/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a id='toc6_'></a>[Validate BIDS structure](#toc0_)\n",
    "\n",
    "Once we have our BIDS dataset, we can use an [online BIDS validator](https://bids-standard.github.io/bids-validator/) to check if our dataset confirms with BIDS standard and what additional information we might need to include in your dataset's metadata. \n",
    "\n",
    "For this example dataset, we get some warnings about events custom columns that have no description. We can include *events.json* file that contains this information. For guidance see the BIDS specification https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/05-task-events.html\n",
    "\n",
    "Suspiciously long event: `sub-10_task-facerecognition_run-09_events.tsv`. We can add this information in the README file: *Owing to scanner error, Subject 10 only has 170 volumes in last run (Run 9) (hence the BIDS warning of some onsets in events.tsv file being later than the data)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a id='toc7_'></a>[PyBIDS](#toc0_)\n",
    "\n",
    "`PyBids` is a Python module to interface with datasets conforming BIDS. See the [documentation](https://bids-standard.github.io/pybids/) and [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7409983/) for more info. \n",
    "\n",
    "```\n",
    "pip install pybids\n",
    "```\n",
    "\n",
    "**Let's explore some of the functionality of pybids.layout.** The material is adapted from https://github.com/bids-standard/pybids/tree/master/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bids.layout import BIDSLayout\n",
    "\n",
    "ds_path = 'FaceProcessing/data'\n",
    "\n",
    "# Initialize the layout\n",
    "layout = BIDSLayout(ds_path)\n",
    "\n",
    "# Print some basic information about the layout\n",
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. <a id='toc7_1_'></a>[Querying the BIDSLayout](#toc0_)\n",
    "The main method for querying `BIDSLayout` is `.get()`.\n",
    "\n",
    "If we call `.get()` with no additional arguments, we get back a list of all the BIDS files in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = layout.get()\n",
    "print(\"There are {} files in the layout.\".format(len(all_files)))\n",
    "print(\"\\nThe first 5 files are:\")\n",
    "all_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned object is a **Python list**. Each element in the list is a `BIDSFile` object. \n",
    "\n",
    "We can also get just filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get(return_type='filename')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get such information as\n",
    "* all `subject` IDs\n",
    "* all `task` names\n",
    "* dataset `description`\n",
    "* the BOLD repetition time TR\n",
    "* how many `runs` there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_dataset_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_tr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding runs, it might be that there are varied number of runs accross participants. So, let's get runs for each participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sID in layout.get_subjects(): \n",
    "    print(layout.get_runs(subject = sID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. <a id='toc7_2_'></a>[Filtering files by entities](#toc0_)\n",
    "We can pass any BIDS-defined entities (keywords) to `.get()` method. For example, here's how we would retrieve all BOLD runs with `.nii.gz` extensions for subject `04`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve filenames of all BOLD runs for subject\n",
    "layout.get(subject='04', extension='nii.gz', suffix='bold', return_type='filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the entities are found in the names of BIDS files. For example `sub-01_task-facerecognition_run-01_bold.nii.gz` has entities: **subject**, **task**, **run**, **suffix**, **extension**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the list of all availabe entities by `layout.get_entities()`.\n",
    "\n",
    "Here are a few of the most common entities:\n",
    "\n",
    "* `suffix`: The part of a BIDS filename just before the extension (e.g., 'bold', 'events', 'T1w', etc.).\n",
    "* `subject`: The subject label\n",
    "* `session`: The session label\n",
    "* `run`: The run index\n",
    "* `task`: The task name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. <a id='toc7_3_'></a>[Filtering by metadata](#toc0_)\n",
    "Sometimes we want to search for files based not just on their names, but also based on metadata defined in JSON files. We can pass any key that occurs in any JSON file in our project as an argument to `.get()`. We can combine these with any number of core BIDS entities (like `subject`, `run`, etc.).\n",
    "\n",
    "For example, we want to retrieve `SpacingBetweenSlices` (measured from center-to-center of each slice, in mm) for all our subjects. And let's create a nice data frame of this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = []\n",
    "for subject in layout.get_subjects():\n",
    "    d.append(\n",
    "        {\n",
    "            'subject': subject,\n",
    "            'spacing': layout.get_SpacingBetweenSlices(subject=subject, suffix='bold')\n",
    "        }\n",
    "    )\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having different spacing between the slices is rather unusual. But authors were trying to cover the whole cortex. So, larger brains, had larger spacing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==================================================================================================**\n",
    "\n",
    "**EXCERCISE**\n",
    "\n",
    "We want to know the time of the day when each subject was scanned. The scanning started with T1 images, so we want to retrieve the `AcquisitionTime` of all subjects' `T1w` images. Adapt the script above to acquire this information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. <a id='toc7_4_'></a>[Other `return_type` values](#toc0_)\n",
    "We can also ask `get()` to return unique values (or IDs) of particular entities. For example, we want to know which subjects had a fieldmap acquired. We can request that information by setting `return_type='id'` - to get subject IDs. When using this option, we also need to specify a `target` entity for the ID (in this case, subject). This combination tells the `BIDSLayout` to return the unique values for the specified `target` entity. \n",
    "\n",
    "For example, in the next example, we ask for all of the unique subject IDs that have at least one file with a `phasediff` (fieldmap) suffix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask get() to return the ids of subjects that have phasediff (fieldmap_ files)\n",
    "\n",
    "layout.get(return_type='id', target='subject', suffix='phasediff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our `target` is a BIDS entity that corresponds to a particular directory in the BIDS specification (e.g., `subject` or `session`) we can also use `return_type='dir'` to get all matching subdirectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get(return_type='dir', target='subject')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. <a id='toc7_5_'></a>[The `BIDSFile`](#toc0_)\n",
    "When you call `.get()` on a `BIDSLayout`, the default returned values are objects of class `BIDSFile`. A `BIDSFile` is a lightweight container for individual files in a BIDS dataset. It provides easy access to a variety of useful attributes and methods. Let's take a closer look. First, let's pick a random file from our existing `layout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the 7th file in the dataset\n",
    "bf = layout.get(subject='04', extension='nii.gz', suffix='bold')[0]\n",
    "# Print it\n",
    "bf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes and methods available to us in a `BIDSFile` (note that some of these are only available for certain subclasses of `BIDSFile`; e.g., you can't call `get_image()` on a `BIDSFile` that doesn't correspond to an image file!):\n",
    "* `.path`: The full path of the associated file\n",
    "* `.filename`: The associated file's filename (without directory)\n",
    "* `.dirname`: The directory containing the file\n",
    "* `.get_entities()`: Returns information about entities associated with this `BIDSFile` (optionally including metadata)\n",
    "* `.get_image()`: Returns the file contents as a nibabel image (only works for image files)\n",
    "* `.get_df()`: Get file contents as a pandas DataFrame (only works for TSV files)\n",
    "* `.get_metadata()`: Returns a dictionary of all metadata found in associated JSON files\n",
    "* `.get_associations()`: Returns a list of all files associated with this one in some way\n",
    "\n",
    "Let's see some of these in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the entities associated with this file, and their values\n",
    "bf.get_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 30 metadata items associated with this file\n",
    "file_metadata = bf.get_metadata()\n",
    "\n",
    "{k: file_metadata[k] for k in list(file_metadata)[:30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.get_image()`: Returns the file contents as a `nibabel` image (only works for image files). We can then display the image, for example, using `OrthoSlicer3D`.   \n",
    "\n",
    "**Note:** When using `orthoview()` in notebook, don't forget to close figures afterward again or use %matplotlib inline again, otherwise, you cannot plot any other figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "bf.get_image().orthoview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. <a id='toc7_6_'></a>[`.tsv` files](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases where a file has a `.tsv.gz` or `.tsv` extension, it will automatically be created as a `BIDSDataFile`, and we can easily grab the contents as a `DataFrame`.\n",
    "\n",
    "Let's look at the first `events` file from our layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first events file\n",
    "evfile = layout.get(suffix='events')[0]\n",
    "\n",
    "# Get contents as a DataFrame and show the first few rows\n",
    "df = evfile.get_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `participants` information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = layout.get(suffix='participants', extension='tsv')[0]\n",
    "\n",
    "df = participants.get_df()\n",
    "df.sort_values(by=['participant_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7. <a id='toc7_7_'></a>[Filename parsing](#toc0_)\n",
    "Let's say you have a filename, and you want to manually extract BIDS entities from it. The `parse_file_entities` method provides the facility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.parse_file_entities('some_path_to_bids_file/sub-04_task-facerecognition_run-01_bold.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same for `BIDSFile` object that we defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.parse_file_entities(bf.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.parse_file_entities(bf.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8. <a id='toc7_8_'></a>[Report generation](#toc0_)\n",
    "`PyBIDS` also allows you to automatically create data acquisition reports based on the available `image` and `meta-data` information. This enables a new level of standardisation and transparency. FAIR-ness, meta-analyses, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the BIDSReport function from the reports submodule\n",
    "from bids.reports import BIDSReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only need to apply the `BIDSReport` function to our `layout` and generate our report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a report for the dataset\n",
    "report = BIDSReport(layout)\n",
    "\n",
    "# Method generate returns a Counter of unique descriptions across subjects\n",
    "try:\n",
    "    descriptions = report.generate()\n",
    "    pub_description = descriptions.most_common()[0][0]\n",
    "    print(pub_description)\n",
    "except IndexError:\n",
    "    print('Sorry, it seems that the dataset is not complete and report cannot be generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
