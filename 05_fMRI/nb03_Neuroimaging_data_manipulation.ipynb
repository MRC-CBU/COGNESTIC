{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4e4ee6",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Apšvalka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) \n",
    "- **Date:** August 2024  \n",
    "- **conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/c0dc3faa699e19187d5d5a8fb491a66baa27b9fb/mri_environment.yml) to run this notebook and any accompanied scripts.\n",
    "\n",
    "# Neuroimaging data manipulation\n",
    "\n",
    "Adapted from https://carpentries-incubator.github.io/SDC-BIDS-IntroMRI/ and https://github.com/miykael/workshop_pybrain\n",
    "\n",
    "The primary goal of this section is to develop a conceptual understanding of the data structures involved, to facilitate diagnosing problems in data or analysis pipelines.\n",
    "\n",
    "We'll be exploring two libraries: [nibabel](http://nipy.org/nibabel/) and [nilearn](https://nilearn.github.io/). Each of these projects has excellent documentation. While this should get you started, it is well worth your time to look through these sites.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7a1fd",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [NiBabel](#toc1_)    \n",
    "2. [Nilearn](#toc2_)    \n",
    "3. [Setup](#toc3_)    \n",
    "4. [Loading and inspecting images in `nibabel`](#toc4_)    \n",
    "4.1. [Header](#toc4_1_)    \n",
    "4.2. [Data](#toc4_2_)    \n",
    "4.3. [Affine](#toc4_3_)    \n",
    "5. [Image manipulation with `nilearn`](#toc5_)    \n",
    "5.1. [The mean image](#toc5_1_)    \n",
    "5.2. [Resample image to a template](#toc5_2_)    \n",
    "5.3. [Smooth an image](#toc5_3_)    \n",
    "5.4. [Plotting a time course](#toc5_4_)    \n",
    "5.5. [Masking an image](#toc5_5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=2\n",
    "\tmaxLevel=3\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab0a5ec",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "## 1. <a id='toc1_'></a>[NiBabel](#toc0_)\n",
    "<img align=\"right\" src=\"https://nipy.org/nibabel/_static/nibabel-logo.svg\" width=\"16%\">\n",
    "\n",
    "**NiBabel** is a low-level Python library that gives access to a variety of imaging formats, with a particular focus on providing a common interface to the various **volumetric** formats produced by scanners and used in common neuroimaging toolkits.\n",
    "\n",
    " - NIfTI-1\n",
    " - NIfTI-2\n",
    " - SPM Analyze\n",
    " - FreeSurfer .mgh/.mgz files\n",
    " - Philips PAR/REC\n",
    " - Siemens ECAT\n",
    " - DICOM (limited support)\n",
    "\n",
    "It also supports **surface** file formats\n",
    "\n",
    " - GIFTI\n",
    " - FreeSurfer surfaces, labels and annotations\n",
    "\n",
    "**Connectivity**\n",
    "\n",
    " - CIFTI-2\n",
    "\n",
    "**Tractography**\n",
    "\n",
    " - TrackViz .trk files\n",
    "\n",
    "And a number of related formats.\n",
    "\n",
    "**Note:** Almost all of these can be loaded through the `nibabel.load` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb62515f",
   "metadata": {},
   "source": [
    "## 2. <a id='toc2_'></a>[Nilearn](#toc0_)\n",
    "<img align=\"right\" src=\"https://nilearn.github.io/stable/_static/nilearn-transparent.png\" width=\"16%\">\n",
    "\n",
    "**Nilearn** labels itself as: *A Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modeling, classification, decoding, or connectivity analysis.*\n",
    "\n",
    "But it's much more than that. It is also an excellent library to **manipulate** (e.g. resample images, smooth images, region-of-interest extraction, etc.) and **visualize** your neuroimages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2243e2",
   "metadata": {},
   "source": [
    "## 3. <a id='toc3_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5373f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "from nilearn import image as nli\n",
    "\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "## Set numpy to print 3 decimal points and suppress small values\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a144fd4",
   "metadata": {},
   "source": [
    "## 4. <a id='toc4_'></a>[Loading and inspecting images in `nibabel`](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012bffa",
   "metadata": {},
   "source": [
    "First, we will use the `load()` function to create a `NiBabel` image object from a NIfTI file. \n",
    "\n",
    "We’ll load in an example `T1w` and `BOLD` images that we will retrieve from our `BIDS` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids.layout import BIDSLayout\n",
    "\n",
    "ds_path = 'FaceProcessing/data'\n",
    "\n",
    "# Initialize the BIDS layout\n",
    "layout = BIDSLayout(ds_path)\n",
    "\n",
    "# Get subject's T1w image and all Bold images\n",
    "t1_file = layout.get(subject='04', extension='nii.gz', datatype='anat', return_type='filename')\n",
    "bold_files = layout.get(subject='04', extension='nii.gz', suffix='bold', return_type='filename')\n",
    "\n",
    "# Load the T1 image and the 1st Bold image\n",
    "t1_img = nib.load(t1_file[0])\n",
    "bold_img = nib.load(bold_files[0])\n",
    "\n",
    "# Print the shape of both images\n",
    "print(f\"The shape of the T1 image: {t1_img.shape}\")\n",
    "print(f\"The shape of the Bold image: {bold_img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8169d01",
   "metadata": {},
   "source": [
    "Loading in a NIfTI file with `NiBabel` gives us a special type of data object which encodes all the information in the file. Each bit of information is called an attribute in Python’s terminology. To see all of these attributes, type `t1_img.` followed by pressing `Tab`. There are three main attributes that we’ll discuss today:\n",
    "* `Header`\n",
    "* `Data`\n",
    "* `Affine`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb62be9",
   "metadata": {},
   "source": [
    "### 4.1. <a id='toc4_1_'></a>[Header](#toc0_)\n",
    "`Header` contains metadata about the image, such as image dimensions, data type, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17177411",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_hdr = t1_img.header\n",
    "print(t1_hdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98a6ab",
   "metadata": {},
   "source": [
    "`t1_hdr` is a Python **dictionary**. Dictionaries are containers that hold pairs of objects - **keys** and **values**. \n",
    "We can access the value stored by a given key by typing: `t1_hdr['<key_name>']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ffd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_hdr['magic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd1e5a",
   "metadata": {},
   "source": [
    "**==================================================================================================**\n",
    "\n",
    "**EXCERCISE**\n",
    "\n",
    "Extract `pixdim` value from the `BOLD image` header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65f6e6",
   "metadata": {},
   "source": [
    "**==================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4fed71",
   "metadata": {},
   "source": [
    "### 4.2. <a id='toc4_2_'></a>[Data](#toc0_)\n",
    "As you’ve seen above, the header contains useful information that gives us information about the properties (metadata) associated with the MR data we have loaded in. Now we’ll move in to loading the actual image data itself. We can achieve this by using the method called `get_fdata()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the T1 and Bold image data\n",
    "t1_data = t1_img.get_fdata()\n",
    "bold_data = bold_img.get_fdata()\n",
    "\n",
    "# How does the T1 data look like\n",
    "print(t1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521c638",
   "metadata": {},
   "source": [
    "The data is a **multidimensional array** representing the image data.\n",
    "\n",
    "How can we see the number of dimensions in the `t1_data` array? Once again, all of the attributes of the array can be seen by typing `t1_data.` followed by `Tab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1 number of dimensions\n",
    "print(f\"T1w image dimensions: {t1_data.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594656c",
   "metadata": {},
   "source": [
    "**==================================================================================================**\n",
    "\n",
    "**EXCERCISE**\n",
    "\n",
    "What's the imensions of our BOLD image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c326c",
   "metadata": {},
   "source": [
    "**==================================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big each dimension is\n",
    "print(f\"T1w image shape is {t1_data.shape}\")\n",
    "print(f\"BOLD image shape is {bold_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006807d",
   "metadata": {},
   "source": [
    "The first 3 numbers given here represent the number of values along a respective dimension *(x,y,z)*. For the `BOLD` image this brain was scanned in `33` axial slices with a resolution of `64 x 64` voxels per slice. That means there are:\n",
    "\n",
    "`64 * 64 * 33 = 135,168` voxels in total! And the BOLD signal was sampled `208` times. \n",
    "\n",
    "Let’s see the type of data inside of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2108d0b",
   "metadata": {},
   "source": [
    "This tells us that each element in the array (or voxel) is a floating-point number.\n",
    "The data type of an image controls the range of possible intensities. As the number of possible values increases, the amount of space the image takes up in memory also increases. \n",
    "\n",
    "Let's see what the range of these images are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f\"T1w image range is {str(np.min(t1_data))} to {str(np.max(t1_data))}\")\n",
    "print(f\"BOLD image range is {str(np.min(bold_data))} to {str(np.max(bold_data))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b288d",
   "metadata": {},
   "source": [
    "\n",
    "How do we examine **what value a particular voxel is**? We can inspect the value of a voxel by selecting an index as follows:\n",
    "\n",
    "`data[x,y,z]`\n",
    "\n",
    "So for example we can inspect a voxel at coordinates `(20,60,50)` by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A value of a T1 image voxel at coordinates (20,60,50)\n",
    "t1_data[19, 59, 49]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff4423",
   "metadata": {},
   "source": [
    "**NOTE**: Python uses **zero-based indexing**. The first item in the array is item `0`. The second item is item `1`, the third is item `2`, etc.\n",
    "\n",
    "We can also extract data from a **slice** for visualisation and analysis. \n",
    "**Slicing** does exactly what it seems to imply. Giving our 3D volume, we pull out a 2D slice of our data. Below is an example of slicing from left to right (sagittal slicing, along the `x-axis`). We look at the '20th' slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1092e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of the T1 image's 20th sagittal slice\n",
    "x_slice = t1_data[19, :, :]\n",
    "print(x_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f027e9",
   "metadata": {},
   "source": [
    "This is similar to the indexing we did before to pull out a single voxel. However, instead of providing a value for each axis, the `:` indicates that we want to grab all values from that particular axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_slice = t1_data[:, :, 2]\n",
    "print(z_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a4170",
   "metadata": {},
   "source": [
    "We’ve been looking at voxel nummerical values, but we have no idea what the images actually look like! Let's look how the `100` slice of each of the `3` dimensions of T1 image look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe06aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "slices = [t1_data[99, :, :], t1_data[:, 99, :], t1_data[:, :, 99]]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(slices), figsize=(15,15))\n",
    "for i, slice in enumerate(slices):\n",
    "    axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f060a0",
   "metadata": {},
   "source": [
    "`Nibabel` has its own viewer, which can be accessed through `img.orthoview()`.\n",
    "\n",
    "**Sidenote to plotting with orthoview()**\n",
    "\n",
    "As with other figures, if you initiated `matplotlib` with `%matplotlib inline`, the output figure will be static. If you use `orthoview()` in a normal IPython console, it will create an interactive window, and you can click to select different slices, similar to `mricron`. To get a similar experience in a `jupyter notebook`, use `%matplotlib notebook`. **But don't forget to close figures afterward again or use` %matplotlib inline` again, otherwise, you cannot plot any other figures.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2efc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "t1_img.orthoview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386b0e8",
   "metadata": {},
   "source": [
    "### 4.3. <a id='toc4_3_'></a>[Affine](#toc0_)\n",
    "The final important piece of metadata associated with an image file is the **affine matrix**. `Affine` tells the position of the image array data in a reference space. \n",
    "\n",
    "The voxel coordinate tells us almost nothing about where the data came from in terms of position in the scanner.  For example, let’s say we have the voxel coordinate (26, 30, 16). Without more information we have no idea whether this voxel position is on the left or right of the brain, or came from the left or right of the scanner.\n",
    "\n",
    "This is because the scanner allows us to collect voxel data in almost any arbitrary position and orientation within the magnet.\n",
    "\n",
    "Usually BOLD images are acquired in a different angle and with a smaller area coverage than the T1w anatomical images - the bounding boxes are different. \n",
    "\n",
    "<img align=\"centre\" src=\"https://nipy.org/nibabel/_images/localizer.png\" width=\"70%\">\n",
    "\n",
    "The center of the BOLD image data is not quite at the center of magnet bore (the magnet *isocenter*).\n",
    "\n",
    "We have an anatomical and an BOLD scan, and later on we will surely want to be able to relate the data from subject's `_bold.nii.gz` to the same subject's `_T1w.nii.gz`. We can’t easily do this at the moment, because we collected the anatomical image with a different field of view and orientation to the EPI image, so the voxel coordinates in the BOLD image refer to **different locations in the magnet** to the voxel coordinates in the anatomical image.\n",
    "\n",
    "We solve this problem by keeping track of the relationship of voxel coordinates to some reference space - e.g, our magnet space. The **affine array** stores the relationship between voxel coordinates in the image data array and coordinates in the reference space. Because we know the relationship of voxel coordinates to the reference space for both images, we can use this information to relate voxel coordinates in subject's `_bold.nii.gz` to spatially equivalent voxel coordinates in the same subject's `_T1w.nii.gz`.\n",
    "\n",
    "The origin of the axes is at the magnet isocenter. This is coordinate `(0, 0, 0)` in our reference space. All three axes pass through the isocenter. The units of the scanner reference space are **mm**. If the subject is lying in the usual position for a brain scan, face up and head first in the scanner, then \n",
    "* scanner-left/right is also the left-**right** axis of the subject’s head, \n",
    "* scanner-floor/ceiling is the posterior-**anterior** axis of the head and \n",
    "* scanner-bore is the inferior-**superior** axis of the head.\n",
    "\n",
    "This is the most common subject-centered scanner coordinate system in neuroimaging, called **scanner RAS+** (right, anterior, superior). The **+** sign means that Right, Anterior, Superior are all positive values on these axes (and left, posterior, inferior are negative). **NOTE**: **right** means the subject’s **right**.\n",
    "\n",
    "<img align=\"left\" src=\"https://people.cas.sc.edu/rorden/anatomy/tspace.gif\" width=\"30%\">\n",
    "\n",
    "<img align=\"right\" src=\"https://www.slicer.org/w/img_auth.php/2/22/Coordinate_sytems.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab35200",
   "metadata": {},
   "source": [
    "Below is the affine matrix for our anatomical `T1w` data. That is, relating the **voxel coordinates** to **world (scanner) coordinates** in **RAS+** space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f89517",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_affine = t1_img.affine\n",
    "print(t1_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fe4c9",
   "metadata": {},
   "source": [
    "In the image header, the different `sform_code` and `qform_code` values specify which RAS+ space the sform affine refers to, with these interpretations:\n",
    "\n",
    "| Code | Label     | Meaning                       |\n",
    "|------|-----------|--------------------------------|\n",
    "| 0    | unknown   | sform not defined              |\n",
    "| 1    | scanner   | RAS+ in scanner coordinates    |\n",
    "| 2    | aligned   | RAS+ aligned to some other scan|\n",
    "| 3    | talairach | RAS+ in Talairach atlas space  |\n",
    "| 4    | mni       | RAS+ in MNI atlas space        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1066f6",
   "metadata": {},
   "source": [
    "How 'shifted' is the T1 image's voxel space center from the reference space (scanner bore) center?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nibabel has a function apply_affine \n",
    "from nibabel.affines import apply_affine \n",
    "\n",
    "# the central voxel in the voxel space\n",
    "t1_vox_center = (np.array(t1_data.shape) - 1) / 2.\n",
    "print(f\"The central voxel in the voxel space is {t1_vox_center.astype(int)}\")\n",
    "\n",
    "# distance from the reference space centre (in mm)\n",
    "# voxel space's central voxel's location in the reference space\n",
    "t1_vox_center_in_scanner = apply_affine(t1_img.affine, t1_vox_center)\n",
    "print(f\"The voxel space central voxel in the scanner space is at {t1_vox_center_in_scanner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8b100",
   "metadata": {},
   "source": [
    "That means the center of the T1 image field of view is ~4.1 mm to the right from the isocenter of the magnet, ~18.1 mm anterior to the isocenter and ~1.2 mm above (superior) the isocenter.\n",
    "\n",
    "The parameters in the affine array can therefore give the position of any voxel coordinate, relative to the scanner RAS+ reference space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ca1d",
   "metadata": {},
   "source": [
    "When we register an image to some template, e.g., **MNI template**, we will get an affine giving the relationship between voxels in the aligned image and the MNI RAS+ space. The origin `(0, 0, 0)` ot the MNI reference space is anterior commissure (AC). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16ed44",
   "metadata": {},
   "source": [
    "## 5. <a id='toc5_'></a>[Image manipulation with `nilearn`](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d8543",
   "metadata": {},
   "source": [
    "### 5.1. <a id='toc5_1_'></a>[The mean image](#toc0_)\n",
    "If you use `nibabel` to compute the mean image, you first need to load the img, get the data and then compute the mean thereof. \n",
    "\n",
    "**With `nilearn`, you can do all this in just one line with `mean image`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73bd2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img = nli.mean_img(bold_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = mean_img.get_fdata()\n",
    "mean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bff42",
   "metadata": {},
   "source": [
    "From version `0.5.0` on, `nilearn` provides interactive visual views. A nice alternative to `nibabel`'s `orthoview()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a35934",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img(mean_img, bg_img=mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e842e",
   "metadata": {},
   "source": [
    "### 5.2. <a id='toc5_2_'></a>[Resample image to a template](#toc0_)\n",
    "Using `resample_to_img`, we can resample one image to have the same dimensions as another one. For example, let's resample an anatomical `T1` image to the computed `mean` image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image shapes before resampling\n",
    "print([mean_img.shape, t1_img.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling T1 to the mean Bold image\n",
    "resampled_t1 = nli.resample_to_img(t1_img, mean_img)\n",
    "\n",
    "# T1 image shape after resampling\n",
    "resampled_t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4ea91",
   "metadata": {},
   "source": [
    "How does the resampled `T1` image look like? Here we will use another `nilearn` plotting function that plots a static image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(t1_img, title = 'original t1', dim=-1)\n",
    "plotting.plot_anat(resampled_t1, title = 'resampled t1', dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05042be1",
   "metadata": {},
   "source": [
    "### 5.3. <a id='toc5_3_'></a>[Smooth an image](#toc0_)\n",
    "Using `smooth_img`, we can very quickly smooth any kind of MRI image. Let's, for example, take the mean image from above and smooth it with different FWHM values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for fwhm in range(1, 12, 5):\n",
    "    smoothed_img = nli.smooth_img(mean_img, fwhm)\n",
    "    plotting.plot_epi(smoothed_img, title=\"Smoothing %imm\" % fwhm,\n",
    "                     display_mode='z', cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53f1eb",
   "metadata": {},
   "source": [
    "### 5.4. <a id='toc5_4_'></a>[Plotting a time course](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f531d0",
   "metadata": {},
   "source": [
    "Let's plot a time course of the central voxel in our BOLD imgage and some other random voxel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24961c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the xyz of the center \n",
    "bold_vox_center = (np.array(bold_data.shape) - 1) / 2.\n",
    "x, y, z, _ = bold_vox_center\n",
    "\n",
    "# set the plot size\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plot the central voxel time course\n",
    "plt.plot(bold_data[int(x), int(y), int(z), :])\n",
    "\n",
    "# plot some random voxel time course\n",
    "plt.plot(bold_data[28, 45, 15, :])\n",
    "\n",
    "# add legends to the plot\n",
    "plt.legend(['center voxel', 'random voxel']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440ac7b",
   "metadata": {},
   "source": [
    "### 5.5. <a id='toc5_5_'></a>[Masking an image](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7aa411",
   "metadata": {},
   "source": [
    "You can consider your images just a special kind of a number array. And you can do any nuber operations on the images. \n",
    "\n",
    "For example, let's take our BOLD functional image, \n",
    "1. create the mean image of it\n",
    "2. threshold it to only keep the voxels that have a value that is higher than 95% of all voxels. Of this thresholded image, we only \n",
    "3. keep those regions that are bigger than 1000mm^3. And finally, we \n",
    "4. binarize those regions to create a mask image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 create the mean image\n",
    "mean_img = nli.mean_img(bold_img)\n",
    "\n",
    "#2  keep voxels that have a value that is higher than 95% of all voxels\n",
    "thr = nli.threshold_img(mean_img, threshold='95%')\n",
    "\n",
    "#let's see how the thresholded image look compared to the original mean image\n",
    "plotting.view_img(thr, bg_img=mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 only keep those voxels that are in regions/clusters that are bigger than 1000mm^3.\n",
    "\n",
    "# get a size of 1 voxel in mm^3\n",
    "print('pixdim:', thr.header['pixdim'])\n",
    "voxel_size = np.prod(thr.header['pixdim'][1:4])  \n",
    "print('one voxel size in mm^3:', voxel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mask that only keeps those big clusters.\n",
    "from nilearn.regions import connected_regions\n",
    "\n",
    "cluster = connected_regions(thr, min_region_size=1000. / voxel_size, smoothing_fwhm=1)[0]\n",
    "\n",
    "#4 And finally, let's binarize this cluster image to create a mask.\n",
    "mask = nli.math_img('np.mean(img,axis=3) > 0', img=cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how our mask looks on the mean BOLD image\n",
    "\n",
    "from nilearn.plotting import plot_roi\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "plotting.plot_roi(mask, \n",
    "                  bg_img=mean_img, \n",
    "                  draw_cross=False, \n",
    "                  dim=-.5, \n",
    "                  cmap=ListedColormap([\"red\"]), \n",
    "                  figure=fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.933px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
