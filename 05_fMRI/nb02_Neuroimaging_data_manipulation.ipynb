{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a383667",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Apšvalka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) \n",
    "- **Date:** August 2024  \n",
    "- **conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts.\n",
    "\n",
    "# Neuroimaging data manipulation\n",
    "\n",
    "Adapted from:\n",
    "- https://carpentries-incubator.github.io/SDC-BIDS-IntroMRI/anatomy-of-nifti.html and \n",
    "- https://github.com/miykael/workshop_pybrain\n",
    "\n",
    "We'll be exploring two libraries: [NiBabel](http://nipy.org/nibabel/) and [Nilearn](https://nilearn.github.io/stable/index.html). \n",
    "\n",
    "**NiBabel** gives read and write access to common neuroimaging file formats. NiBabel’s API gives full or selective access to header information (metadata), and image data is made available via NumPy arrays.\n",
    "\n",
    "\"**Nilearn** enables **approachable and versatile analyses of brain volumes**. It provides statistical and machine-learning tools, with instructive documentation & open community. It supports general linear model (GLM) based analysis and leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis.\"\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87b6b8",
   "metadata": {},
   "source": [
    "**Table of contents**  \n",
    "1. Setup   \n",
    "2. Loading and inspecting images in `nibabel`  \n",
    "2.1. Header   \n",
    "2.2. Data  \n",
    "2.3. Affine   \n",
    "3. Image manipulation with `nilearn`   \n",
    "3.1. The mean image](#toc3_1_)    \n",
    "3.2. Resample image to a template   \n",
    "3.3. Smooth an image \n",
    "3.4. Plotting a time course   \n",
    "3.5. Masking an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "from nilearn import plotting\n",
    "from nilearn import image as nli\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as plt\n",
    "\n",
    "import numpy as np\n",
    "## Set numpy to print 3 decimal points and suppress small values\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and inspecting images in `nibabel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee195e",
   "metadata": {},
   "source": [
    "First, we will use the `load()` function to create a `NiBabel` image object from a NIfTI file. \n",
    "\n",
    "We’ll load in an example `T1w` and `BOLD` images that we will retrieve from our `BIDS` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids.layout import BIDSLayout\n",
    "\n",
    "fmri_data_dir = 'FaceProcessing/data'\n",
    "\n",
    "# Initialize the BIDS layout\n",
    "layout = BIDSLayout(fmri_data_dir)\n",
    "\n",
    "# Get subject's T1w image and all Bold images\n",
    "t1_file = layout.get(subject='04', extension='nii.gz', datatype='anat', return_type='filename')\n",
    "bold_files = layout.get(subject='04', extension='nii.gz', suffix='bold', return_type='filename')\n",
    "\n",
    "# Load the T1 image and the 1st Bold image\n",
    "t1_img = nib.load(t1_file[0])\n",
    "bold_img = nib.load(bold_files[0])\n",
    "\n",
    "# Print the shape of both images\n",
    "print(f\"The shape of the T1 image: {t1_img.shape}\")\n",
    "print(f\"The shape of the Bold image: {bold_img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8169d01",
   "metadata": {},
   "source": [
    "Loading in a NIfTI file with `NiBabel` gives us a special type of data object which encodes all the information in the file. Each bit of information is called an attribute in Python’s terminology. To see all of these attributes, type `t1_img.` followed by pressing `Tab`. There are three main attributes that we’ll discuss today:\n",
    "* `Header`\n",
    "* `Data`\n",
    "* `Affine`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header\n",
    "`Header` contains metadata about the image, such as image dimensions, data type, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7626f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_hdr = t1_img.header\n",
    "print(t1_hdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98a6ab",
   "metadata": {},
   "source": [
    "`t1_hdr` is a Python **dictionary**. Dictionaries are containers that hold pairs of objects - **keys** and **values**. \n",
    "We can access the value stored by a given key by typing: `t1_hdr['<key_name>']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ffd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_hdr['magic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd1e5a",
   "metadata": {},
   "source": [
    "**==================================================================================================**\n",
    "\n",
    "**EXCERCISE**\n",
    "\n",
    "Extract `pixdim` value from the `BOLD image` header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec28e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65f6e6",
   "metadata": {},
   "source": [
    "**==================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "As you’ve seen above, the header contains valuable metadata about the MR data we’ve loaded. Now, we'll move on to loading the actual image data itself. This can be done using the `get_fdata()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the T1 and Bold image data\n",
    "t1_data = t1_img.get_fdata()\n",
    "bold_data = bold_img.get_fdata()\n",
    "\n",
    "# How does the T1 data look like\n",
    "print(t1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521c638",
   "metadata": {},
   "source": [
    "The data is a **multidimensional array** representing the image.\n",
    "\n",
    "How can we check the number of dimensions in the t1_data array? You can view all the available attributes by typing `t1_data.` and then pressing `Tab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1 number of dimensions\n",
    "print(f\"T1w image dimensions: {t1_data.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594656c",
   "metadata": {},
   "source": [
    "**==================================================================================================**\n",
    "\n",
    "**EXCERCISE**\n",
    "\n",
    "What's the imensions of our BOLD image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dcb822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c326c",
   "metadata": {},
   "source": [
    "**==================================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How large each dimension is\n",
    "print(f\"T1w image shape is {t1_data.shape}\")\n",
    "print(f\"BOLD image shape is {bold_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006807d",
   "metadata": {},
   "source": [
    "The first 3 numbers given here represent the number of values along a respective dimension *(x,y,z)*. For the `BOLD` image this brain was scanned in `33` axial slices with a resolution of `64 x 64` voxels per slice. That means there are:\n",
    "\n",
    "`64 * 64 * 33 = 135,168` voxels in total. And the BOLD signal was sampled `208` times. \n",
    "\n",
    "Let’s see the type of data inside of the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b288d",
   "metadata": {},
   "source": [
    "\n",
    "How do we examine **what value a particular voxel is**? We can inspect the value of a voxel by selecting an index as follows:\n",
    "\n",
    "`data[x,y,z]`\n",
    "\n",
    "So for example we can inspect a voxel at coordinates `(20,60,50)` by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A value of a T1 image voxel at coordinates (20,60,50)\n",
    "t1_data[19, 59, 49]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff4423",
   "metadata": {},
   "source": [
    "**NOTE**: Python uses **zero-based indexing**. The first item in the array is item `0`. The second item is item `1`, the third is item `2`, etc.\n",
    "\n",
    "We can also extract data from a **slice** for visualisation and analysis. Slicing does exactly what it sounds like: from our 3D volume, we extract a 2D slice of the data. Below is an example of slicing from left to right (sagittal slicing, along the x-axis). In this case, we'll view the 20th slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1092e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of the T1 image's 20th sagittal slice\n",
    "x_slice = t1_data[19, :, :]\n",
    "print(x_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f027e9",
   "metadata": {},
   "source": [
    "This is similar to the indexing we did before to pull out a single voxel. However, instead of providing a value for each axis, the `:` indicates that we want to grab all values from that particular axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_slice = t1_data[:, :, 2]\n",
    "print(z_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a4170",
   "metadata": {},
   "source": [
    "We’ve been looking at voxel nummerical values, but we have no idea what the images actually look like! Let's look how the `100` slice of each of the `3` dimensions of T1 image look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe06aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [t1_data[99, :, :], t1_data[:, 99, :], t1_data[:, :, 99]]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(slices), figsize=(15,15))\n",
    "for i, slice in enumerate(slices):\n",
    "    axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine\n",
    "\n",
    "The final important piece of metadata associated with an image file is the **affine matrix**. The affine matrix defines the position of the image data in a reference space.\n",
    "\n",
    "A voxel coordinate by itself tells us very little about where the data originates within the scanner. For example, if we have the voxel coordinate (26, 30, 16), without additional information, we wouldn’t know if this position is on the left or right side of the brain, or if it came from the left or right of the scanner.\n",
    "\n",
    "This is because the scanner can collect voxel data in almost any arbitrary position and orientation within the magnet.\n",
    "\n",
    "For instance, BOLD images are typically acquired at a different angle and with smaller coverage than T1-weighted anatomical images, resulting in different bounding boxes. \n",
    "\n",
    "<img align=\"centre\" src=\"https://nipy.org/nibabel/_images/localizer.png\" width=\"50%\">\n",
    "\n",
    "Additionally, the center of the BOLD image data is often not located at the exact center of the magnet bore (the magnet’s *isocenter*).\n",
    "\n",
    "In our case, we have both an anatomical and a BOLD scan. Later, we will want to relate the data from the subject's _bold.nii.gz file to the same subject’s _T1w.nii.gz file. However, this is not straightforward, as the anatomical image and BOLD image were acquired with different orientations and fields of view, meaning the voxel coordinates in the BOLD image refer to different locations in the magnet compared to the anatomical image.\n",
    "\n",
    "We solve this by using the affine matrix, which keeps track of the relationship between voxel coordinates and a reference space (e.g., the magnet space). The affine array stores how voxel coordinates in the image data correspond to coordinates in the reference space. Knowing this relationship for both images allows us to align the voxel coordinates of the BOLD image to the spatially equivalent coordinates in the T1-weighted image.\n",
    "\n",
    "The origin of this reference system is at the magnet isocenter, at coordinate (0, 0, 0). The scanner’s axes, measured in mm, pass through this point. If the subject is lying face up, head first in the scanner, the axes are aligned with the subject’s head:\n",
    "\n",
    "* The scanner's left/right axis corresponds to the subject's left-**right** axis. \n",
    "* The scanner's floor/ceiling axis corresponds to subject's posterior-**anterior** axis.\n",
    "* The scanner's bore axid corresponds to the subject's inferior-**superior** axis..\n",
    "\n",
    "This subject-centered scanner coordinate system is commonly used in neuroimaging and is referred to as **scanner RAS+** (right, anterior, superior). The **+** indicates that the right, anterior, and superior directions are positive on these axes (while left, posterior, and inferior are negative). **Note**: **Right** refers to the subject’s **right** side.\"\n",
    "\n",
    "<img align=\"left\" src=\"https://people.cas.sc.edu/rorden/anatomy/tspace.gif\" width=\"30%\">\n",
    "\n",
    "<img align=\"right\" src=\"https://www.slicer.org/w/img_auth.php/2/22/Coordinate_sytems.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9604b9b",
   "metadata": {},
   "source": [
    "Below is the affine matrix for our anatomical `T1w` data. This matrix relates the **voxel coordinates** to the **world (scanner) coordinates** in **RAS+** space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f89517",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_affine = t1_img.affine\n",
    "print(t1_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fe4c9",
   "metadata": {},
   "source": [
    "In the image header, the different `sform_code` and `qform_code` values specify which RAS+ space the sform affine refers to, with these interpretations:\n",
    "\n",
    "| Code | Label     | Meaning                       |\n",
    "|------|-----------|--------------------------------|\n",
    "| 0    | unknown   | sform not defined              |\n",
    "| 1    | scanner   | RAS+ in scanner coordinates    |\n",
    "| 2    | aligned   | RAS+ aligned to some other scan|\n",
    "| 3    | talairach | RAS+ in Talairach atlas space  |\n",
    "| 4    | mni       | RAS+ in MNI atlas space        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1066f6",
   "metadata": {},
   "source": [
    "How 'shifted' is the T1 image's voxel space center from the reference space (scanner bore) center?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nibabel has a function apply_affine \n",
    "from nibabel.affines import apply_affine \n",
    "\n",
    "# the central voxel in the voxel space\n",
    "t1_vox_center = (np.array(t1_data.shape) - 1) / 2.\n",
    "print(f\"The central voxel in the voxel space is {t1_vox_center.astype(int)}\")\n",
    "\n",
    "# distance from the reference space centre (in mm)\n",
    "# voxel space's central voxel's location in the reference space\n",
    "t1_vox_center_in_scanner = apply_affine(t1_img.affine, t1_vox_center)\n",
    "print(f\"The voxel space central voxel in the scanner space is at {t1_vox_center_in_scanner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8b100",
   "metadata": {},
   "source": [
    "That means the center of the T1 image field of view is **4.1 mm to the right** from the isocenter of the magnet, **18.8 mm anterior** to the isocenter and **1.2 mm above** (superior) the isocenter.\n",
    "\n",
    "The parameters in the affine array can therefore give the position of any voxel coordinate, relative to the scanner RAS+ reference space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85ca1d",
   "metadata": {},
   "source": [
    "When we register an image to a template, such as the **MNI template**, we obtain an affine matrix that defines the relationship between the voxels in the aligned image and the MNI RAS+ space. In the MNI reference space, the origin `(0, 0, 0)` is located at the **anterior commissure**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image manipulation with `nilearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018b388",
   "metadata": {},
   "source": [
    "### The mean image\n",
    "\n",
    "If you're using NiBabel to compute the mean image, you first need to load the image, extract the data, and then compute the mean.\n",
    "\n",
    "With Nilearn, you can do all of this in a single line using the `mean_img function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710833f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img = nli.mean_img(bold_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = mean_img.get_fdata()\n",
    "mean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bff42",
   "metadata": {},
   "source": [
    "Nilearn also offers interactive visualisation option, which is a great alternative to NiBabel's orthoview() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a35934",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img(mean_img, bg_img=mean_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample image to a template\n",
    "\n",
    "Using `resample_to_img`, we can resample one image to match the dimensions of another. For example, let's resample an anatomical `T1` image to the dimensions of the `mean` image we computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image shapes before resampling\n",
    "print([mean_img.shape, t1_img.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling T1 to the mean Bold image\n",
    "resampled_t1 = nli.resample_to_img(t1_img, mean_img)\n",
    "\n",
    "# T1 image shape after resampling\n",
    "resampled_t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4ea91",
   "metadata": {},
   "source": [
    "How does the resampled `T1` image look like? Here we will use another `nilearn` plotting function that plots a static image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(t1_img, title = 'original t1', dim=-1)\n",
    "plotting.plot_anat(resampled_t1, title = 'resampled t1', dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth an image\n",
    "\n",
    "Using `smooth_img`, we can quickly smooth any type of MRI image. For example, let's take the mean image from above and apply smoothing with different FWHM values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fwhm in range(1, 12, 5):\n",
    "    smoothed_img = nli.smooth_img(mean_img, fwhm)\n",
    "    plotting.plot_epi(smoothed_img, title=\"Smoothing %imm\" % fwhm,\n",
    "                     display_mode='z', cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a time course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf01dea",
   "metadata": {},
   "source": [
    "Let's plot a time course of the central voxel in our BOLD imgage and some other random voxel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24961c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the xyz of the center \n",
    "bold_vox_center = (np.array(bold_data.shape) - 1) / 2.\n",
    "x, y, z, _ = bold_vox_center\n",
    "\n",
    "# set the plot size\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plot the central voxel time course\n",
    "plt.plot(bold_data[int(x), int(y), int(z), :])\n",
    "\n",
    "# plot some random voxel time course\n",
    "plt.plot(bold_data[28, 45, 15, :])\n",
    "\n",
    "# add legends to the plot\n",
    "plt.legend(['center voxel', 'random voxel']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909c13a",
   "metadata": {},
   "source": [
    "Alternatively, we can use Nilearn's *NiftiSpheresMasker* function, which allows us to extract time series from a single voxel or a sphere around it. The input coordinates, in this case, must be in **world coordinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the two previously used voxel coordinates to the world coordinates\n",
    "bold_center_coords = apply_affine(bold_img.affine, [x, y, z])\n",
    "print(f\"The center of the BOLD image in the world coordinates is {bold_center_coords}\")\n",
    "\n",
    "random_voxel_coords = apply_affine(bold_img.affine, [28, 45, 15])\n",
    "print(f\"The random voxel in the BOLD image in the world coordinates is {random_voxel_coords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb6212c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time series of the center and random voxels\n",
    "from nilearn.maskers import NiftiSpheresMasker\n",
    "\n",
    "coord_masker = NiftiSpheresMasker(\n",
    "    [bold_center_coords, random_voxel_coords], t_r=2\n",
    ")\n",
    "coord_time_series = coord_masker.fit_transform(bold_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series of the center and random voxels\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(coord_time_series)\n",
    "plt.legend(['center voxel', 'random voxel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250329e4",
   "metadata": {},
   "source": [
    "Let's take our BOLD functional image, compute its mean image, and apply a threshold to keep only the voxels with values higher than 95% of all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the mean image\n",
    "mean_img = nli.mean_img(bold_img)\n",
    "\n",
    "#keep voxels that have a value that is higher than 95% of all voxels\n",
    "thr = nli.threshold_img(mean_img, threshold='95%')\n",
    "\n",
    "#let's see how the thresholded image look compared to the original mean image\n",
    "plotting.view_img(thr, bg_img=mean_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mri)",
   "language": "python",
   "name": "mri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.933px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
