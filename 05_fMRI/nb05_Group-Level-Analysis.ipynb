{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Apšvalka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) \n",
    "- **Date:** August 2024  \n",
    "- **conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI Data Analysis: Group-Level Analysis\n",
    "\n",
    "Once you have beta (or contrast) maps for conditions (or contrasts) from all subjects, you can perform group-level statistics. Importantly, all subject first-level results need to be in common space, e.g., MNI, to perform voxel-wise group analyses. Group-level analysis allows you to make inferences about the population, rather than individual subjects, by assessing common patterns across participants. Common statistical methods for group-level analysis include one-sample or paired t-tests, as well as more complex mixed-effects models, depending on your study design.\n",
    "\n",
    "In this tutorial, we are adopting a **mixed-effects model** approach. We will incorporate the beta maps of all nine conditions into a single design matrix with subject-specific regressors. This approach can capture more of the explained variance by accounting for both condition effects and subject-level variability in one model. This differs from a one-sample t-test on first-level contrast estimates, where the model only assesses the variance in the contrast across subjects. The one-sample t-test simplifies the analysis but doesn't explicitly model within-subject variability or interactions across conditions, which can lead to a less comprehensive understanding of the underlying effects. The mixed-effects model approach handles both within- and between-subject variability, allowing for a more nuanced analysis of comparisons and potentially reducing unexplained variance.\n",
    "\n",
    "Mixed-effects models are often analysed with ANOVA, which Nilean's `second_level_model.compute_contrast` would do. However, there isn’t a straightforward way with Nilearn to check and account for non-sphericity in the data *(see [Rik’s Stats tutorial](../02_Statistics/cognestic_stats_python.ipynb) on ANOVA for the importance of this)*. To mitigate this issue, we can use Nilearn's non-parametric inference, which we will employ for our final results in this group-level analysis example.\n",
    "\n",
    "Here is a recommended viewing to help better understand the principles of the group-level analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('__cOYPifDWk', width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**   \n",
    "1. Import required packages and set up some stuff   \n",
    "2. Retrieve First-Level results   \n",
    "3. Displaying subject Effects-Of-Interest z-maps   \n",
    "4. Specify the second-level model  \n",
    "4.1. Design matrix   \n",
    "4.2. Contrasts   \n",
    "4.3. Model specification and fit \n",
    "5. Computing contrasts and plotting result maps\n",
    "5.1. False-positive-rate with cluster-forming threshold    \n",
    "5.2. FWE correction using Bonferroni correction \n",
    "5.3. FWE correction using non-parametric permutation testing    \n",
    "6. Summary results  \n",
    "6.1. Using atlasreader package    \n",
    "6.2. Nilearn's report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Import required packages and set up some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conda environment used for this tutorial is available here: https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml \n",
    "\n",
    "import os.path as op # for file path operations\n",
    "import glob # to search for files using regex\n",
    "\n",
    "import pandas as pd # for data manipulation\n",
    "import numpy as np # for numerical operations\n",
    "\n",
    "from bids.layout import BIDSLayout # to fetch data from BIDS-compliant datasets\n",
    "\n",
    "import matplotlib.pyplot as plt # for basic plotting\n",
    "\n",
    "import nibabel as nib # NiBabel, to read and write neuroimaging data, https://nipy.org/nibabel/\n",
    "\n",
    "# Nilearn modules, for the analysis of brain volumes, plotting, etc., https://nilearn.github.io/\n",
    "from nilearn.plotting import plot_glass_brain, plot_design_matrix, plot_contrast_matrix, plot_stat_map, view_img, view_img_on_surf\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.glm.thresholding import threshold_stats_img\n",
    "from nilearn.datasets import load_mni152_template\n",
    "from nilearn.glm.second_level import non_parametric_inference\n",
    "\n",
    "from atlasreader import create_output # For generating result tables https://github.com/miykael/atlasreader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNI152 template will be used as a backgound for plotting MNI-space ROIs\n",
    "mni152_template = load_mni152_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve First-Level results\n",
    "\n",
    "For the group analysis, we will use the single-condition contrast estimate (beta estimate) maps for all nine conditions. Because we saved the results in BIDS format, we can us PyBIDS to retrieve the subject-level results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up the paths to the data and results folders\n",
    "fmri_data_dir = 'FaceProcessing/data' # data in BIDS format\n",
    "fmri_results_dir = 'FaceProcessing/results' # results in BIDS format\n",
    "\n",
    "# --- Set up the BIDS layout\n",
    "layout = BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "\n",
    "# Attach the results folder to the layout. It must complay with BIDS standards. \n",
    "# And must include dataset_description.json file!\n",
    "layout.add_derivatives(op.join(fmri_results_dir, \"first-level\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Specify which conditions to include in the analysis and retrieve their effect files from the first-level results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['FAMOUS1', 'FAMOUS2dl', 'FAMOUS2im', 'UNFAMILIAR1', 'UNFAMILIAR2dl', 'UNFAMILIAR2im', 'SCRAMBLED1', 'SCRAMBLED2dl', 'SCRAMBLED2im']\n",
    "\n",
    "effect_files = layout.get(desc=conditions, suffix='effect', extension='.nii.gz', return_type='filename')\n",
    "\n",
    "# print to see if it found what we expexted\n",
    "print(f\"Found {len(effect_files)} effect files:\")\n",
    "print(*effect_files, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying subject Effects-Of-Interest z-maps\n",
    "\n",
    "To check how the first-level results look overall, it is helpful to display effects-of-interest for all subjects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoi_maps = layout.get(desc='EffectsOfInterest', extension='.nii.gz', return_type='file')\n",
    "print(*eoi_maps, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = layout.get_subjects()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(14, 14))\n",
    "\n",
    "for i, stat_map in enumerate(eoi_maps):\n",
    "    plot_glass_brain(stat_map, \n",
    "                              title = 'sub-' + subjects[i],\n",
    "                              axes = axes[int(i / 4), int(i % 4)],\n",
    "                              plot_abs = False, \n",
    "                              display_mode='x')\n",
    "fig.suptitle('Effects of interest' + ', unthresholded z-maps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the second-level model\n",
    "\n",
    "At the group-level analysis, we also use a GLM. The outcome variable is the beta/contrast estimate from each subject, and the predictor variables typically include group-level factors such as experimental conditions, subject-specific regressors (in repeated-measures designs), group-specific regressors (in between-subject designs), and subject-specific covariates (e.g., age, gender, or behavioural scores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design matrix\n",
    "\n",
    "In this example, the design matrix we generate will represent a mixed-effects design, incorporating both **within-subject (conditions)** and **between-subject (subjects)** factors. Each row in the design matrix corresponds to a specific observation, which in this case is a beta estimate from a given condition and subject, while each column represents a predictor variable.\n",
    "\n",
    "The number of rows in the design matrix must match the number of first-level result files that will be entered into the second-level model. The order of the rows in the design matrix must match the order of the provided files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['FAMOUS1', 'FAMOUS2dl', 'FAMOUS2im', 'UNFAMILIAR1', 'UNFAMILIAR2dl', 'UNFAMILIAR2im', 'SCRAMBLED1', 'SCRAMBLED2dl', 'SCRAMBLED2im']\n",
    "\n",
    "# Create an empty DataFrame\n",
    "df = pd.DataFrame(columns=conditions + subjects)\n",
    "\n",
    "# Populate the DataFrame with 0s and 1s\n",
    "for i, condition in enumerate(conditions):\n",
    "    # Filter files based on condition\n",
    "    condition_files = [1 if condition in file else 0 for file in effect_files]\n",
    "    # Add a column for the condition\n",
    "    df[condition] = condition_files\n",
    "\n",
    "# Populate the DataFrame with 0s and 1s for subjects\n",
    "for i, subject in enumerate(subjects):\n",
    "    # Filter files based on subject\n",
    "    subject_files = [1 if f\"sub-{subject}\" in file else 0 for file in effect_files]\n",
    "    # Add a column for the subject\n",
    "    df[subject] = subject_files\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "design_matrix = df\n",
    "design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_design_matrix(design_matrix)\n",
    "ax.set_title(\"Second level design matrix\", fontsize=12)\n",
    "ax.set_ylabel(\"stat maps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrasts\n",
    "\n",
    "For these group-level results, we are only interested in the statistics, not the contrast estimates, so scaling the contrasts is not strictly necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_columns = design_matrix.shape[1]\n",
    "\n",
    "contrasts = {\n",
    "  'Faces_Scrambled': np.pad([1, 1, 1, 1, 1, 1, -2, -2, -2], (0, n_columns - 9), 'constant'),\n",
    "  'Famous_Unfamiliar': np.pad([1, 1, 1, -1, -1, -1, 0, 0, 0], (0, n_columns - 9), 'constant')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for contrast_id, contrast_val in contrasts.items():\n",
    "    plot_contrast_matrix(contrast_val, design_matrix=design_matrix)\n",
    "    plt.suptitle(contrast_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model specification and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify\n",
    "second_level_model = SecondLevelModel() \n",
    "# fit\n",
    "second_level_model = second_level_model.fit(\n",
    "  effect_files, \n",
    "  design_matrix = design_matrix\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing contrasts and plotting result maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the z-maps for the contrast\n",
    "z_map = second_level_model.compute_contrast(\n",
    "  contrasts['Faces_Scrambled'], \n",
    "  output_type='z_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False-positive-rate with cluster-forming threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_map_fpr, threshold_fpr = threshold_stats_img(\n",
    "  z_map, \n",
    "  alpha=0.001, \n",
    "  height_control='fpr', \n",
    "  cluster_threshold=20,\n",
    "  two_sided=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Uncorrected p<.001 threshold: %.3f' % threshold_fpr)\n",
    "\n",
    "plot_stat_map(\n",
    "    thresholded_map_fpr,\n",
    "    bg_img = mni152_template, \n",
    "    threshold=threshold_fpr,   \n",
    "    display_mode = 'ortho',\n",
    "    black_bg = True,    \n",
    "    title = 'Faces > Scrambled  (p<.001, uncorrected, k=20)'\n",
    "    )\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plotting\n",
    "plot = view_img(\n",
    "  thresholded_map_fpr, \n",
    "  bg_img=mni152_template, \n",
    "  threshold=threshold_fpr, \n",
    "  colorbar=True, \n",
    "  title='Faces > Scrambled  (p < .001, uncorrected)'\n",
    "  )\n",
    "\n",
    "plot.open_in_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWE correction using Bonferroni correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_map_bonf, threshold_bonf = threshold_stats_img(\n",
    "  z_map, \n",
    "  alpha=0.05, \n",
    "  height_control='bonferroni', \n",
    "  cluster_threshold=20,\n",
    "  two_sided=False)\n",
    "\n",
    "print('Bonferroni p<.05 threshold: %.3f' % threshold_bonf)\n",
    "\n",
    "plot_stat_map(\n",
    "    thresholded_map_bonf, \n",
    "    bg_img = mni152_template,\n",
    "    threshold=threshold_bonf,   \n",
    "    display_mode = 'ortho',\n",
    "    black_bg = True,\n",
    "    cmap = 'hot',    \n",
    "    title = 'Faces > Scrambled  (Bonf. p<.05, k=20)'\n",
    "    )\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWE correction using non-parametric permutation testing\n",
    "\n",
    "Nilearn's FWE correction using the Bonferroni approach (`height_control='bonferroni'`) is applied to the number of voxels. However, this method is not well-suited for fMRI data because neuroimaging data typically exhibit spatially correlated data points, which violate the Bonferroni assumption of independent tests.\n",
    "\n",
    "As an alternative, neuroscientists have developed **Random Field Theory** (RFT), which accounts for spatial correlations by applying multiple comparison corrections that consider the smoothness of the data. Specifically, the correction is applied to the number of **'resels'** (RESolution ELements), rather than the raw number of voxels. However, it's important to note that this RFT-based approach is not implemented in Nilearn. At the group level, Nilearn provides an option for non-parametric inference with permutation testing, which is a more appropriate approach for fMRI data when accounting for the spatial correlation of voxels.\n",
    "\n",
    "*(More on non-parametric permutation testing, have a look at [Rik's Stats notebook](../02_Statistics/cognestic_stats_python.ipynb).)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = non_parametric_inference(\n",
    "    effect_files,\n",
    "    design_matrix = design_matrix,\n",
    "    second_level_contrast = contrasts['Faces_Scrambled'],\n",
    "    n_perm = 100, # ideally at least 10000\n",
    "    two_sided_test = False,\n",
    "    n_jobs = -1, # Use all available cores\n",
    "    threshold = 0.001 # cluster level threshold; enables cluster-level inference\n",
    ")\n",
    "\n",
    "# Print the keys of the output dictionary\n",
    "print(out_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is multiple images (maps), organised in a dictionary. \n",
    "* Voxel-level inference\n",
    "  * **t**: t-statistics\n",
    "  * **logp_max_t**: Negative log10 family-wise error rate-corrected p-values corrected based on the distribution of maximum t-statistics from permutations.\n",
    "* Cluster-level inference\n",
    "  * **size**: Cluster size values associated with the significance test \n",
    "  * **logp_max_size**: Negative log10 family-wise error rate-corrected p-values corrected based on the distribution of maximum cluster sizes from permutations.\n",
    "  * **mass**: Cluster mass values associated with the significance test \n",
    "  * **logp_max_mass**: Negative log10 family-wise error rate-corrected p-values corrected based on the distribution of maximum cluster masses from permutations. \n",
    "\n",
    "**We will focus only on the voxel-level inference.**\n",
    "\n",
    "To report the FWE-corrected maps, we could display the *logp_max_t*; however, these values can be difficult to interpret if you're not familiar with them. It might be better to plot and report a t-map, masked to exclude the voxels that did not survive the FWE correction.\n",
    "\n",
    "Let's create a new image displaying t-values for the voxels with a p-value < 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "masked = out_dict['logp_max_t'].get_fdata() > -np.log10(alpha)\n",
    "masked_t_map = out_dict['t'].get_fdata() * masked\n",
    "\n",
    "# save the masked t-map as a nifti image\n",
    "masked_t_map_img = nib.Nifti1Image(masked_t_map, out_dict['t'].affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the smallest t-value that is above the threshold (for the colorbar; the maps themselves are thresholded already)\n",
    "threshold_fwe = masked_t_map[masked_t_map > 0].min()\n",
    "print('FWE (perm.) p<.05 threshold: %.3f' % threshold_fwe)\n",
    "\n",
    "plot_stat_map(\n",
    "    masked_t_map_img, \n",
    "    threshold = threshold_fwe,       \n",
    "    display_mode = 'ortho',\n",
    "    black_bg = True,\n",
    "    bg_img = mni152_template,\n",
    "    cmap = 'hot',\n",
    "    title = f\"Faces > Scrambled (FWE p <.{alpha})\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we observe that the non-parametric FWE correction is slightly less conservative than the Bonferroni correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary results\n",
    "\n",
    "### Using atlasreader package\n",
    "\n",
    "We can use ['atlasreader'](https://github.com/miykael/atlasreader) package to get summary results (peak table, cluster table, .png images of each cluster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and save atlasreader output\n",
    "outdir = op.join(fmri_results_dir, 'group-level', 'permutation', 'FacesScrambled')\n",
    "\n",
    "create_output(\n",
    "    masked_t_map_img, \n",
    "    cluster_extent = 20, \n",
    "    voxel_thresh = threshold_fwe,\n",
    "    direction = 'pos',\n",
    "    outdir = outdir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the peak table\n",
    "peaks = glob.glob(op.join(outdir, '*_peaks.csv'))\n",
    "display(pd.read_csv(peaks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the cluster table\n",
    "clusters = glob.glob(op.join(outdir, '*_clusters.csv'))\n",
    "display(pd.read_csv(clusters[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some more plotting options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5 peaks' x values\n",
    "x = pd.read_csv(peaks[0])['peak_x'][:5]\n",
    "# sort the x values\n",
    "x = x.sort_values()\n",
    "\n",
    "# plot these peaks\n",
    "plot_stat_map(\n",
    "    masked_t_map_img, \n",
    "    threshold = threshold_fwe,       \n",
    "    display_mode = 'x',\n",
    "    cut_coords = x,\n",
    "    black_bg = True,\n",
    "    bg_img = mni152_template,\n",
    "    cmap = 'hot',\n",
    "    title = f\"Faces > Scrambled (FWE p <.{alpha})\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at a 3D brain using `plotly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = view_img_on_surf(masked_t_map_img, \n",
    "    threshold = threshold_fwe)\n",
    "#view.open_in_browser()\n",
    "view.resize(1600, 800)\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use, for example, FSLeyes to plot and explore the result maps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nilearn's report\n",
    "\n",
    "Nilearn has a built-in report generator that can create reports for all defined contrasts. However, a limitation is that it cannot generate reports for results obtained using non-parametric inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_report = second_level_model.generate_report(\n",
    "  contrasts, \n",
    "  title = \"Results of the second-level analysis\", \n",
    "  bg_img = mni152_template, \n",
    "  alpha = 0.001, \n",
    "  cluster_threshold = 20, \n",
    "  height_control = 'fpr', \n",
    "  min_distance = 8.0, \n",
    "  plot_type = 'slice', \n",
    "  display_mode = 'x', \n",
    "  report_dims = (1600, 800))\n",
    "\n",
    "second_level_report.open_in_browser()\n",
    "\n",
    "#second_level_report.save_as_html(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXCERCISE\n",
    "\n",
    "Perform a one-sample t-test on the first-level results of *Faces > Scrambled* contrast. How do the results compare to our mixed-effect model approach above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "337px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "874.85px",
    "left": "2183px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
