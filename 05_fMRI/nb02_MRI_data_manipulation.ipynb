{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Apšvalka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) \n",
    "- **Date:** August 2024  \n",
    "- **conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts.\n",
    "\n",
    "# MRI Data Manipulation With NiBabel and Nilearn\n",
    "\n",
    "In neuroimaging research, the ability to efficiently load, inspect, and manipulate MRI data is essential for both exploration and analysis. Python libraries such as [NiBabel](http://nipy.org/nibabel/) and [Nilearn](https://nilearn.github.io/stable/index.html) provide powerful tools for handling neuroimaging data in common formats like NIfTI. In this tutorial, we will explore how to use these libraries to perform fundamental operations on MRI data, including loading images, inspecting headers, working with affine transformations, and manipulating the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8a9a4",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "**Table of contents**\n",
    "1. Import required packages and set up the some stuff   \n",
    "2. Loading and inspecting images with NiBabel    \n",
    "2.1. Header  \n",
    "2.2. Data   \n",
    "2.3. Affine   \n",
    "1. Image manipulation with NiLearn   \n",
    "3.1. The mean image   \n",
    "3.2. Resample image to a template    \n",
    "3.3. Smooth an image   \n",
    "3.4. Plotting a time course    \n",
    "3.5. Masking an image\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267413c",
   "metadata": {},
   "source": [
    "## Import required packages and set up the some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import nilearn as nl\n",
    "\n",
    "from bids.layout import BIDSLayout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as plt\n",
    "\n",
    "import numpy as np\n",
    "# Set numpy to print 3 decimal points and suppress small values\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def4708",
   "metadata": {},
   "source": [
    "Throughout the examples, we will work with one subject's anatomical and functional images, which we will retrieve from our BIDS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data_dir = 'FaceProcessing/data'\n",
    "\n",
    "# Initialize the BIDS layout\n",
    "layout = BIDSLayout(fmri_data_dir)\n",
    "\n",
    "# Get subject's T1w image and all Bold images\n",
    "anat_file = layout.get(subject='04', extension='nii.gz', datatype='anat', return_type='filename')[0]\n",
    "bold_file = layout.get(subject='04', extension='nii.gz', suffix='bold', return_type='filename')[0]\n",
    "\n",
    "# Print the file names\n",
    "print(\"Subject's anatomical image\", anat_file)\n",
    "print(\"Subject's functional image\", bold_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and inspecting images with NiBabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921eb960",
   "metadata": {},
   "source": [
    "First, let's use the `load()` function to create a `NiBabel` image object from a NIfTI file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_image = nib.load(anat_file)\n",
    "bold_image = nib.load(bold_file)\n",
    "\n",
    "# Print the shape of both images\n",
    "print(f\"The shape of the anatomical image: {anat_image.shape}\")\n",
    "print(f\"The shape of the functional image: {bold_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9892b",
   "metadata": {},
   "source": [
    "Loading a NIfTI file with NiBabel provides us with a special type of data object that encapsulates all the information stored in the file. In Python, each piece of this information is referred to as an attribute. To explore all available attributes, type `anat_image.` followed by pressing `Tab`. Here, we’ll focus on three key attributes:\n",
    "- Header\n",
    "- Data\n",
    "- Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header\n",
    "\n",
    "Header contains important metadata about the image, including information such as the image dimensions, data type, voxel sizes, and spatial transformation matrices (qform and sform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_header = anat_image.header\n",
    "print(anat_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98a6ab",
   "metadata": {},
   "source": [
    "`anat_header` is a Python **dictionary**. Dictionaries are containers that hold pairs of objects - **keys** and **values**. \n",
    "We can access the value stored by a given key by typing: `anat_header['<key_name>']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anat_header['dim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f33963",
   "metadata": {},
   "source": [
    "We can also access some of the header values by `.get_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ffd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_header.get_data_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd1e5a",
   "metadata": {},
   "source": [
    "**==================================================================================================**\n",
    "\n",
    "**EXCERCISE**\n",
    "\n",
    "Extract `pixdim` value from the `BOLD image` header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a1c0e",
   "metadata": {},
   "source": [
    "**==================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "As demonstrated above, the header contains valuable metadata about the MRI data we’ve loaded. Next, we’ll move on to accessing the actual image data itself, which can be done using the `get_fdata()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc019565",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_data = anat_image.get_fdata()\n",
    "bold_data = bold_image.get_fdata()\n",
    "\n",
    "# How does the T1 data look like\n",
    "print(anat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521c638",
   "metadata": {},
   "source": [
    "The data is a **multidimensional array** representing the image.\n",
    "\n",
    "How can we check the number of dimensions in the anat_data array? To view all available attributes and methods, you can type `anat_data.` and press Tab. Specifically, you can use the `ndim` attribute to check the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"T1w image dimensions: {anat_data.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594656c",
   "metadata": {},
   "source": [
    "**==================================================================================================**\n",
    "\n",
    "**EXCERCISE**\n",
    "\n",
    "What's the imensions of our BOLD image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c326c",
   "metadata": {},
   "source": [
    "**==================================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How large each dimension is\n",
    "print(f\"T1w image shape is {anat_data.shape}\")\n",
    "print(f\"BOLD image shape is {bold_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006807d",
   "metadata": {},
   "source": [
    "The first three numbers represent the number of values along the respective dimensions (x, y, z). For the BOLD image, the brain was scanned in 33 axial slices with a resolution of 64 x 64 voxels per slice. This results in: 64 * 64 * 33 = 135,168 voxels in total. Additionally, the BOLD signal was sampled 210 times.\n",
    "\n",
    "Now, let’s examine the type of data inside the array.\n",
    "\n",
    "***Note:** Python uses **zero-based** indexing, which means that the first voxel in each dimension starts at index 0. So, when inspecting voxel coordinates, make sure to account for this.*\n",
    "\n",
    "To inspect the value of a particular voxel, you can access it by selecting its index using the following syntax: `data[x, y, z]`. For example, to inspect the voxel at coordinates (20, 60, 50) in zero-based indexing (which corresponds to the 21st, 61st, and 51st voxel), you can use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_value = anat_data[20, 60, 50]\n",
    "print(f\"Voxel value at (20, 60, 50) is {voxel_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff4423",
   "metadata": {},
   "source": [
    "We can also extract data from a **slice** for visualisation and analysis. Slicing does exactly what it sounds like: from our 3D volume, we extract a 2D slice of the data. Below is an example of slicing from left to right (sagittal slicing, along the x-axis). In this case, we'll view the 20th slice.\n",
    "\n",
    "\n",
    "We can also extract data from a **slice** for visualisation and analysis. Slicing works as the name suggests: it allows us to extract a 2D slice from our 3D volume. For example, we can slice along the x-axis to obtain sagittal views of the brain. Below is an example of extracting a slice. In this case, we'll view the 20th slice (keeping in mind Python's zero-based indexing, this corresponds to the 21st slice in the volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1092e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_slice = anat_data[20, :, :]\n",
    "print(x_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a4170",
   "metadata": {},
   "source": [
    "We’ve been examining numerical voxel values, but we haven't yet visualised how the actual images look. Let's take a look at the 100th slice from each of the three dimensions of the T1 image: axial (z-axis), coronal (y-axis), and sagittal (x-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b006ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = {\n",
    "  'sagital slice': anat_data[100, :, :], \n",
    "  'coronal slice': anat_data[:, 100, :], \n",
    "  'axial slice': anat_data[:, :, 100]}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(slices), figsize=(15,15))\n",
    "\n",
    "for i, slice in enumerate(slices.values()):\n",
    "    axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[i].set_title(list(slices.keys())[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine\n",
    "\n",
    "The final important piece of metadata associated with an image file is the **affine matrix**, a 4x4 matrix which defines how the voxel grid of the image is positioned, oriented, and scaled relative to the scanner or reference space. Without the affine matrix, the voxel coordinates are just abstract numbers. With it, you know how the image data aligns with the real world, enabling comparisons and transformations between different images or scans.\n",
    "\n",
    "For example, if we have the voxel coordinate (26, 30, 16), without additional information, we wouldn’t know if this position is on the left or right side of the brain, or if it came from the left or right of the scanner. This is because the scanner can collect voxel data in almost any arbitrary position and orientation within the magnet.\n",
    "\n",
    "For example, BOLD images are typically acquired at a different angle and with smaller coverage than T1-weighted anatomical images, resulting in different bounding boxes. \n",
    "\n",
    "<img align=\"centre\" src=\"https://nipy.org/nibabel/_images/localizer.png\" width=\"50%\">\n",
    "\n",
    "Additionally, the center of the image data is often not located at the exact center of the magnet bore (the magnet’s *isocenter*).\n",
    "\n",
    "In our case, we have both an anatomical and a BOLD scan. Later, we will want to relate the data from the subject's *_bold.nii.gz* file to the same subject’s *_T1w.nii.gz file*. However, this is not straightforward, as the anatomical image and BOLD image were acquired with different orientations and fields of view, meaning the voxel coordinates in the BOLD image refer to different locations in the magnet compared to the anatomical image.\n",
    "\n",
    "Rhe affine matrix solves this as it keeps track of the relationship between voxel coordinates and a reference space (e.g., the magnet space). Knowing this relationship for both images allows us to align the voxel coordinates of the BOLD image to the spatially equivalent coordinates in the natomical image.\n",
    "\n",
    "The origin of the scanner's reference system is at the magnet isocenter, at coordinate (0, 0, 0). The scanner’s axes, measured in mm, pass through this point. If the subject is lying face up, head first in the scanner, the axes are aligned with the subject’s head:\n",
    "\n",
    "* The scanner's left/right axis corresponds to the subject's left-**right** axis (x). \n",
    "* The scanner's floor/ceiling axis corresponds to subject's posterior-**anterior** axis (y).\n",
    "* The scanner's bore axid corresponds to the subject's inferior-**superior** axis (z).\n",
    "\n",
    "<img align=\"left\" src=\"https://people.cas.sc.edu/rorden/anatomy/tspace.gif\" width=\"30%\">\n",
    "\n",
    "<img align=\"right\" src=\"https://www.slicer.org/w/img_auth.php/2/22/Coordinate_sytems.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5862fc8",
   "metadata": {},
   "source": [
    "Below is the affine matrix for our anatomical data. This matrix relates the **voxel coordinates** to the **world (scanner) coordinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f89517",
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_affine = anat_image.affine\n",
    "print(anat_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fe4c9",
   "metadata": {},
   "source": [
    "In the image header, the different `sform_code` and `qform_code` values specify which space the sform affine refers to, with these interpretations:\n",
    "\n",
    "| Code | Label     | Meaning                       |\n",
    "|------|-----------|--------------------------------|\n",
    "| 0    | unknown   | sform not defined              |\n",
    "| 1    | scanner   | scanner coordinates    |\n",
    "| 2    | aligned   | aligned to some other scan|\n",
    "| 3    | talairach | Talairach atlas space  |\n",
    "| 4    | mni       | MNI atlas space        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1066f6",
   "metadata": {},
   "source": [
    "Let’s measure how 'shifted' the center of the anatomical image's voxel space is from the reference space (scanner bore) center. To do this, we can extract the affine matrix from the anatomical image and calculate the physical coordinates of the center voxel, comparing them with the origin of the scanner's reference space (0, 0, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc374d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the image data (dimensions of the voxel grid)\n",
    "data_shape = anat_image.shape\n",
    "\n",
    "# Calculate the voxel space center (the middle voxel in each dimension)\n",
    "voxel_center = np.array(data_shape) / 2\n",
    "\n",
    "# Convert voxel center to reference (scanner) space using the affine matrix\n",
    "center_in_scanner_space = anat_affine.dot(np.append(voxel_center, 1))[:3]  # Applying affine transformation\n",
    "\n",
    "# The scanner bore center is at (0, 0, 0)\n",
    "scanner_bore_center = np.array([0, 0, 0])\n",
    "\n",
    "# Calculate the shift (distance between voxel space center and scanner bore center)\n",
    "shift = center_in_scanner_space - scanner_bore_center\n",
    "\n",
    "print(f\"Shift of the anatomical image's voxel center from the scanner bore center: {shift} mm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01146b0",
   "metadata": {},
   "source": [
    "NiBabel also provides a built-in function, `apply_affine`, which allows us to apply the affine transformation in a more straightforward way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nibabel.affines import apply_affine \n",
    "\n",
    "shift = apply_affine(anat_image.affine, voxel_center)\n",
    "\n",
    "print(f\"Shift of the anatomical image's voxel center from the scanner bore center: {shift} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc055d5",
   "metadata": {},
   "source": [
    "This means that the center of the anatomical image's field of view is **4.6 mm to the right** of the magnet's isocenter, **19.3 mm anterior**, and **1.7 mm above** (superior) the isocenter.\n",
    "\n",
    "The parameters in the affine matrix allow us to determine the position of any voxel coordinate relative to the scanner's reference space.\n",
    "\n",
    "When we register an image to a template, such as the **MNI template**, we obtain an affine matrix that defines the relationship between the voxel coordinates of the aligned image and the MNI space. In MNI space, the origin (0, 0, 0) is located at the **anterior commissure**.\n",
    "\n",
    "When working with MNI-aligned images, it is often necessary to translate between voxel coordinates and MNI space coordinates. For example, we may need to retrieve the value of a voxel at a specific set of MNI coordinates. \n",
    "\n",
    "To **convert voxel coordinates into MNI space coordinates**, you can apply the affine matrix of the registered image:\n",
    "\n",
    "```python\n",
    "# Specify voxel coordinates to translate (e.g., voxel at (30, 40, 50))\n",
    "voxel_coords = np.array([30, 40, 50])\n",
    "\n",
    "# Convert voxel coordinates to MNI space using the affine matrix\n",
    "mni_coords = apply_affine(image_affine, voxel_coords)\n",
    "```\n",
    "\n",
    "To **translate MNI space coordinates back to voxel coordinates**, you need to apply the inverse of the affine matrix:\n",
    "\n",
    "```python\n",
    "# Specify MNI coordinates (e.g., MNI coordinates at [10, 20, -15])\n",
    "mni_coords = np.array([10, 20, -15])\n",
    "\n",
    "# Calculate the inverse of the affine matrix\n",
    "affine_inv = np.linalg.inv(image_affine)\n",
    "\n",
    "# Convert MNI coordinates to voxel coordinates using the inverse affine\n",
    "voxel_coords = apply_affine(affine_inv, mni_coords)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image manipulation with NiLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13e2b7",
   "metadata": {},
   "source": [
    "### The mean image\n",
    "\n",
    "If you're using NiBabel to compute the mean image, you first need to load the image, extract the data, and then compute the mean manually.\n",
    "\n",
    "However, with Nilearn, you can achieve the same result in a single line using the `mean_img` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn.image \n",
    "bold_mean_image = nl.image.mean_img(bold_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_mean_data = bold_mean_image.get_fdata()\n",
    "bold_mean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bff42",
   "metadata": {},
   "source": [
    "Nilearn also offers interactive visualisation option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c421cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn.plotting\n",
    "\n",
    "nl.plotting.view_img(\n",
    "  bold_mean_image, \n",
    "  bg_img = bold_mean_image\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample image to a template\n",
    "\n",
    "Using the `resample_to_img` function from Nilearn, we can resample one image to match the dimensions of another. For example, let’s resample an anatomical T1 image to match the dimensions of the mean image we computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59923d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image shapes before resampling\n",
    "print([bold_mean_image.shape, anat_image.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_anat = nl.image.resample_to_img(anat_image, bold_mean_image)\n",
    "\n",
    "# T1 image shape after resampling\n",
    "resampled_anat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4ea91",
   "metadata": {},
   "source": [
    "How does the resampled anatomical image look? To visualize it, we will use Nilearn’s `plot_anat` function, which allows us to plot a static image of the resampled anatomical image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf113e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nl.plotting.plot_anat(anat_image, title = 'original t1', dim=-1)\n",
    "nl.plotting.plot_anat(resampled_anat, title = 'resampled t1', dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth an image\n",
    "\n",
    "Using the `smooth_img` function from Nilearn, we can easily apply smoothing to any type of MRI image. For example, let’s take the mean image from above and apply smoothing with different full width at half maximum (FWHM) values to see how smoothing affects the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4aaea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fwhm in range(1, 12, 5):\n",
    "  \n",
    "    smoothed_img = nl.image.smooth_img(bold_mean_image, fwhm)\n",
    "    \n",
    "    nl.plotting.plot_epi(\n",
    "      smoothed_img, \n",
    "      title = \"Smoothing %imm\" % fwhm,\n",
    "      display_mode = 'z', \n",
    "      cmap = 'magma'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a time course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9421e",
   "metadata": {},
   "source": [
    "Let's plot a time course of the central voxel in our BOLD imgage and some other random voxel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24961c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the xyz of the center \n",
    "bold_vox_center = np.array(bold_data.shape) / 2.\n",
    "x, y, z, _ = bold_vox_center\n",
    "\n",
    "# set the plot size\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plot the central voxel time course\n",
    "plt.plot(bold_data[int(x), int(y), int(z), :])\n",
    "\n",
    "# plot some random voxel time course\n",
    "plt.plot(bold_data[28, 45, 15, :])\n",
    "\n",
    "# add legends to the plot\n",
    "plt.legend(['center voxel', 'random voxel']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909c13a",
   "metadata": {},
   "source": [
    "Alternatively, we can use Nilearn's `NiftiSpheresMasker` function, which allows us to extract time series from a single voxel or a sphere around it. The input coordinates, in this case, must be in **world coordinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the two previously used voxel coordinates to the world coordinates\n",
    "\n",
    "bold_center_coords = apply_affine(bold_image.affine, [x, y, z])\n",
    "print(f\"The center of the BOLD image in the world coordinates is {bold_center_coords}\")\n",
    "\n",
    "random_voxel_coords = apply_affine(bold_image.affine, [28, 45, 15])\n",
    "print(f\"The random voxel of the BOLD image in the world coordinates is {random_voxel_coords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb6212c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time series of the center and random voxel using NiftiSpheresMasker\n",
    "\n",
    "from nilearn.maskers import NiftiSpheresMasker\n",
    "\n",
    "coord_masker = NiftiSpheresMasker(\n",
    "    [bold_center_coords, random_voxel_coords], t_r=2\n",
    ")\n",
    "coord_time_series = coord_masker.fit_transform(bold_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series of the center and random voxels\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(coord_time_series)\n",
    "plt.legend(['center voxel', 'random voxel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a6814",
   "metadata": {},
   "source": [
    "Let's take our BOLD functional image, compute its mean image, and apply a threshold to keep only the voxels with values higher than 95% of all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the mean image\n",
    "bold_mean_image = nl.image.mean_img(bold_image)\n",
    "\n",
    "# keep voxels that have a value that is higher than 95% of all voxels\n",
    "thr = nl.image.threshold_img(bold_mean_image, threshold='95%')\n",
    "\n",
    "# let's see how the thresholded image look compared to the original mean image\n",
    "nl.plotting.view_img(\n",
    "  thr, \n",
    "  bg_img = bold_mean_image\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294b7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.933px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
