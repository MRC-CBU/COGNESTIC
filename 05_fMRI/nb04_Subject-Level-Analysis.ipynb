{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Ap≈°valka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) \n",
    "- **Date:** August 2024  \n",
    "- **conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI Data Analysis: Subject-Level Analysis\n",
    "\n",
    "Once the data have been pre-processed, we can proceed to the analysis. In this tutorial, we will focus on subject-level (or first-level) fMRI data analysis using [Nilearn](https://nilearn.github.io/stable/index.html). This process involves constructing a design matrix, fitting the model to the data, and computing beta maps (i.e., the estimated response for each condition) or contrast maps. Subject-level analysis is a crucial step before progressing to group-level analysis.\n",
    "\n",
    "For analysing fMRI data, we commonly use general linear model (GLM). A GLM in fMRI analysis has the **BOLD signal** as the **outcome** variable (derived from the pre-processed functional MRI images) and **predictor** variables such as **events** (e.g., task conditions) and **confounds** (e.g., motion parameters). Typically analysis is performed by constructing a separate model for each voxel - **a mass univariate approach**. \n",
    "\n",
    "For more information on GLM, please see [Rik's Stats tutorial](../02_Statistics/cognestic_stats_python.ipynb) (corresponding [web link](https://github.com/MRC-CBU/COGNESTIC/tree/main/02_Statistics)).\n",
    "\n",
    "And here are some recommended short videos to help better understand the principles of fMRI analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"width: 1200px; margin: 0; display: flex; justify-content: space-around;\">\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>GLM applied to fMRI (11 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/OyLKMb9FNhg\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>Conditions and contrasts (12 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7MibM1ATai4\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<div style=\"width: 1200px; margin: 0; display: flex; justify-content: space-around;\">\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>Nuisance variables (14 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DEtwsFdFwYc\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>Multiple Comparisons (9 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AalIM9-5-Pk\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('''\n",
    "<div style=\"width: 1200px; margin: 0; display: flex; justify-content: space-around;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>GLM applied to fMRI (11 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/OyLKMb9FNhg\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>Conditions and contrasts (12 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7MibM1ATai4\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"width: 1200px; margin: 0; display: flex; justify-content: space-around;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>Nuisance variables (14 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DEtwsFdFwYc\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>Multiple Comparisons (9 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AalIM9-5-Pk\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "</div>\n",
    "'''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "**Table of contents**    \n",
    "1. Import required packages   \n",
    "2. Retrieve the preprocessed (fMRIprep) data   \n",
    "3. Setting up GLM model components   \n",
    "3.1. Dependent variable (Y): BOLD signal from each voxel of the functional MRI images    \n",
    "3.2. Predictors (X): Events    \n",
    "3.3. Predictors: Confounds    \n",
    "4. Performing the GLM analysis    \n",
    "4.1. Creating the First Level Model  \n",
    "4.2. Fitting the model   \n",
    "4.3. Inspecting the Design Matrix    \n",
    "5. Contrast specification    \n",
    "5.1. A simple case contrast    \n",
    "5.2. Contrast scaling    \n",
    "5.3. The final contrasts for our model    \n",
    "6. Computing contrasts and plotting result maps    \n",
    "6.1. Control the False Positive Rate    \n",
    "6.2. False Discovery Rate (FDR) correction    \n",
    "6.3. Family Wise Error (FWE) correction    \n",
    "6.4. Cluster threshold    \n",
    "7. The impact of first-level model parameters    \n",
    "7.1. Model 1: Glover's HRF    \n",
    "7.2. Model 2: SPM's HRF    \n",
    "7.3. Plotting both models    \n",
    "8. First Level for multiple subjects    \n",
    "8.1. A generic first-level analysis script    \n",
    "8.2. Processing multiple subjects in paralel   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conda environment used for this tutorial is available here: https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml \n",
    "\n",
    "import pandas as pd # for data manipulation\n",
    "import numpy as np # for numerical operations\n",
    "\n",
    "import matplotlib.pyplot as plt # for basic plotting\n",
    "\n",
    "from bids.layout import BIDSLayout # to fetch data from BIDS-compliant datasets\n",
    "\n",
    "# Nilearn modules, for the analysis of brain volumes, plotting, etc., https://nilearn.github.io/\n",
    "from nilearn.glm.first_level import FirstLevelModel, compute_regressor, glover_hrf, spm_hrf\n",
    "from nilearn.glm.thresholding import threshold_stats_img\n",
    "from nilearn.plotting import plot_design_matrix, plot_contrast_matrix, plot_stat_map, plot_roi\n",
    "from nilearn.maskers import NiftiSpheresMasker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the preprocessed (fMRIprep) data\n",
    "\n",
    "BIDS applications, such as `fMRIprep`, output data into a data structure similar to `BIDS` organization principals. And these data also can be inspected using [PyBIDS](https://bids-standard.github.io/pybids/index.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up the paths to the BIDS data directory which includes derivatives\n",
    "fmri_data_dir = 'FaceProcessing/data' # BIDS format data directory\n",
    "\n",
    "# --- Set up the BIDS layoutt and include the derivatives in it\n",
    "layout = BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "\n",
    "# or to include only a specific derivatives folder\n",
    "# layout = BIDSLayout(fmri_data_dir, derivatives = 'path/to/derivatives')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GLM model components\n",
    "\n",
    "Remember (or refer to [Rik's stats](../02_Statistics/cognestic_stats_python.ipynb)) that GLM typically expressed as **Y = XB + e**, where **Y** is a vector containing the data, **X** is the 'design matrix' that defines the model for the data, **B** (or 'Betas') is a vector of parameters to be estimated, and **e** represents random error.\n",
    "\n",
    "In the next sections, we will set up our GLM model by specifying the following:\n",
    "* **The data (Y)**, which are our pre-processed functional images. These contain the time series data (length n; 210 in our case) for each voxel.\n",
    "* **The design matrix (X)**, where we specify the effects (predictors) that may have influenced the acquired signal in the data. At a minimum, these effects include our experimental conditions, which we specify through event files containing the names and onsets of the conditions. Additionally, we can include other predictors, such as confounding (nuisance) variables, which are known to have occurred and likely influenced the data. Including these can help explain the acquired signal. We get a large list of confounding variables from the fMRIPrep output (confound files). The size of the design matrix must be n x p (time series length x number of predictors). There should be more data points (n) than predictors (p), so be cautious not to include too many confounding variables.\n",
    "\n",
    "Nilearn will use the specified inputs for the GLM analysis and compute the beta estimates for each predictor. We can then define contrasts to compare beta estimates between conditions‚Äîfor example, to test whether seeing Faces had a greater effect on the signal than seeing Scrambled images.\n",
    "\n",
    "Let's set up our GLM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent variable (Y): BOLD signal from each voxel of the functional MRI images\n",
    "\n",
    "We need to specify which MRI images we want to analyse. Here, we will focus on a single subject's **9 functional runs**. Using PyBIDS, we can easily locate the preprocessed files required for the analysis. Let's retrieve the preprocessed functional image files for **subject sub-04**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sID = '04'\n",
    "\n",
    "preproc_functional_files = layout.get(\n",
    "    subject = sID, \n",
    "    datatype = 'func', \n",
    "    desc = 'preproc', \n",
    "    extension = '.nii.gz',\n",
    "    return_type = 'filename'\n",
    ")\n",
    "\n",
    "print('\\nSubject''s', sID, 'preprocessed functional images:')\n",
    "print(*preproc_functional_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also specify the subject's anatomical image (warped to the standard space) to use it as a background image when plotting results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_anatomical_file = layout.get(\n",
    "    subject = sID, \n",
    "    datatype = 'anat', \n",
    "    space = 'MNI152NLin2009cAsym', \n",
    "    desc = 'preproc', \n",
    "    extension = '.nii.gz',\n",
    "    return_type ='filename'\n",
    ")[0]\n",
    "\n",
    "print('Subject''s', sID, 'preprocessed anatomical image:')\n",
    "print(preprocessed_anatomical_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functional images contain time series data for each voxel. The GLM will be fitted, and beta estimates will be computed for each voxel.\n",
    "\n",
    "Let's extract the time series of one voxel from the first run to get a sense of how our dependent variable looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel of interest\n",
    "MNI_coord = [(41,\t-48,\t-18)] \n",
    "\n",
    "# extract the time series \n",
    "coord_masker = NiftiSpheresMasker(\n",
    "    MNI_coord, t_r=2, standardize='zscore'\n",
    ")\n",
    "coord_time_series = coord_masker.fit_transform(\n",
    "  preproc_functional_files[0]\n",
    "  )\n",
    "\n",
    "# plot the time series  \n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.plot(coord_time_series, color='grey')\n",
    "\n",
    "plt.title(f\"run-01 {MNI_coord[0]} voxel time series\")\n",
    "plt.xlabel('Time (TRs)')\n",
    "plt.ylabel('BOLD signal (z-scored)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BOLD signal in this voxel during the 7-minute functional run comes from various sources. We want to determine whether our experimental conditions (viewing Famous, Unfamiliar, or Scrambled faces) contribute to this signal, and if so, by how much. To better estimate this, we aim to regress out other potential contributors that we are not interested in, such as head movement artifacts, scanner drift, and other noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors (X): Events\n",
    "\n",
    "Next, we need to specify the events that occurred during the functional acquisitions. The event files are stored in the `data/func` folder, and again, we can use `PyBIDS` to locate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_files = layout.get(\n",
    "    subject = sID, \n",
    "    datatype = 'func', \n",
    "    suffix = 'events', \n",
    "    extension = \".tsv\", \n",
    "    return_type = 'filename'\n",
    ")\n",
    "print('Subject''s', sID, 'event files:')\n",
    "print(*event_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event files contain the names of our experimental conditions, their onset times (in seconds), and their durations (in seconds). The events table will have as many rows as there are trials you want to include in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and display one of the event files\n",
    "events_run1 = pd.read_csv(event_files[0], sep='\\t')\n",
    "events_run1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the GLM's design matrix, we include one regressor (the same length as the time series data points) for each condition. The regressor represents the predicted BOLD response to the stimulus. The BOLD signal does not simply switch from 0 to 1 (on or off); instead, it follows a specific shape known as the **haemodynamic response function (HRF)**. The HRF models the delayed and dispersed nature of the BOLD signal in response to neural activity, typically peaking a few seconds after the stimulus and then gradually returning to baseline.\n",
    "\n",
    "To obtain our condition's predicted BOLD response, we convolve the onset and duration times of the condition with an HRF. Several HRF models have been derived from research, with two of the most commonly used in typical populations being Glover's HRF and SPM's HRF.\n",
    "\n",
    "To see how it works, let's create a regressor for the **FAMOUS_1** condition. We'll use the condition's onset and duration times from the event file and convolve them with Glover's HRF. Nilearn has a built-in function for this ([glm.first_level.compute_regressor](https://nilearn.github.io/stable/modules/generated/nilearn.glm.first_level.compute_regressor.html)), so we will use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter events_run1 DataFrame for the 'FAMOUS_1' condition\n",
    "famous1_events = events_run1[events_run1['trial_type'] == 'FAMOUS_1']\n",
    "\n",
    "# Extract onsets and durations for the 'FAMOUS_1' condition\n",
    "onsets_famous1 = famous1_events['onset'].values\n",
    "durations_famous1 = famous1_events['duration'].values\n",
    "# Set amplitudes to 1 for all events\n",
    "amplitudes_famous1 = np.ones_like(onsets_famous1)  \n",
    "\n",
    "# Create a list of frame times (number of data points x TR, sampled every TR)\n",
    "TR = 2\n",
    "frame_times = np.arange(0, 210*TR, TR) \n",
    "\n",
    "# Create the regressor for 'FAMOUS_1' condition\n",
    "regressor_famous1, _ = compute_regressor(\n",
    "  [onsets_famous1, durations_famous1, amplitudes_famous1],\n",
    "  hrf_model='glover', \n",
    "  frame_times=frame_times\n",
    ")\n",
    "\n",
    "# plot the regressor and also show the onsets\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.plot(frame_times, regressor_famous1)\n",
    "plt.plot(onsets_famous1, np.zeros_like(onsets_famous1), 'o')\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (a.u.)')\n",
    "plt.legend(['Predicted BOLD response', 'Stimulus onset'])\n",
    "plt.title('Predicted BOLD response for the FAMOUS_1 condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Nilearn, we don‚Äôt need to manually create these regressors when setting up the first-level model; we only need to provide the event files and ensure they contain the '*onset*', '*duration*', and '*trial_type*' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors: Confounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confounds represent fluctuations with a potential non-neuronal origin, such as head motion artifacts, scanner noise, and cardiac or respiratory effects. We can include them in the GLM as regressors of no interest to minimize the confounding effects of non-neuronal signals. fMRIPrep calculates a wide range of possible confounds. You can find detailed information in the [fMRIPrep documentation](https://fmriprep.org/en/stable/outputs.html#confounds).\n",
    "\n",
    "Which confounding variables you include in the GLM depends on the analysis you want to perform. The most well-established confounds are the six head-motion parameters (three rotations and three translations). If your dataset includes physiological recordings (e.g., heart rate, respiration), you might consider including these as confounds. If not, you might include PCA components from CompCor, a method implemented in fMRIPrep that identifies noise components from CSF and white matter signals. **However, please read the fMRIPrep documentation on confounds carefully before including any additional confounds, and never include all confounds in the GLM design matrix!**\n",
    "\n",
    "If your study has specific considerations (e.g., participant behavior or task-related noise), you may need to include additional custom confounds.\n",
    "\n",
    "The confounds computed by `fMRIPrep` can be found in the `data/derivatives/fmriprep/{sub}/func/` directory. They are stored separately for each run in `.tsv` files, with one column for each confound variable. Let's use `PyBIDS` to locate these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confound_files = layout.get(\n",
    "    subject = sID, \n",
    "    datatype = 'func', \n",
    "    desc = 'confounds', \n",
    "    extension = \".tsv\", \n",
    "    return_type = 'filename'\n",
    ")\n",
    "\n",
    "print('Subject''s', sID, 'confound files:')\n",
    "print(*confound_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of all confounds of the first functional run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_run1 = pd.read_table(confound_files[0])\n",
    "\n",
    "print('Number of confounds in run 1:', len(list(confounds_run1)))\n",
    "print('Number of data points for ech confound:', len(confounds_run1), '\\n')\n",
    "#print(*list(confounds_run1), sep='\\n')\n",
    "confounds_run1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will include only the most commonly used confounds‚Äîthe **six motion parameters**.\n",
    "\n",
    "Additionally, we need to include **a regressor for each dummy scan** (we had two in this case).\n",
    "\n",
    "**! Important**: If your data contains dummy scans and you specified this when running fMRIPrep, dummy scan confounds (non_steady_state_outlier##) will also be generated, and you must include them in your GLM! This ensures that the dummy volumes do not contribute to the parameter estimates.\n",
    "\n",
    "Since we had two dummy scans, we will include the two '*non_steady_state_outliers*' in our design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_of_interest = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z', \n",
    "                         'non_steady_state_outlier00', 'non_steady_state_outlier01']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the confounds of interest of the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_run1[confounds_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of interest, we can use the `fit_transform` function again, which we previously used to extract an example voxel's time series, but this time, we'll include the confounds to regress them out from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This we used to extact the original time series:\n",
    "# coord_masker = NiftiSpheresMasker(\n",
    "#     MNI_coord, t_r=2, standardize='zscore'\n",
    "# )\n",
    "\n",
    "# coord_time_series = coord_masker.fit_transform(\n",
    "#   preproc_functional_files[0]\n",
    "#   )\n",
    "\n",
    "# Now we will extract the time series with the confounds of interest regressed out\n",
    "coord_time_series_no_confounds = coord_masker.fit_transform(\n",
    "  preproc_functional_files[0], \n",
    "  confounds = confounds_run1[confounds_of_interest]\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the time series with and without confounds\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.plot(coord_time_series, label='original time series', color='blue', linewidth=2)\n",
    "plt.plot(coord_time_series_no_confounds, label='time series without confounds', color='orange', linestyle='--', linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.title(f\"run-01 {MNI_coord[0]} voxel time series with and without confounds\")\n",
    "plt.xlabel('Time (TRs)')\n",
    "plt.ylabel('BOLD signal (z-scored)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to select the confounds of interest from the other eight runs as well. We will include the confounds of interest from each run in our GLM model by creating a list of confound tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_for_glm = []\n",
    "for conf_file in confound_files:\n",
    "    this_conf = pd.read_table(conf_file)\n",
    "    conf_subset = this_conf[confounds_of_interest].fillna(0) # replace NaN with 0\n",
    "    confounds_for_glm.append(conf_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the GLM analysis\n",
    "\n",
    "Now we have the BOLD images, event files, and confounds for each of the 9 functional runs. Nilearn will analyse them together and **compute fixed-effects statistics for this subject**.\n",
    "\n",
    "Next, we will create and estimate a `FirstLevelModel` object, which **will generate our design matrix based on the inputs we have provided**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the First Level Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify the model with the parameters of our choice (see the [documentation](https://nilearn.github.io/stable/modules/generated/nilearn.glm.first_level.FirstLevelModel.html) for a full list of parameters). Here we will specify the folowing:\n",
    "* **t_r** - repetition time (the sampling interval of the functional runs) in seconds. We can get it from our BIDS layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the TR\n",
    "TR = layout.get_tr()\n",
    "print('TR:', TR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **slice_time_ref**: *This parameter indicates the time of the reference slice used in the slice timing preprocessing step of the experimental runs. It is expressed as a percentage of the t_r (time repetition), so it can have values between 0. and 1. Default=0.* \n",
    "  \n",
    "  We can find this information in our fMRIPrep Methods (*data/derivatives/fmriprep/logs/CITATION.html*). There we read: *\"BOLD runs were slice-time corrected to 0.974s (0.5 of slice acquisition range 0s-1.95s)\"*. This means, that **0.5** is the value we need to use for this parameter. \n",
    "  \n",
    "  Alternativel, and perhaps preferably, we can obtain this information from the BIDS derivatives metadata. It also tells if the slice-time correction was performed at all. \n",
    "\n",
    "  `*data\\derivatives\\fmriprep\\sub-{sID}\\func\\sub-{sID}_task-facerecognition_run-{runID}_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.json*`\n",
    "  ```json\n",
    "    {\n",
    "      \"RepetitionTime\": 2,\n",
    "      \"SkullStripped\": false,\n",
    "      \"SliceTimingCorrected\": true,\n",
    "      \"StartTime\": 0.974,\n",
    "      \"TaskName\": \"faceprocessing\"\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata indicates that slice-time correction was applied, and the start time was adjusted from 0 to 0.974‚Äîrepresenting the middle of the acquisition range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If slice timing correction was applied, get the slice time reference\n",
    "slice_timing = layout.get_metadata(preproc_functional_files[0])\n",
    "if slice_timing['SliceTimingCorrected']:\n",
    "  slice_time_ref = slice_timing['StartTime'] / TR\n",
    "  print('Slice timing reference:', slice_time_ref)\n",
    "else:\n",
    "  slice_time_ref = 0\n",
    "  print('Slice timing correction was not applied')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **hrf_model**: defines the HRF model to be used. Possible options:\n",
    "  * `spm`: This is the HRF model used in SPM. \n",
    "  * `spm + derivative`: SPM model plus its time derivative. This gives 2 regressors.\n",
    "  * `spm + derivative + dispersion`: Idem, plus dispersion derivative. This gives 3 regressors.\n",
    "  * `glover`: This corresponds to the Glover HRF.\n",
    "  * `glover + derivative`: The Glover HRF + time derivative. This gives 2 regressors. \n",
    "  * `glover + derivative + dispersion`: Idem, plus dispersion derivative. This gives 3 regressors. \n",
    "  * `fir`: Finite impulse response basis. This is a set of delayed dirac models.\n",
    "\n",
    "It can also be a custom model. In this case, a function should be provided for each regressor.\n",
    "  \n",
    "The choice of the HRF model is up to the user. There's little difference between using the SPM or Glover model. It's advisable to include derivatives when there's some uncertainty in the timing information, as this can help identify timing problems more effectively.\n",
    "\n",
    "Let's see how, for example, SPM and Glover HRFs compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeled time in seconds\n",
    "time_length = 32\n",
    "\n",
    "# Generate the HRF timecourses\n",
    "glover_timecourse = glover_hrf(TR, time_length=time_length)\n",
    "spm_timecourse = spm_hrf(TR, time_length=time_length)\n",
    "\n",
    "# Plot the timecourses\n",
    "timepoints = np.linspace(0, time_length, num=len(glover_timecourse))\n",
    "plt.plot(timepoints, glover_timecourse, label='Glover HRF', color='blue')\n",
    "plt.plot(timepoints, spm_timecourse, label='SPM HRF', color='red')\n",
    "plt.title('Comparison of Glover and SPM HRF Timecourses')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('HRF Amplitude (a.u.)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **drift_model**: Specifies the desired drift model for the design matrices. It can be '*polynomial*', '*cosine*', or *None*. The default is '*cosine*'. Additional nuisance regressors are added to the design matrix to account for low-frequency noise (such as scanner drift or physiological fluctuations) that are unlikely to come from the experimental conditions. \n",
    "* **high_pass**: Used only if drift_model is *'cosine'*. Default is 0.01 Hz (1/128 Hz). It defines the cutoff frequency for the cosine drift model, and cosine functions with frequencies lower than this threshold are added as drift regressors. This ensures that only higher-frequency components, typically associated with neural activity, remain unmodeled in the design matrix.\n",
    "* **smoothing_fwhm**: the full-width at half maximum in millimeters of the spatial smoothing to apply to the signal (smoothing was not done in fMRIPrep!).\n",
    "* **noise_model**: {‚Äòar1‚Äô, ‚Äòols‚Äô} The temporal variance model. Default=‚Äôar1‚Äô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_model = FirstLevelModel(\n",
    "    t_r = TR,\n",
    "    slice_time_ref = slice_time_ref,\n",
    "    smoothing_fwhm = 6, # a rule of thumb is 3x the voxel size\n",
    "    hrf_model = 'glover', # default\n",
    "    drift_model = 'cosine', # default\n",
    "    high_pass = 0.01, # default; Used only if drift_model is ‚Äòcosine‚Äô to set its cutoff frequency\n",
    "    noise_model = 'ar1', # default\n",
    "    memory = 'FaceProcessing/scratch', # kernel kept dying without this; Path to the directory used to cache the masking process and the glm fit.\n",
    "    n_jobs = 2 # use two CPUs for computation\n",
    "   # mask_img = path_to_mask_image, # default is None and Nilearn will compute it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "Now that we have specified the model, we can run it on our data. We need to include the list of our functional image files, the list of event timing files, and the list of our confound tables (one per run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_model = first_level_model.fit(preproc_functional_files, event_files, confounds_for_glm)\n",
    "\n",
    "# If the kernel keeps dying here, then fit only the first two runs\n",
    "#first_level_model = first_level_model.fit(preproc_functional_files[:2], event_files[:2], confounds_for_glm[:2])\n",
    "\n",
    "# Remove the cashed directory\n",
    "!rm -rf FaceProcessing/scratch/joblib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the cashed directory\n",
    "!rm -rf FaceProcessing/scratch/joblib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Design Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the design matrix of our GLM model (rows represent time, and columns contain the predictors).\n",
    "\n",
    "The `design_matrices` is a list of 9 tables (one per run). Let's look at the first run's design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the design matrices from the glm model\n",
    "design_matrices = first_level_model.design_matrices_\n",
    "\n",
    "print('Design matrix for run', 1)\n",
    "plot_design_matrix(design_matrices[0], output_file=None)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that one of the non-steady-state volumes 'spoils the image'. It's a good thing we are excluding them from our analysis.\n",
    "\n",
    "However, for visualisation purposes, let's exclude the two dummy regressors from our design matrix plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the first two rows of the design matrix\n",
    "plot_design_matrix(design_matrices[0].iloc[2:], output_file=None)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs also take a look at the data included in the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the design matrix of the first run\n",
    "design_matrices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the values for FAMOUS_1 are somewhat different from the values in the event regressor `regressor_famous1` that we calculated earlier. This is because the onset time of the events is not 0, but 0.974, as you may have also noticed. **Why is that?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the design matrix, we can observe several drift regressors. These are generated from the `drift_model` ('*cosine*') that we specified for our GLM model, capturing frequencies lower than the threshold set by the `high_pass` parameter. These regressors account for slow oscillations, which are unlikely to arise from our experimental conditions.\n",
    "\n",
    "Now, let's plot the event regressors alongside the low-frequency drift regressors to better visualise these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = design_matrices[0]\n",
    "event_regressors = dm.columns[:9]\n",
    "drift_regressors = [col for col in dm.columns if col.startswith('drift_')]\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot the predicted event responses\n",
    "for event in event_regressors:\n",
    "    plt.plot(dm.index, dm[event], lw=1)\n",
    "\n",
    "# Plot the modeled noise\n",
    "for drift in drift_regressors:\n",
    "    plt.plot(dm.index, dm[drift], color='grey', lw=1, linestyle='--')\n",
    "    \n",
    "plt.title('run-01 predicted event responses and modeled noise')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (a.u.)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the low-frequency drifts differ in shape and timing from the event-related signals. This helps illustrate how the GLM separates task-related neural activity from unrelated noise.\n",
    "\n",
    "Ideally, the event regressors should not overlap much with the drift regressors. It's not a problem in our event-relighted design, but could be more problematic in slow block-designs. Significant overlap could suggest that some task-related variance is being captured by the noise model, reducing the sensitivity to detect neural signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the `fit_transform` function on our original example voxel's time series data. We will fit the nuisance regressors (motion and drift parameters, as well as dummy volumes) to the time series to remove the variance associated with these noise sources from the data.\n",
    "\n",
    "Next, let's plot this denoised time series against our predicted event responses. This will help visualise how well the model fits the data after accounting for nuisance factors. It provides an indication of how accurately the GLM captures the relationship between the experimental conditions and brain activity. A strong correspondence between the predicted responses and the denoised time series would suggest that the beta estimates are capturing meaningful signals related to the experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuiseance_regressors = design_matrices[0].columns[9:-1]\n",
    "print('Nuisance regressors:', nuiseance_regressors)\n",
    "\n",
    "# extract the time series with the nuiseance regressors regressed out                                                                   \n",
    "coord_time_series_denoised = coord_masker.fit_transform(\n",
    "  preproc_functional_files[0], \n",
    "  confounds = design_matrices[0][nuiseance_regressors]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the regressors along the bold time series\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.plot(dm.index, coord_time_series_denoised, label='denoised BOLD signal', linewidth=2)\n",
    "\n",
    "# Plot the predicted event responses\n",
    "for event in event_regressors:\n",
    "    zscored_event = (dm[event] - dm[event].mean()) / dm[event].std()\n",
    "    plt.plot(dm.index, zscored_event, \n",
    "             color='orange', lw=1, alpha=0.8)\n",
    "    \n",
    "plt.plot([],[], label=\"predicted event response\", color='orange', lw=1, alpha=0.8)\n",
    "\n",
    "plt.title(f\"run-01 {MNI_coord[0]} voxel denoised BOLD signal and predictors\")\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('z-scored values')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the actual beta estimates, we need to specify contrasts. But before that, let's look at another output contained in the `first_level_model`‚Äîthe estimated mask image. In our model specification, we didn‚Äôt specify a mask, although we could have, such as a grey matter mask to perform the GLM on grey matter voxels only, or the subject's functional brain mask from the fMRIPrep output. Since we didn‚Äôt provide one, Nilearn generated it automatically. Let‚Äôs take a look at how sensible it is and, for example, how it compares to the fMRIPrep mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nilearn's generated mask\n",
    "glm_mask = first_level_model.masker_.mask_img_\n",
    "\n",
    "# Subjects brain mask from fmriprep\n",
    "fmriprep_run1_mask = layout.get(subject = sID, datatype = 'func', suffix = 'mask', extension = '.nii.gz')[0]\n",
    "\n",
    "# Plot the Nilearn's mask as a red overlay on the subject's anatomical image\n",
    "glm_mask_plot = plot_roi(\n",
    "  glm_mask, \n",
    "  bg_img=preprocessed_anatomical_file, \n",
    "  title=\"Nilearn's generated mask (red) and fMRIPrep's generated mask (yellow)\", \n",
    "  display_mode='ortho',\n",
    "  cmap='Set1', alpha=0.5)\n",
    "\n",
    "# Add the fmriprep mask as a yellow contour\n",
    "glm_mask_plot.add_contours(fmriprep_run1_mask, levels=[0.2], colors='y')\n",
    "\n",
    "# Note that the GLM mask is computed from all runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast specification\n",
    "\n",
    "A contrast is a linear combination of the beta estimates that allows us to test specific hypotheses about the experimental conditions. Each condition or regressor in the model is assigned a weight in the contrast, and these weights are combined to produce a contrast estimate.\n",
    "\n",
    "For example, to **compare two conditions** and test whether the mean activation in Condition A is greater than in Condition B, the contrast vector would be `c = [1, -1, 0, ...]`. Here, the 1 corresponds to Condition A, and the -1 corresponds to Condition B, indicating that we are comparing the two conditions. Zeros correspond to other conditions or regressors that are not part of the contrast. The resulting contrast estimate tests the difference between the beta estimates for these two conditions.\n",
    "\n",
    "You can also test **combinations of conditions** by assigning equal weights to multiple conditions. For example, to test whether the mean activation across two conditions (A and B) is greater than that in a third condition (C), the contrast vector would be `[0.5, 0.5, -1, 0, ...]`. This reflects an average of Conditions A and B compared to Condition C.\n",
    "\n",
    "If you are interested in testing **the effect of a single condition** against the baseline, the contrast would be `[1, 0, 0, ...]`, where the 1 refers to the condition of interest, and the zeros correspond to other conditions or regressors that are not part of the contrast.\n",
    "\n",
    "An **effects of interest** contrast is often used to assess the overall impact of all conditions or predictors of interest in the model. For effects of interest, a matrix-type contrast is used. This allows testing whether all conditions of interest, taken together, explain a significant amount of variance in the data. For example, if you have three conditions (A, B, C) and want to test whether they collectively explain significant variance, your matrix-type contrast would be:\n",
    "\n",
    " $c = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    " This type of contrast is commonly used in omnibus tests to determine whether any of the conditions have an effect, similar to an ANOVA in traditional statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple case contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns to know how to set up the contrasts\n",
    "design_matrices[0].head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A contrast must have the same length as the number of predictors in the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple case contrast\n",
    "\n",
    "n_columns = design_matrices[0].shape[1]  # Number of predictors in the model\n",
    "\n",
    "contrasts = {\n",
    "    'Faces_Scrambled': np.pad([1, 1, 1, -2, -2, -2, 1, 1, 1], (0, n_columns - 9), 'constant'),\n",
    "    'Famous_Unfamiliar': np.pad([1, 1, 1, 0, 0, 0, -1, -1, -1], (0, n_columns - 9), 'constant'),\n",
    "    'EffectsOfInterest': np.eye(n_columns)[:9]  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the contrasts\n",
    "for key, values in contrasts.items():\n",
    "    plot_contrast_matrix(values, design_matrix=design_matrices[0])\n",
    "    plt.suptitle(key)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrast scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, a GLM contrast is a linear combination of predictor beta coefficients, where each beta represents the weight or contribution of a given predictor to the overall model.\n",
    "\n",
    "For example, if we have conditions (predictors): Famous, Unfamiliar, and Scrambled, a contrast estimating Faces vs. Scrambled with weights `[1, 1, -2]` would calculate Famous + Unfamiliar - 2 * Scrambled. This can be interpreted as the sum of the responses to Famous and Unfamiliar faces, minus twice the response to Scrambled faces. However, what we typically want is the average response to Famous and Unfamiliar faces minus the response to Scrambled faces. To achieve this, we would need to scale the contrast as follows: `[0.5, 0.5, -1]`.\n",
    "\n",
    "Scaling contrast weights affects the magnitude of the resulting contrast estimate, but not the t-values or p-values, so the statistical inference remains unchanged. However, contrast estimates that reflect averages rather than sums are generally more meaningful for interpretation.\n",
    "\n",
    "If we want to obtain beta estimates for each condition separately (e.g., a beta estimate for Famous, a beta estimate for Unfamiliar, etc.), and we have multiple functional runs, we need to scale the contrast to account for the number of runs to get the average beta across them. When scaled appropriately, we can report the result as a *'beta estimate'* rather than just a *'contrast estimate'*, since scaling allows the contrast to represent the average of the beta estimates. This average can then be interpreted directly as a beta estimate for the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A contrast scaling function, which scales for the number of conditions and runs\n",
    "\n",
    "def scale_contrast(contrast_vector, nruns):\n",
    "    positive_sum = np.sum(contrast_vector[contrast_vector > 0])  # Sum of positive weights\n",
    "    negative_sum = np.abs(np.sum(contrast_vector[contrast_vector < 0]))  # Sum of negative weights (absolute)\n",
    "    \n",
    "    # Scale each part of the vector separately\n",
    "    scaled_vector = np.zeros_like(contrast_vector, dtype=float)\n",
    "    scaled_vector[contrast_vector > 0] = contrast_vector[contrast_vector > 0] / (positive_sum * nruns)\n",
    "    scaled_vector[contrast_vector < 0] = contrast_vector[contrast_vector < 0] / (negative_sum * nruns)\n",
    "    \n",
    "    return scaled_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_contrast = np.array([1, 1, 1, -2, -2, -2, 1, 1, 1])\n",
    "print('\\n Original contrast:\\n', example_contrast)\n",
    "print('\\n Scaled contrast for one run:\\n', scale_contrast(example_contrast, 1))\n",
    "print('\\n Scaled contrast for nine runs:\\n', scale_contrast(example_contrast, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final contrasts for our model\n",
    "\n",
    "In our model, we have 9 runs. If we have the same number of regressors, in the same order, in each run, we can specify the contrast for one run, and Nilearn will automatically reuse it for the other runs. However, to be cautious, it's advisable to define contrasts for each run separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled contrasts for comparing conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nruns = len(design_matrices)  # Number of runs\n",
    "\n",
    "comparison_contrast_list = []\n",
    "\n",
    "for design_matrix in design_matrices:\n",
    "  n_columns = design_matrix.shape[1]  # number of predictors\n",
    "\n",
    "  # Define the contrasts and scale them\n",
    "  contrasts = {\n",
    "      'Faces_Scrambled': scale_contrast(np.pad([1, 1, 1, -2, -2, -2, 1, 1, 1], (0, n_columns - 9), 'constant'), nruns),\n",
    "      'Famous_Unfamiliar': scale_contrast(np.pad([1, 1, 1, 0, 0, 0, -1, -1, -1], (0, n_columns - 9), 'constant'), nruns),\n",
    "      'EffectsOfInterest': np.eye(n_columns)[:9]  # No scaling needed here\n",
    "      }\n",
    "  \n",
    "  comparison_contrast_list.append(contrasts)\n",
    "\n",
    "len(comparison_contrast_list) # Should be one per run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled contrasts for each condition of interest\n",
    "\n",
    "Single-condition contrasts are useful for obtaining condition-specific beta estimates for further analysis. For example, in [group-level analysis](nb05_Group-Level-Analysis.ipynb), subject-level single-condition contrasts allow for more flexibility in combining or comparing conditions at the group level. Additionally, for [region-of interest (ROI) analysis](nb06_ROI_analysis.ipynb), single-condition beta estimates are often necessary to assess the specific effects of each condition within a defined region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contrasts for all unique conditions\n",
    "events_df = pd.read_table(event_files[0])\n",
    "unique_conditions = events_df['trial_type'].unique()\n",
    "# sort the conditions\n",
    "unique_conditions.sort() # don't neccessarily need to sort them\n",
    "\n",
    "basic_contrast_list = [] # we will append these to the condition-comparing contrasts created above\n",
    "\n",
    "for design_matrix in design_matrices:\n",
    "    n_columns = design_matrix.shape[1]  # number of predictors\n",
    "    column_names = design_matrix.columns  # get the names of the columns\n",
    "    \n",
    "    # Create an empty dictionary to store contrasts for all conditions\n",
    "    contrasts = {}\n",
    "    \n",
    "    for condition in unique_conditions:\n",
    "        # Initialize the contrast vector with zeros\n",
    "        contrast_vector = np.zeros(n_columns)\n",
    "        \n",
    "        # Assign 1 to the columns that correspond to the current condition\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            if col_name.startswith(condition):\n",
    "                contrast_vector[i] = 1/nruns # Scale the contrast for the number of runs\n",
    "        \n",
    "        # Store the contrast for the current condition\n",
    "        contrasts[condition] = contrast_vector    \n",
    "    \n",
    "    # Append all contrasts for this design matrix\n",
    "    basic_contrast_list.append(contrasts)\n",
    "    \n",
    "len(basic_contrast_list) # Should be one per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in comparison_contrast_list[0].items():\n",
    "    plot_contrast_matrix(values, design_matrix=design_matrices[0])\n",
    "    plt.suptitle(key)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in basic_contrast_list[0].items():\n",
    "    plot_contrast_matrix(values, design_matrix=design_matrices[0])\n",
    "    plt.suptitle(key)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing contrasts and plotting result maps\n",
    "\n",
    "With Nilearn, you can compute the `effect size` maps, `t-statistics` maps, `z-scores` and some other types. See the [documentation](https://nilearn.github.io/dev/modules/generated/nilearn.glm.Contrast.html) for more information.\n",
    "\n",
    "'Effect size' maps correspond to the 'beta' (or contrast estimate) maps. These can be used, for example, to plot effect sizes (beta estimates) in ROI analysis (see examples in the [ROI analysis notebook](nb06_ROI_analysis.ipynb)), or as inputs for [group-level analysis](nb05_Group-Level-Analysis.ipynb). \n",
    "\n",
    "In the case of a single-subject analysis, let's look at z-scored maps for **Faces > Scrambled contrast**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map = first_level_model.compute_contrast(\n",
    "    [c['Faces_Scrambled'] for c in comparison_contrast_list], \n",
    "    output_type = 'z_score'\n",
    ")\n",
    "\n",
    "# You can save the z_map to a file\n",
    "# z_map.to_filename('/path/to/output/z_map.nii.gz')"
   ]
  },
  {
   "attachments": {
    "4ba452b1-b52e-4070-85ad-02f09539dd29.webp": {
     "image/webp": "UklGRmpMAABXRUJQVlA4IF5MAADwwgGdASqeAm0BPm00lEcpKKIhq3OZaSANiWlukVa3sdOVAx41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\nx4CCE8vMn5GjFIa1mARjlwAAAA==\n"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fMRI analyses involve thousands of voxel-wise comparisons, which increases the risk of false positives‚Äîfamously illustrated by the case of the [\"Dead Salmon\"](https://teenspecies.github.io/pdfs/NeuralCorrelates.pdf).\n",
    "\n",
    "![fmri-salmon.webp](attachment:4ba452b1-b52e-4070-85ad-02f09539dd29.webp)\n",
    "\n",
    "Applying a threshold, particularly one that corrects for multiple comparisons (e.g., using Family-Wise Error (FWE) or False Discovery Rate (FDR) corrections), helps ensure that the displayed activations are statistically reliable. For more information on multiple comparisons correction, see [Rik‚Äôs Stats tutorial](../02_Statistics/cognestic_stats_python.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the False Positive Rate\n",
    "\n",
    "A minimum recommendation in the fMRI analysis is to control the **False Positive Rate** (FPR) at a specific threshold, such as p < .001. This is referred to as 'uncorrected' because it does not account for the number of multiple comparisons, but it can still help reduce false positives to a reasonable level when applied conservatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the statistical threshold\n",
    "_, threshold_fpr = threshold_stats_img(\n",
    "    z_map, \n",
    "    alpha = .001, \n",
    "    height_control = 'fpr'\n",
    ")\n",
    "\n",
    "print('Uncorrected p<.001 threshold: %.3f' % threshold_fpr)\n",
    "\n",
    "# plot the thresholded map\n",
    "plot_stat_map(\n",
    "    z_map, \n",
    "    bg_img = preprocessed_anatomical_file, \n",
    "    threshold = threshold_fpr,\n",
    "    cut_coords=MNI_coord[0], # let's center the plot on our previously used example voxel\n",
    "    display_mode = 'ortho', \n",
    "    black_bg = True,\n",
    "    title = 'Faces > Scrambled  (p<.001, uncorrected)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Discovery Rate (FDR) correction\n",
    "\n",
    "A commonly used alternative to controlling the false positive rate is the **False Discovery Rate** (FDR) method, which addresses the expected proportion of false discoveries among all detected activations. However, in neuroimaging research, the appropriateness of FDR remains a topic of debate. While it is often preferred in exploratory or hypothesis-generating studies due to its balance between discovery and error control, it may be less suitable for confirmatory studies where stricter error control is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, threshold_fdr = threshold_stats_img(\n",
    "    z_map, \n",
    "    alpha = .05, \n",
    "    height_control = 'fdr'\n",
    ")\n",
    "\n",
    "print('FDR, p<.05 threshold: %.3f' % threshold_fdr)\n",
    "\n",
    "plot_stat_map(\n",
    "    z_map, \n",
    "    bg_img = preprocessed_anatomical_file, \n",
    "    threshold = threshold_fdr,\n",
    "    cut_coords=MNI_coord[0], # let's center the plot on our previously used example voxel\n",
    "    display_mode = 'ortho', \n",
    "    black_bg = True,\n",
    "    title = 'Faces > Scrambled  (p<.05, FDR corrected)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family Wise Error (FWE) correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more conservative approach is to control the **Family-Wise Error** (FWE) rate, which represents the probability of making one or more false discoveries, or Type I errors, across all tests. This rate is typically set at 5%. The **Bonferroni correction** is a commonly used method to achieve this level of control and is also used in Nilearn. \n",
    "\n",
    "In Nilearn, Bonferroni correction is applied to the number of voxels. The Bonferroni correction, when applied directly to the number of voxels, is not appropriate for fMRI data. This is because neuroimaging data typically have spatially correlated data points, which violate the Bonferroni assumption of independent tests. As an alternative, neuroscientists have developed **Random Field Theory** (RFT). This method accounts for the spatial correlation by applying multiple comparison correction in a way that considers the smoothness of the data. Specifically, the correction is applied to the number of '***resels'*** (RESolution ELements), rather than the raw number of voxels. However, it's important to note that this **RFT-based approach is not implemented in Nilearn**. \n",
    "At the second-level analysis, Nilearn provides an option for non-parametric inference with permutation testing, which is a more suitable approach for fMRI data when considering the spatial correlation of voxels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, threshold_fwe = threshold_stats_img(\n",
    "    z_map, \n",
    "    alpha = .05, \n",
    "    height_control = 'bonferroni')\n",
    "\n",
    "print('Bonferroni-corrected, p<.05 threshold: %.3f' % threshold_fwe)\n",
    "\n",
    "plot_stat_map(\n",
    "    z_map, \n",
    "    bg_img = preprocessed_anatomical_file, \n",
    "    threshold = threshold_fwe,\n",
    "    cut_coords=MNI_coord[0], # let's center the plot on our previously used example voxel\n",
    "    display_mode = 'ortho', \n",
    "    black_bg = True,\n",
    "    title = 'Faces > Scrambled  (p<.05, FWE corrected)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a common practice to discard isolated voxels from the images. It is possible to generate a thresholded map with small clusters removed by providing a `cluster_threshold` argument. Here clusters smaller than `20` voxels will be discarded from the `fpr` corrected map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_map_fpr, threshold_fpr = threshold_stats_img(\n",
    "    z_map, \n",
    "    alpha = .001, \n",
    "    height_control='fpr', \n",
    "    cluster_threshold = 20\n",
    ")\n",
    "\n",
    "plot_stat_map(\n",
    "    thresholded_map_fpr, \n",
    "    bg_img = preprocessed_anatomical_file, \n",
    "    threshold = threshold_fpr,\n",
    "    cut_coords=MNI_coord[0], # let's center the plot on our previously used example voxel\n",
    "    display_mode = 'ortho', \n",
    "    black_bg = True, \n",
    "    colorbar = True,\n",
    "    title = 'Faces > Scrambled (p<.001, uncorrected, k=20'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The impact of first-level model parameters\n",
    "\n",
    "See the Nilearn tutorial for more examples on how the first_level parameters impact the results: https://nilearn.github.io/stable/auto_examples/04_glm_first_level/plot_first_level_details.html\n",
    "\n",
    "Here we will just compare our original model where we used Glover's HRF to a model SPM's HRF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Glover's HRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fmri_glm = FirstLevelModel(\n",
    "    t_r = TR,\n",
    "    slice_time_ref = slice_time_ref,\n",
    "    smoothing_fwhm = 6,\n",
    "    hrf_model = 'glover',\n",
    "    drift_model = 'cosine', \n",
    "    high_pass = 0.01, \n",
    "    noise_model = 'ar1',\n",
    "    memory = 'FaceProcessing/scratch',\n",
    "    n_jobs = 2\n",
    ")\n",
    "\n",
    "fmri_glm = fmri_glm.fit(bold, events, confounds_glm)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_z_map = z_map\n",
    "model1_title = 'Glover HRF model, Faces > Scrambled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: SPM's HRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm2 = FirstLevelModel(\n",
    "    t_r = TR,\n",
    "    slice_time_ref = slice_time_ref,\n",
    "    smoothing_fwhm = 6,\n",
    "    hrf_model = 'spm',\n",
    "    drift_model = 'cosine', \n",
    "    high_pass = 0.01, \n",
    "    noise_model = 'ar1',\n",
    "    memory = 'FaceProcessing/scratch', \n",
    "    n_jobs = 2\n",
    ")\n",
    "\n",
    "fmri_glm2 = fmri_glm2.fit(preproc_functional_files, event_files, confounds_for_glm)\n",
    "\n",
    "# fmri_glm2 = fmri_glm2.fit(preproc_functional_files[:2], event_files[:2], confounds_for_glm[:2])\n",
    "\n",
    "# Remove the cashed directory\n",
    "\n",
    "!rm -rf FaceProcessing/scratch/joblib/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has the same regressors as model1, therefore we can use our original (model1) contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_z_map = fmri_glm2.compute_contrast(\n",
    "  [c['Faces_Scrambled'] for c in comparison_contrast_list],\n",
    "  output_type='z_score')\n",
    "\n",
    "model2_title = 'SPM HRF model, Faces > Scrambled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_maps = [model1_z_map, model2_z_map]\n",
    "titles = [model1_title, model2_title]\n",
    "\n",
    "for (z_map, title) in zip(z_maps, titles):\n",
    "    _, threshold = threshold_stats_img(z_map, alpha = .001, height_control = 'fpr')\n",
    "    plot_stat_map(\n",
    "      z_map, \n",
    "      bg_img = preprocessed_anatomical_file, \n",
    "      threshold = threshold,\n",
    "      display_mode = 'ortho', \n",
    "      cut_coords=MNI_coord[0], # let's center the plot on our previously used example voxel\n",
    "      black_bg = True,\n",
    "      title = title\n",
    "      )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Level for multiple subjects\n",
    "\n",
    "### A generic first-level analysis script\n",
    "\n",
    "Using a notebook like this is an effective method for preparing the final analysis script. Now, we can combine all the elements discussed into a unified, generic script that will conduct the first-level analysis.\n",
    "\n",
    "**Example of a generic first-level script**: [code-examples/first_level_script.py](code-examples/first_level_script.py)\n",
    "\n",
    "**Things to note:**\n",
    "* We are saving the result files in BIDS format so that they can be easily queried with PyBIDS for further analysis.\n",
    "* We are generating a *dataset_description.json* file for the result model, which is required to make the results directory BIDS-compatible. This file will also contain our model parameters, which is useful for understanding how the results were obtained.\n",
    "\n",
    "**Main steps:**\n",
    "1. Specify the input and output paths and the subject's ID. The input should be the BIDS dataset, which contains the derivatives folder with fmriprep's preprocessed data.\n",
    "2. Specify the subject's preprocessed functional, event, and confound files.\n",
    "3. Specify the confounds of interest.\n",
    "4. Specify the first-level model parameters and fit the model.\n",
    "5. Specify the contrasts (in this example, the contrasts include Faces > Scrambled, Effects of Interest, and single contrasts for each of the nine conditions).\n",
    "6. Compute the contrasts and save the results in BIDS format.\n",
    "7. Create the model's *dataset_description.json* file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# ======================================================================\n",
    "# Dace Ap≈°valka (MRC CBU 2024)\n",
    "# Subject-level fMRI analysis using Nilearn\n",
    "# \n",
    "# This script requres step08_first_level_analysis.sh, unless you define ds, sID, \n",
    "# and output here manually \n",
    "#\n",
    "# ======================================================================\n",
    "\n",
    "# ======================================================================\n",
    "# IMPORT REQUIRED PACKAGES\n",
    "# ======================================================================\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bids.layout import BIDSLayout\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======================================================================\n",
    "# DEFINE PATHS\n",
    "# arguments passed from step08_first_level_analysis.sh\n",
    "# ======================================================================\n",
    "\n",
    "ds = sys.argv[1] # dataset location\n",
    "sID = sys.argv[2].split(\"sub-\")[1] # subject id\n",
    "output = sys.argv[3]\n",
    "\n",
    "# ======================================================================\n",
    "print(\"Running first-level analysis for subject \" + sID)\n",
    "start_time = time.time()\n",
    "print(\"Started at: \" + time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n",
    "# ======================================================================\n",
    "# DEFINE PARAMETERS\n",
    "# =====================================================================\n",
    "model_name = 'first-level'\n",
    "\n",
    "bids_path = os.path.join(ds, 'data')\n",
    "\n",
    "outdir = os.path.join(output, model_name, 'sub-' + sID)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "print(\"BIDS data location: \" + bids_path)\n",
    "print(\"Output directory: \" + outdir)\n",
    "\n",
    "# ======================================================================\n",
    "# PERFORM SUBJECT LEVEL GLM ANALYSIS\n",
    "# ======================================================================\n",
    "\n",
    "# --- Initialize the BIDS layout and include the derivatives in it\n",
    "layout = BIDSLayout(bids_path, derivatives=True)\n",
    "\n",
    "# --- Get the preprocessed functional files\n",
    "bold = layout.get(\n",
    "    subject=sID, \n",
    "    datatype='func', \n",
    "    space='MNI152NLin2009cAsym', \n",
    "    desc='preproc', \n",
    "    extension='.nii.gz',\n",
    "    return_type='filename'\n",
    "    )\n",
    "print(\"Found \" + str(len(bold)) + \" preprocessed functional files\")\n",
    "\n",
    "# --- Get the event files\n",
    "events = layout.get(\n",
    "    subject=sID, \n",
    "    datatype='func', \n",
    "    suffix='events', \n",
    "    extension=\".tsv\", \n",
    "    return_type='filename'\n",
    "    )\n",
    "print(\"Found \" + str(len(events)) + \" event files\")\n",
    "\n",
    "# --- Get the confounds and select which ones to include in the design\n",
    "confounds = layout.get(\n",
    "    subject=sID, \n",
    "    datatype='func', \n",
    "    desc='confounds', \n",
    "    extension=\".tsv\", \n",
    "    return_type='filename'\n",
    "    )\n",
    "print(\"Found \" + str(len(confounds)) + \" confounds files\")\n",
    "\n",
    "# --- Define which confounds to include in the GLM\n",
    "confounds_of_interest = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z', \n",
    "                         'non_steady_state_outlier00', 'non_steady_state_outlier01']\n",
    "\n",
    "# --- For each run, load the confounds and select the ones of interest\n",
    "confounds_glm = []\n",
    "for conf_file in confounds:\n",
    "    this_conf = pd.read_table(conf_file)\n",
    "    # only include the confounds of interest that are present in the file\n",
    "    confounds_of_interest = [conf for conf in confounds_of_interest if conf in this_conf.columns]\n",
    "    # select the confounds of interest and fill NaN with 0    \n",
    "    conf_subset = this_conf[confounds_of_interest].fillna(0) # replace NaN with 0\n",
    "    confounds_glm.append(conf_subset)\n",
    "\n",
    "# --- Get the TR value\n",
    "TR = layout.get_tr()\n",
    "\n",
    "# --- If slice timing correction was applied, get the slice time reference\n",
    "slice_timing = layout.get_metadata(bold[0])\n",
    "if slice_timing['SliceTimingCorrected']:\n",
    "  slice_time_ref = slice_timing['StartTime'] / TR\n",
    "else:\n",
    "  slice_time_ref = 0\n",
    "\n",
    "# --- Define the GLM model\n",
    "fmri_glm = FirstLevelModel(\n",
    "    t_r = TR,\n",
    "    slice_time_ref = slice_time_ref, \n",
    "    hrf_model = 'glover',\n",
    "    drift_model = 'cosine',\n",
    "    high_pass = 0.01,\n",
    "    noise_model = 'ar1',\n",
    "    smoothing_fwhm = 6\n",
    "    )\n",
    "\n",
    "# --- Fit the model\n",
    "fmri_glm = fmri_glm.fit(bold, events, confounds_glm)\n",
    "\n",
    "# --- Get the design matrices\n",
    "design_matrices = fmri_glm.design_matrices_\n",
    "\n",
    "# --- Create contrasts \n",
    "events_df = pd.read_table(events[0])\n",
    "unique_conditions = events_df['trial_type'].unique()\n",
    "\n",
    "contrast_list = []\n",
    "nruns = len(design_matrices)\n",
    "\n",
    "for design_matrix in design_matrices:\n",
    "    n_columns = design_matrix.shape[1]  # number of predictors in the model\n",
    "    column_names = design_matrix.columns  # get the names of the columns\n",
    "    \n",
    "    # Create an empty dictionary to store contrasts for all conditions\n",
    "    contrasts = {}\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Create a contrast vector for each condition\n",
    "    # ------------------------------------------------------------------\n",
    "    for condition in unique_conditions:\n",
    "        # Initialize the contrast vector with zeros\n",
    "        contrast_vector = np.zeros(n_columns)\n",
    "        \n",
    "        # Assign 1 to the columns that correspond to the current condition and scale by the number of runs\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            if col_name.startswith(condition):\n",
    "                contrast_vector[i] = 1/nruns\n",
    "        \n",
    "        # Store the contrast for the current condition\n",
    "        contrasts[condition] = contrast_vector\n",
    "        \n",
    "    # ------------------------------------------------------------------\n",
    "    # Create contrast for Faces > Scrambled\n",
    "    # ------------------------------------------------------------------\n",
    "    # Calculate the number of \"Faces\" and \"Scrambled\" conditions\n",
    "    num_faces = sum(1 for col_name in column_names if col_name.startswith(('FAMOUS', 'UNFAMILIAR')))\n",
    "    num_scrambled = sum(1 for col_name in column_names if col_name.startswith('SCRAMBLED'))\n",
    "    \n",
    "    # Initialize the contrast vector with zeros\n",
    "    contrast_vector = np.zeros(n_columns)\n",
    "    \n",
    "    # Assign weights to the contrast vector\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        if col_name.startswith(('FAMOUS', 'UNFAMILIAR')):\n",
    "            contrast_vector[i] = 1 / num_faces /nruns # weigh the faces conditions\n",
    "        elif col_name.startswith('SCRAMBLED'):\n",
    "            contrast_vector[i] = -1 / num_scrambled/nruns  # weigh the scrambled conditions\n",
    "    \n",
    "    contrasts['Faces_Scrambled'] = contrast_vector\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Create effects of interest contrast\n",
    "    # ------------------------------------------------------------------\n",
    "    contrasts['EffectsOfInterest'] = np.eye(n_columns)[:len(unique_conditions)]\n",
    "    \n",
    "    # Append all contrasts for this design matrix\n",
    "    contrast_list.append(contrasts)\n",
    "    \n",
    "# --- Compute the contrasts and save the results\n",
    "for contrast_id in contrast_list[0].keys():   \n",
    "    if contrast_id == 'EffectsOfInterest':\n",
    "        stats = 'z_score' \n",
    "    else:\n",
    "        stats = 'effect_size'\n",
    "    stats_map = fmri_glm.compute_contrast(\n",
    "        [c[contrast_id] for c in contrast_list], \n",
    "        output_type = stats)\n",
    "    # Save results following BIDS standart\n",
    "    res_name = os.path.basename(bold[0]).split(\"run\")[0]\n",
    "    # from stats get only the part before _ for the BIDS file name\n",
    "    stats_suffix = stats.split(\"_\")[0]\n",
    "    # in contrast_id remove underscores\n",
    "    contrast_id = contrast_id.replace(\"_\", \"\")\n",
    "    # Save the result\n",
    "    stats_map.to_filename(os.path.join(outdir, res_name + 'desc-' + contrast_id + '_' + stats_suffix + '.nii.gz'))\n",
    "\n",
    "# ======================================================================\n",
    "# CREATE THIS MODEL'S dataset_description.json FILE\n",
    "# This is needed to use the results directory as BIDS data. \n",
    "# We will save our model parameters in the file as well, which is very useful.\n",
    "# ======================================================================\n",
    "\n",
    "jason_file = os.path.join(output, model_name, \"dataset_description.json\")\n",
    "\n",
    "if not os.path.exists(jason_file):\n",
    "    import json\n",
    "    import datetime\n",
    "    from importlib.metadata import version\n",
    "\n",
    "    bids_version = layout.get_dataset_description()['BIDSVersion']\n",
    "    nilearn_version = version('nilearn')\n",
    "    date_created = datetime.datetime.now()\n",
    "    \n",
    "    # Data to be written\n",
    "    content = {\n",
    "        \"Name\": \"First-level GLM analysis\",\n",
    "        \"BIDSVersion\": bids_version,\n",
    "        \"DatasetType\": \"results\",\n",
    "        \"GeneratedBy\": [\n",
    "            {\n",
    "                \"Name\": \"Nilearn\",\n",
    "                \"Version\": nilearn_version,\n",
    "                \"CodeURL\": \"https://nilearn.github.io\"\n",
    "            }\n",
    "        ],    \n",
    "        \"Date\": date_created,\n",
    "        \"ConfoundsIncluded\": confounds_of_interest,\n",
    "        \"FirstLevelModel\": [\n",
    "            fmri_glm.get_params()\n",
    "        ], \n",
    "    }\n",
    "    \n",
    "    # Serializing json\n",
    "    json_object = json.dumps(content, indent=4, default=str)\n",
    "    \n",
    "    # Writing to .json\n",
    "    with open(jason_file, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "# ======================================================================\n",
    "print(\"Finished first-level analysis for subject \" + sID)\n",
    "print(\"Finished at: \" + time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "print(\"Processing time: \" + str(round((time.time() - start_time)/60, 2)) + \" minutes\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing multiple subjects in paralel\n",
    "\n",
    "#### **Using `SLURM`**\n",
    "  \n",
    "If `SLURM `is available on the system, you can create a project-specific script where you define the paths and use the `sbatch` command to execute the generic script for each subject in parallel using.\n",
    "\n",
    "**Example script**: [step08_first_level_analysis.sh](code-examples/step08_first_level_analysis.sh)\n",
    "\n",
    "If `SLURM` is not available, you can still process multiple subjects in parallel using alternative methods depending on the system you're working with.\n",
    "  \n",
    "#### **Using Python's `multiprocessing` Module**\n",
    "  \n",
    "Python's `multiprocessing` module allows you to run processes in parallel on your local machine, distributing the workload across multiple CPU cores. An advantage is that it can be run on a single machine without needing a cluster or SLURM. A disatvantage is that it is limited by the local resources (RAM, CPU), and not ideal for very large datasets or many subjects.\n",
    "Here is an example how you could use the `multiprocessing` module:\n",
    "\n",
    "```python\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "# Define a function to process a single subject\n",
    "def process_subject(subject_id, dataset_location, output_dir):\n",
    "    # Execute the first-level script with the appropriate arguments\n",
    "    os.system(f\"python first_level_script.py {dataset_location} sub-{subject_id} {output_dir}\")\n",
    "\n",
    "# Define the dataset location and output directory\n",
    "dataset_location = \"/path/to/bids_dataset\"\n",
    "output_dir = \"/path/to/output\"\n",
    "\n",
    "# List of subject IDs\n",
    "subjects = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16']\n",
    "# subjects = [f\"{i:02d}\" for i in range(1, 17)]\n",
    "\n",
    "# Create a pool of processes using the available CPU cores\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(mp.cpu_count())  # Use all available CPU cores\n",
    "\n",
    "    # Use pool.starmap to pass multiple arguments to the process_subject function\n",
    "    pool.starmap(process_subject, [(subject, dataset_location, output_dir) for subject in subjects])\n",
    "\n",
    "    # Close the pool and wait for the processes to complete\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "  ```\n",
    "\n",
    "#### **Using Cloud Services**\n",
    "  \n",
    "If you don‚Äôt have SLURM or a powerful local machine, cloud computing platforms (e.g., AWS, Google Cloud, Microsoft Azure) allow you to run parallel processes with scalable resources. You can set up virtual machines or use a service like AWS Batch or Google Cloud Dataflow to handle parallel jobs across multiple subjects. However, it requires cloud expertise and comes with costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "874.85px",
    "left": "2183px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
