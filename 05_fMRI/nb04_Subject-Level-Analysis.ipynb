{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Ap≈°valka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) \n",
    "- **Date:** August 2024  \n",
    "- **conda environment**: I used the [fMRI workshop's conda environment](https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml) to run this notebook and any accompanied scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI Data Analysis: Subject-Level Analysis\n",
    "\n",
    "Once our data has been pre-processed, we can proceed to the analysis. In this tutorial, we will focus on subject-level (or first-level) fMRI data analysis using [Nilearn](https://nilearn.github.io/stable/index.html). This process involves constructing a design matrix, fitting the model to the data, and computing beta maps (i.e., the estimated response for each condition) and contrast maps. Subject-level analysis is a crucial step before progressing to group-level analysis.\n",
    "\n",
    "For analysing fMRI data, we commonly use general linear model (GLM). A GLM in fMRI analysis has the **BOLD signal** as the **dependent** or **outcome** variable (derived from the functional MRI images) and **predictors** or **independent variables** such as **events** (e.g., task conditions) and **confounds** (e.g., motion parameters). Typically analysis is performed by constructing a separate model for each voxel - a mass univariate approach. \n",
    "\n",
    "For more information on GLM, please see Rik's Stats tutorial!\n",
    "\n",
    "And here are some recommended short videos to help better understand the principles of fMRI analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"width: 1200px; margin: 0 auto; display: flex; justify-content: space-around;\">\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>GLM applied to fMRI (11 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/OyLKMb9FNhg\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>Conditions and contrasts (12 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7MibM1ATai4\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<div style=\"width: 1200px; margin: 0 auto; display: flex; justify-content: space-around;\">\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>Nuisance variables (14 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DEtwsFdFwYc\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "    <div style=\"text-align: center;\">\n",
       "        <h3>Multiple Comparisons (9 min)</h3>\n",
       "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AalIM9-5-Pk\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('''\n",
    "<div style=\"width: 1200px; margin: 0 auto; display: flex; justify-content: space-around;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>GLM applied to fMRI (11 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/OyLKMb9FNhg\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>Conditions and contrasts (12 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7MibM1ATai4\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"width: 1200px; margin: 0 auto; display: flex; justify-content: space-around;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>Nuisance variables (14 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DEtwsFdFwYc\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3>Multiple Comparisons (9 min)</h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AalIM9-5-Pk\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "</div>\n",
    "'''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Import required packages](#toc1_)    \n",
    "2. [Retrieve the preprocessed (fMRIprep) data](#toc2_)    \n",
    "3. [Setting up GLM model components](#toc3_)    \n",
    "3.1. [Dependent variable: BOLD signal from each voxel of the functional MRI images](#toc3_1_)    \n",
    "3.2. [Predictors: Events](#toc3_2_)    \n",
    "3.3. [Predictors: Confounds](#toc3_3_)    \n",
    "4. [Performing the GLM analysis](#toc4_)    \n",
    "4.1. [Creating the First Level Model](#toc4_1_)    \n",
    "4.2. [Fitting the model](#toc4_2_)    \n",
    "4.3. [Inspecting the Design Matrix](#toc4_3_)    \n",
    "5. [Contrast specification](#toc5_)    \n",
    "5.1. [A simple case contrast](#toc5_1_)    \n",
    "5.2. [Contrast scaling](#toc5_2_)    \n",
    "5.3. [Scaled contrasts for comparing conditions](#toc5_3_)    \n",
    "6. [Computing contrasts and plotting result maps](#toc6_)    \n",
    "6.1. [Statistical signifiance testing](#toc6_1_)    \n",
    "6.2. [Control the false positive rate](#toc6_2_)    \n",
    "6.3. [False Discovery Rate (FDR) correction](#toc6_3_)    \n",
    "6.4. [Cluster threshold](#toc6_4_)    \n",
    "6.5. [The impact of first-level model parameters](#toc6_5_)    \n",
    "7. [First Level for multiple subjects](#toc7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=2\n",
    "\tmaxLevel=3\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## 1. <a id='toc1_'></a>[Import required packages](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conda environment used for this tutorial is available here: https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml \n",
    "\n",
    "import pandas as pd # for data manipulation\n",
    "import numpy as np # for numerical operations\n",
    "\n",
    "import matplotlib.pyplot as plt # for basic plotting\n",
    "\n",
    "from bids.layout import BIDSLayout # to fetch data from BIDS-compliant datasets\n",
    "\n",
    "# Nilearn modules, for the analysis of brain volumes, plotting, etc., https://nilearn.github.io/\n",
    "from nilearn.glm.first_level import FirstLevelModel, glover_hrf, spm_hrf\n",
    "from nilearn.glm.thresholding import threshold_stats_img\n",
    "from nilearn.plotting import plot_design_matrix, plot_contrast_matrix, plot_stat_map\n",
    "\n",
    "import warnings # to suppress warnings\n",
    "\n",
    "from IPython.display import YouTubeVideo # to embed YouTube videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a id='toc2_'></a>[Retrieve the preprocessed (fMRIprep) data](#toc0_)\n",
    "\n",
    "BIDS applications, such as `fMRIprep`, output data into a data structure similar to `BIDS` organization principals. And these data can also be inspected using [PyBIDS](https://bids-standard.github.io/pybids/index.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up the paths to the BIDS data directory which includes derivatives\n",
    "fmri_data_dir = 'FaceProcessing/data' # data in BIDS format\n",
    "\n",
    "# --- Set up the BIDS layoutt and include the derivatives in it\n",
    "layout = BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "\n",
    "# or to include only a specific derivatives folder\n",
    "# layout = BIDSLayout(fmri_data_dir, derivatives = 'path/to/derivatives')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a id='toc3_'></a>[Setting up GLM model components](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. <a id='toc3_1_'></a>[Dependent variable: BOLD signal from each voxel of the functional MRI images](#toc0_)\n",
    "\n",
    "We need to specify which MRI images we want to analyze. Here, we will analyze a single subject's **9 functional runs**. With `PyBIDS`, we can easily locate the preprocessed files needed for the analysis. Let's retrieve the preprocessed functional image files for **subject sub-04**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sID = '04'\n",
    "\n",
    "bold = layout.get(\n",
    "    subject = sID, \n",
    "    datatype = 'func', \n",
    "    desc = 'preproc', \n",
    "    extension = '.nii.gz',\n",
    "    return_type = 'filename'\n",
    ")\n",
    "\n",
    "print('\\nSubject''s', sID, 'preprocessed functional images:')\n",
    "print(*bold, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also specify the subject's anatomical image (warped to the standard space) to use it as a background image when plotting results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anat = layout.get(\n",
    "    subject = sID, \n",
    "    datatype = 'anat', \n",
    "    space = 'MNI152NLin2009cAsym', \n",
    "    desc = 'preproc', \n",
    "    extension = '.nii.gz',\n",
    "    return_type ='filename'\n",
    ")\n",
    "\n",
    "print('Subject''s', sID, 'preprocessed anatomical image:')\n",
    "print(*anat, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. <a id='toc3_2_'></a>[Predictors: Events](#toc0_)\n",
    "\n",
    "We also need to specify the events that occurred during the functional acquisitions. The event files are stored in the `data/func` folder, and again, we can use `PyBIDS` to locate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = layout.get(\n",
    "    subject = sID, \n",
    "    datatype = 'func', \n",
    "    suffix = 'events', \n",
    "    extension = \".tsv\", \n",
    "    return_type = 'filename'\n",
    ")\n",
    "print('Subject''s', sID, 'event files:')\n",
    "print(*events, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. <a id='toc3_3_'></a>[Predictors: Confounds](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confounds represent fluctuations with a potential non-neuronal origin, such as head motion artifacts, scanner noise, and cardiac or respiratory effects. We can include them in the GLM as regressors of no interest to minimize the confounding effects of non-neuronal signals. fMRIPrep calculates a wide range of possible confounds. You can find detailed information in the [fMRIPrep documentation](https://fmriprep.org/en/stable/outputs.html#confounds).\n",
    "\n",
    "Which confounding variables you include in the GLM depends on the analysis you want to perform. The most well-established confounds are the six head-motion parameters (three rotations and three translations). If your dataset includes physiological recordings (e.g., heart rate, respiration), you might consider including these as confounds. If not, you might include PCA components from CompCor, a method implemented in fMRIPrep that identifies noise components from CSF and white matter signals. **However, please read the fMRIPrep documentation on confounds carefully before including any additional confounds, and never include all confounds in the GLM design matrix!**\n",
    "\n",
    "If your study has specific considerations (e.g., participant behavior or task-related noise), you may need to include additional custom confounds.\n",
    "\n",
    "The confounds computed by `fMRIPrep` can be found in the `data/derivatives/fmriprep/{sub}/func/ directory`. They are stored separately for each run in `.tsv` files, with one column for each confound variable. Let's use `PyBIDS` to locate these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds = layout.get(\n",
    "    subject = sID, \n",
    "    datatype = 'func', \n",
    "    desc = 'confounds', \n",
    "    extension = \".tsv\", \n",
    "    return_type = 'filename'\n",
    ")\n",
    "\n",
    "print('Subject''s', sID, 'confound files:')\n",
    "print(*confounds, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of all confounds of the first functional run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confounds_run1 = pd.read_table(confounds[0])\n",
    "\n",
    "print('Number of confounds in run 1:', len(list(confounds_run1)))\n",
    "print(*list(confounds_run1), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will include the most commonly used confounds‚Äîthe six motion parameters.\n",
    "\n",
    "**If your data contains dummy scans and you specified this when running fMRIPrep, dummy scan confounds (non_steady_state_outlier##) will also be generated, and you must include them in your GLM!** This ensures that the dummy volumes do not contribute to the parameter estimates.\n",
    "\n",
    "We had two dummy scans, so we will include the two *'non_steady_state_outliers'* in our design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_of_interest = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z', 'non_steady_state_outlier00', 'non_steady_state_outlier01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the confounds of interest of the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_run1[confounds_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to extract the confounds of interest from all nine runs. We will include these in our GLM model by creating a list of confound tables, resulting in a list of nine tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_glm = []\n",
    "for conf_file in confounds:\n",
    "    this_conf = pd.read_table(conf_file)\n",
    "    conf_subset = this_conf[confounds_of_interest].fillna(0) # replace NaN with 0\n",
    "    confounds_glm.append(conf_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the BOLD images, event files, and confounds for each of the 9 functional runs. Nilearn will analyse them together and compute fixed effects statistics for this subject. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a id='toc4_'></a>[Performing the GLM analysis](#toc0_)\n",
    "\n",
    "It is now time to create and estimate a `FirstLevelModel` object, which will generate our *design matrix* based on the inputs we have provided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. <a id='toc4_1_'></a>[Creating the First Level Model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify the model with the parameters of our choice (see the [documentation](https://nilearn.github.io/stable/modules/generated/nilearn.glm.first_level.FirstLevelModel.html) for a full list of parameters). Here we will specify the folowing:\n",
    "* **t_r** - repetition time (the sampling interval of the functional runs) in seconds. We can get it from our BIDS layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the TR\n",
    "TR = layout.get_tr()\n",
    "print('TR:', TR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **slice_time_ref**: *This parameter indicates the time of the reference slice used in the slice timing preprocessing step of the experimental runs. It is expressed as a percentage of the t_r (time repetition), so it can have values between 0. and 1. Default=0.* \n",
    "  \n",
    "  We can find this information in our fMRIPrep Methods (*data/derivatives/fmriprep/logs/CITATION.html*). There we read: *\"BOLD runs were slice-time corrected to 0.974s (0.5 of slice acquisition range 0s-1.95s)\"*. This means, that **0.5** is the value we need to use for this parameter. \n",
    "  \n",
    "  Alternativel, and perhaps preferably, we can obtain this information from the BIDS derivatives metadata. It also tells if the slice-time correction was performed at all. \n",
    "\n",
    "  *data\\derivatives\\fmriprep\\sub-{sID}\\func\\sub-{sID}_task-facerecognition_run-{runID}_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.json*\n",
    "  ```json\n",
    "  {\n",
    "    \"RepetitionTime\": 2,\n",
    "    \"SkullStripped\": false,\n",
    "    \"SliceTimingCorrected\": true,\n",
    "    \"StartTime\": 0.974,\n",
    "    \"TaskName\": \"facerecognition\"\n",
    "  } \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If slice timing correction was applied, get the slice time reference\n",
    "slice_timing = layout.get_metadata(bold[0])\n",
    "if slice_timing['SliceTimingCorrected']:\n",
    "  slice_time_ref = slice_timing['StartTime'] / TR\n",
    "  print('Slice timing reference:', slice_time_ref)\n",
    "else:\n",
    "  slice_time_ref = 0\n",
    "  print('Slice timing correction was not applied')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **hrf_model**: defines the HRF model to be used. Possible options:\n",
    "  * `spm`: This is the HRF model used in SPM. \n",
    "  * `spm + derivative`: SPM model plus its time derivative. This gives 2 regressors.\n",
    "  * `spm + derivative + dispersion`: Idem, plus dispersion derivative. This gives 3 regressors.\n",
    "  * `glover`: This corresponds to the Glover HRF.\n",
    "  * `glover + derivative`: The Glover HRF + time derivative. This gives 2 regressors. \n",
    "  * `glover + derivative + dispersion`: Idem, plus dispersion derivative. This gives 3 regressors. \n",
    "  * `fir`: Finite impulse response basis. This is a set of delayed dirac models.\n",
    "\n",
    "It can also be a custom model. In this case, a function should be provided for each regressor.\n",
    "  \n",
    "The choice of the HRF model is up to the user. There's little difference between using the SPM or Glover model. It's advisable to include derivatives when there's some uncertainty in the timing information, as this can help identify timing problems more effectively.\n",
    "\n",
    "Let's see how, for example, SPM and Glover HRFs compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeled time in seconds\n",
    "time_length = 32\n",
    "\n",
    "# Generate the HRF timecourses\n",
    "glover_timecourse = glover_hrf(TR, time_length=time_length)\n",
    "spm_timecourse = spm_hrf(TR, time_length=time_length)\n",
    "\n",
    "# Plot the timecourses\n",
    "timepoints = np.linspace(0, time_length, num=len(glover_timecourse))\n",
    "plt.plot(timepoints, glover_timecourse, label='Glover HRF', color='blue')\n",
    "plt.plot(timepoints, spm_timecourse, label='SPM HRF', color='red')\n",
    "plt.title('Comparison of Glover and SPM HRF Timecourses')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('HRF Amplitude (a.u.)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **drift_model**: specifies the desired drift model for the design matrices. It can be ‚Äòpolynomial‚Äô, ‚Äòcosine‚Äô or None. Default=‚Äôcosine‚Äô.\n",
    "* **high_pass**: specifies the cut frequency of the high-pass filter in Hz for the design matrices. Used only if drift_model is ‚Äòcosine‚Äô. Default=0.01 (1/128, as in SPM).\n",
    "* **smoothing_fwhm**: the full-width at half maximum in millimeters of the spatial smoothing to apply to the signal (smoothing was not done in fMRIPrep!).\n",
    "* **noise_model**: {‚Äòar1‚Äô, ‚Äòols‚Äô} The temporal variance model. Default=‚Äôar1‚Äô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = FirstLevelModel(\n",
    "    t_r = TR,\n",
    "    slice_time_ref = slice_time_ref,\n",
    "    smoothing_fwhm = 6, # a rule of thumb is 3x the voxel size\n",
    "    hrf_model = 'glover', # default\n",
    "    drift_model = 'cosine', # default\n",
    "    high_pass = 0.01, # default; Used only if drift_model is ‚Äòcosine‚Äô\n",
    "    noise_model = 'ar1', # default\n",
    "   # mask_img = path_to_mask_image, # default is None and Nilearn will compute it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. <a id='toc4_2_'></a>[Fitting the model](#toc0_)\n",
    "\n",
    "Now that we have specified the model, we can run it on our data. We need to include the list of our functional image files (we named them `bold)`, the list of event timing files (we named them `events`), and the list of our confound tables (one per run, named `confounds_glm`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = fmri_glm.fit(bold, events, confounds_glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. <a id='toc4_3_'></a>[Inspecting the Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the design matrix of our GLM model (rows represent time, and columns contain the predictors).\n",
    "\n",
    "The `design_matrices` is a list of 9 tables (one per run). Let's look at the first run's design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the design matrices from the glm model\n",
    "design_matrices = fmri_glm.design_matrices_\n",
    "\n",
    "# display the design matrix of the first run\n",
    "design_matrices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the design matrix, we can extract and plot the expected signal of our conditions. Here we will plot it for the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = design_matrices[0][['FAMOUS_1', 'SCRAMBLED_1', 'UNFAMILIAR_1']]\n",
    "dm.columns.name = 'Condition'\n",
    "dm.index.name = 'Seconds'\n",
    "dm.plot(figsize=(12,4), title='Expected responses per condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: How was this expected signal obtained?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot all predicted event responses, we can observe the expected oscillations. Any slow oscillations (modelled by the cosine function) will be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = design_matrices[0]\n",
    "event_regressors = dm.columns[:9]\n",
    "noise_regressors = [col for col in dm.columns if col.startswith('drift_')]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the predicted event responses\n",
    "for event in event_regressors:\n",
    "    plt.plot(dm.index, dm[event], label=event, color='lightblue')\n",
    "\n",
    "# Plot the modeled noise\n",
    "for drift in noise_regressors:\n",
    "    plt.plot(dm.index, dm[drift], color='grey', lw=1, alpha=0.7)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the actual design matrix. Let's plot it for the first run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Design matrix for run', 1)\n",
    "plot_design_matrix(design_matrices[0], output_file=None)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the non-steady-state volume 'spoils the image'. It's good that we are excluding it from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id='toc5_'></a>[Contrast specification](#toc0_)\n",
    "\n",
    "If we have the same number of regressors, in the same order, in each run, we can specify the contrast for one run, and it will automatically be reused for the other runs. However, to be cautious, it's advisable to define contrasts separately for each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. <a id='toc5_1_'></a>[A simple case contrast](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple case contrast\n",
    "\n",
    "n_columns = design_matrices[0].shape[1]  # Number of predictors in the model\n",
    "\n",
    "contrasts = {\n",
    "    'Faces_Scrambled': np.pad([1, 1, 1, -2, -2, -2, 1, 1, 1], (0, n_columns - 9), 'constant'),\n",
    "    'Famous_Unfamiliar': np.pad([1, 1, 1, 0, 0, 0, -1, -1, -1], (0, n_columns - 9), 'constant'),\n",
    "    'EffectsOfInterest': np.eye(n_columns)[:9]  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in contrasts.items():\n",
    "    plot_contrast_matrix(values, design_matrix=design_matrices[0])\n",
    "    plt.suptitle(key)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. <a id='toc5_2_'></a>[Contrast scaling](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GLM is a linear combination of predictor beta coefficients, where each beta represents the weight or contribution of a given predictor to the overall model.\n",
    "\n",
    "For example, if we have conditions (predictors): Famous, Unfamiliar, and Scrambled, a contrast estimating Faces vs. Scrambled with weights [1, 1, -2] would calculate Famous + Unfamiliar - 2 * Scrambled. This can be interpreted as the sum of the responses to Famous and Unfamiliar faces, minus twice the response to Scrambled faces. However, what we typically want is the average response to Famous and Unfamiliar faces minus the response to Scrambled faces. To achieve this, we would need to scale the contrast as follows: [0.5, 0.5, -1].\n",
    "\n",
    "Scaling contrast weights affects the magnitude of the resulting contrast estimate, but not the t-values or p-values, so the statistical inference remains unchanged. However, contrast estimates that reflect averages rather than sums are generally more meaningful for interpretation.\n",
    "\n",
    "If we want to obtain beta estimates for each condition separately (e.g., a beta estimate for Famous, a beta estimate for Unfamiliar, etc.), and we have multiple functional runs, we need to scale the contrast to account for the number of runs to get the average beta across them. When scaled appropriately, we can report the result as a 'beta estimate' rather than just a 'contrast estimate,' since scaling allows the contrast to represent the average of the beta estimates. This average can then be interpreted directly as a beta estimate for the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A contrast scaling function, which scales for the number of conditions and runs\n",
    "\n",
    "def scale_contrast(contrast_vector, nruns):\n",
    "    positive_sum = np.sum(contrast_vector[contrast_vector > 0])  # Sum of positive weights\n",
    "    negative_sum = np.abs(np.sum(contrast_vector[contrast_vector < 0]))  # Sum of negative weights (absolute)\n",
    "    \n",
    "    # Scale each part of the vector separately\n",
    "    scaled_vector = np.zeros_like(contrast_vector, dtype=float)\n",
    "    scaled_vector[contrast_vector > 0] = contrast_vector[contrast_vector > 0] / (positive_sum * nruns)\n",
    "    scaled_vector[contrast_vector < 0] = contrast_vector[contrast_vector < 0] / (negative_sum * nruns)\n",
    "    \n",
    "    return scaled_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_contrast = np.array([1, 1, 1, -2, -2, -2, 1, 1, 1])\n",
    "print('\\n Original contrast:\\n', example_contrast)\n",
    "print('\\n Scaled contrast for one run:\\n', scale_contrast(example_contrast, 1))\n",
    "print('\\n Scaled contrast for nine runs:\\n', scale_contrast(example_contrast, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. <a id='toc5_3_'></a>[Scaled contrasts for comparing conditions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_columns = design_matrices[0].shape[1]  # Number of predictors in the model\n",
    "nruns = len(design_matrices)  # Number of runs\n",
    "\n",
    "contrast_list = []\n",
    "\n",
    "# Define the contrasts and scale them\n",
    "contrasts = {\n",
    "    'Faces_Scrambled': scale_contrast(np.pad([1, 1, 1, -2, -2, -2, 1, 1, 1], (0, n_columns - 9), 'constant'), nruns),\n",
    "    'Famous_Unfamiliar': scale_contrast(np.pad([1, 1, 1, 0, 0, 0, -1, -1, -1], (0, n_columns - 9), 'constant'), nruns),\n",
    "    'EffectsOfInterest': np.eye(n_columns)[:9]  # No scaling needed here\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled contrasts for each condition of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contrasts\n",
    "design_matrices = fmri_glm.design_matrices_\n",
    "\n",
    "# Create contrasts for all unique conditions\n",
    "events_df = pd.read_table(events[0])\n",
    "unique_conditions = events_df['trial_type'].unique()\n",
    "\n",
    "#contrast_list = []\n",
    "nruns = len(design_matrices)\n",
    "\n",
    "for design_matrix in design_matrices:\n",
    "    n_columns = design_matrix.shape[1]  # number of predictors in the model\n",
    "    column_names = design_matrix.columns  # get the names of the columns\n",
    "    \n",
    "    # Create an empty dictionary to store contrasts for all conditions\n",
    "    #contrasts = {}\n",
    "    \n",
    "    for condition in unique_conditions:\n",
    "        # Initialize the contrast vector with zeros\n",
    "        contrast_vector = np.zeros(n_columns)\n",
    "        \n",
    "        # Assign 1 to the columns that correspond to the current condition\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            if col_name.startswith(condition):\n",
    "                contrast_vector[i] = 1/nruns\n",
    "        \n",
    "        # Store the contrast for the current condition\n",
    "        contrasts[condition] = contrast_vector    \n",
    "    \n",
    "    # Append all contrasts for this design matrix\n",
    "    contrast_list.append(contrasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in contrast_list[0].items():\n",
    "    plot_contrast_matrix(values, design_matrix=design_matrices[0])\n",
    "    plt.suptitle(key)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a id='toc6_'></a>[Computing contrasts and plotting result maps](#toc0_)\n",
    "\n",
    "With Nilearn, you can compute the `effect size` maps, `t-statistics` maps, `z-scores` and some other types. See the [documentation](https://nilearn.github.io/dev/modules/generated/nilearn.glm.Contrast.html) for more information.\n",
    "\n",
    "'Effect size' maps correspond to the 'beta' (or contrast estimate) maps. These can be used, for example, to plot effect sizes (beta estimates) in ROI analysis (see examples in the ROI analysis notebook), or as inputs for group-level analysis. \n",
    "\n",
    "In the case of a single-subject analysis, let's look at z-scored maps for **Faces > Scrambled contrast**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map = fmri_glm.compute_contrast(\n",
    "    [c['Faces_Scrambled'] for c in contrast_list],\n",
    "    output_type = 'z_score'\n",
    ")\n",
    "\n",
    "# You can sace the z_map to a file\n",
    "# z_map.to_filename('/path/to/output/z_map.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. <a id='toc6_1_'></a>[Statistical signifiance testing](#toc0_)\n",
    "\n",
    "fMRI analyses involve thousands of voxel-wise comparisons, which increases the risk of false positives‚Äîillustrated by the well-known case of the [\"Dead Salmon\"](https://teenspecies.github.io/pdfs/NeuralCorrelates.pdf).\n",
    "\n",
    "Applying a threshold, especially one that corrects for multiple comparisons (e.g., using Family-Wise Error (FWE) or False Discovery Rate (FDR) corrections), ensures that the displayed activations are statistically reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. <a id='toc6_2_'></a>[Control the false positive rate](#toc0_)\n",
    "\n",
    "A minimum recommendation is to control the **false positive rate** (fpr) at a specific threshold, such as p < .001. This is referred to as 'uncorrected' because it does not account for or correct the number of multiple comparisons, but it can still help reduce false positives to a reasonable level when applied conservatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the statistical threshold\n",
    "_, threshold_fpr = threshold_stats_img(\n",
    "    z_map, \n",
    "    alpha = .001, \n",
    "    height_control = 'fpr'\n",
    ")\n",
    "\n",
    "print('Uncorrected p<.001 threshold: %.3f' % threshold_fpr)\n",
    "\n",
    "# plot the thresholded map\n",
    "plot_stat_map(\n",
    "    z_map, \n",
    "    bg_img = anat[0], \n",
    "    threshold = threshold_fpr,\n",
    "    display_mode = 'ortho', \n",
    "    black_bg = True,\n",
    "    title = 'Faces > Scrambled  (p<.001, uncorrected)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. <a id='toc6_3_'></a>[False Discovery Rate (FDR) correction](#toc0_)\n",
    "\n",
    "A commonly used alternative to controlling the false positive rate is the **False Discovery Rate** (FDR) method, which addresses the expected proportion of false discoveries among all detected activations. However, in neuroimaging research, the appropriateness of FDR remains a topic of debate. While it is often preferred in exploratory or hypothesis-generating studies due to its balance between discovery and error control, it may be less suitable for confirmatory studies where stricter error control is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, threshold_fdr = threshold_stats_img(\n",
    "    z_map, \n",
    "    alpha = .05, \n",
    "    height_control = 'fdr'\n",
    ")\n",
    "\n",
    "print('FDR, p<.05 threshold: %.3f' % threshold_fdr)\n",
    "\n",
    "plot_stat_map(\n",
    "    z_map, \n",
    "    bg_img = anat[0], \n",
    "    threshold = threshold_fdr,\n",
    "    display_mode = 'ortho', \n",
    "    black_bg = True,\n",
    "    title = 'Faces > Scrambled  (p<.05, FDR corrected)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Family Wise Error (FWE) correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more conservative approach is to control the **family-wise error** (FWE) rate, which represents the probability of making one or more false discoveries, or Type I errors, across all tests. This rate is typically set at 5%. The **Bonferroni correction** is a commonly used method to achieve this level of control and is also used in Nilearn. \n",
    "\n",
    "In Nilearn, Bonferroni correction is applied to the number of voxels. The Bonferroni correction, when applied directly to the number of voxels, is not appropriate for fMRI data. This is because neuroimaging data typically have spatially correlated data points, which violate the Bonferroni assumption of independent tests (even more so if data have been smoothed). As an alternative, neuroscientists have developed **Random Field Theory** (RFT). This method accounts for the spatial correlation by applying multiple comparison correction in a way that considers the smoothness of the data. Specifically, the correction is applied to the number of '***resels'*** (RESolution ELements), rather than the raw number of voxels. However, it's important to note that this RFT-based approach is not implemented in Nilearn. \n",
    "At the second-level analysis, Nilearn provides an option for non-parametric inference with permutation testing, which is a more suitable approach for fMRI data when considering the spatial correlation of voxels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, threshold_fwe = threshold_stats_img(\n",
    "    z_map, \n",
    "    alpha = .05, \n",
    "    height_control = 'bonferroni')\n",
    "\n",
    "print('Bonferroni-corrected, p<.05 threshold: %.3f' % threshold_fwe)\n",
    "\n",
    "plot_stat_map(\n",
    "    z_map, \n",
    "    bg_img = anat[0], \n",
    "    threshold = threshold_fwe,\n",
    "    display_mode = 'ortho', \n",
    "    black_bg = True,\n",
    "    title = 'Faces > Scrambled  (p<.05, FWE corrected)'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. <a id='toc6_4_'></a>[Cluster threshold](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a common practice to discard isolated voxels from the images. It is possible to generate a thresholded map with small clusters removed by providing a `cluster_threshold` argument. Here clusters smaller than `20` voxels will be discarded from the `fpr` corrected map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_map_fpr, threshold_fpr = threshold_stats_img(\n",
    "    z_map, \n",
    "    alpha = .001, \n",
    "    height_control='fpr', \n",
    "    cluster_threshold = 20\n",
    ")\n",
    "\n",
    "plot_stat_map(\n",
    "    thresholded_map_fpr, \n",
    "    bg_img = anat[0], \n",
    "    threshold = threshold_fpr,\n",
    "    display_mode = 'ortho', \n",
    "    black_bg = True, \n",
    "    colorbar = True,\n",
    "    title = 'Faces > Scrambled (p<.001, uncorrected, k=20'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. <a id='toc6_5_'></a>[The impact of first-level model parameters](#toc0_)\n",
    "\n",
    "See the Nilearn tutorial for more examples on how the first_level parameters impact the results: https://nilearn.github.io/stable/auto_examples/04_glm_first_level/plot_first_level_details.html\n",
    "\n",
    "Here we will just compare our original model where we used Glover's HRF to a model SPM's HRF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Glover's HRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fmri_glm = FirstLevelModel(\n",
    "    t_r = TR,\n",
    "    slice_time_ref = slice_time_ref,\n",
    "    smoothing_fwhm = 6,\n",
    "    hrf_model = 'glover',\n",
    "    drift_model = 'cosine', \n",
    "    high_pass = 0.01, \n",
    "    noise_model = 'ar1'\n",
    ")\n",
    "\n",
    "fmri_glm = fmri_glm.fit(bold, events, confounds_glm)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_z_map = z_map\n",
    "model1_title = 'Glover HRF model, Faces > Scrambled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: SPM's HRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm2 = FirstLevelModel(\n",
    "    t_r = TR,\n",
    "    slice_time_ref = slice_time_ref,\n",
    "    smoothing_fwhm = 6,\n",
    "    hrf_model = 'spm',\n",
    "    drift_model = 'cosine', \n",
    "    high_pass = 0.01, \n",
    "    noise_model = 'ar1'\n",
    ")\n",
    "\n",
    "fmri_glm2 = fmri_glm2.fit(bold, events, confounds_glm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has the same regressors as model1, therefore we can use our original (model1) contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_z_map = fmri_glm2.compute_contrast(\n",
    "  [c['Faces_Scrambled'] for c in contrast_list],\n",
    "  output_type='z_score')\n",
    "\n",
    "model2_title = 'SPM HRF model, Faces > Scrambled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_maps = [model1_z_map, model2_z_map]\n",
    "titles = [model1_title, model2_title]\n",
    "\n",
    "for (z_map, title) in zip(z_maps, titles):\n",
    "    _, threshold = threshold_stats_img(z_map, alpha = .001, height_control = 'fpr')\n",
    "    plot_stat_map(z_map, bg_img = anat[0], threshold = threshold,\n",
    "              display_mode = 'ortho', \n",
    "              cut_coords = [33, -72, -12],\n",
    "              black_bg = True,\n",
    "              title = title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a id='toc7_'></a>[First Level for multiple subjects](#toc0_)\n",
    "\n",
    "Using a notebook like this is an effective method for preparing the final analysis script. Now, we can combine all the elements discussed into a unified, generic script that will conduct the first-level analysis.\n",
    "\n",
    "**Example of a generic first-level script**: [code-examples/first_level_script.py](https://github.com/dcdace/fMRI_training/blob/main/code/first_level_script.py)\n",
    "\n",
    "Things to note:\n",
    "* We are saving the result files in BIDS format so that we can easily query them with PyBIDS for further analysis.\n",
    "* We are generating this result model's dataset_description.json file, which is required to use the results directory as BIDS-compatible data. We will also save our model parameters in this file, which is very useful for understanding how the results were obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# ======================================================================\n",
    "# Dace Ap≈°valka (MRC CBU 2024)\n",
    "# Subject-level fMRI analysis using Nilearn\n",
    "# \n",
    "# This script requres step08_first_level_analysis.sh, unless you define ds, sID, \n",
    "# and output here manually \n",
    "#\n",
    "# ======================================================================\n",
    "\n",
    "# ======================================================================\n",
    "# IMPORT REQUIRED PACKAGES\n",
    "# ======================================================================\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bids.layout import BIDSLayout\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======================================================================\n",
    "# DEFINE PATHS\n",
    "# arguments passed from step08_first_level_analysis.sh\n",
    "# ======================================================================\n",
    "\n",
    "ds = sys.argv[1] # BIDS dataset location\n",
    "sID = sys.argv[2].split(\"sub-\")[1] # subject id\n",
    "output = sys.argv[3]\n",
    "\n",
    "# ======================================================================\n",
    "print(\"Running first-level analysis for subject \" + sID)\n",
    "start_time = time.time()\n",
    "print(\"Started at: \" + time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n",
    "# ======================================================================\n",
    "# DEFINE PARAMETERS\n",
    "# =====================================================================\n",
    "model_name = 'first-level'\n",
    "\n",
    "bids_path = os.path.join(ds, 'data')\n",
    "\n",
    "outdir = os.path.join(output, model_name, 'sub-' + sID)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "print(\"BIDS data location: \" + bids_path)\n",
    "print(\"Output directory: \" + outdir)\n",
    "\n",
    "# ======================================================================\n",
    "# PERFORM SUBJECT LEVEL GLM ANALYSIS\n",
    "# ======================================================================\n",
    "\n",
    "# --- Initialize the BIDS layout and include the derivatives in it\n",
    "layout = BIDSLayout(bids_path, derivatives=True)\n",
    "\n",
    "# --- Get the preprocessed functional files\n",
    "bold = layout.get(\n",
    "    subject=sID, \n",
    "    datatype='func', \n",
    "    space='MNI152NLin2009cAsym', \n",
    "    desc='preproc', \n",
    "    extension='.nii.gz',\n",
    "    return_type='filename'\n",
    "    )\n",
    "print(\"Found \" + str(len(bold)) + \" preprocessed functional files\")\n",
    "\n",
    "# --- Get the event files\n",
    "events = layout.get(\n",
    "    subject=sID, \n",
    "    datatype='func', \n",
    "    suffix='events', \n",
    "    extension=\".tsv\", \n",
    "    return_type='filename'\n",
    "    )\n",
    "print(\"Found \" + str(len(events)) + \" event files\")\n",
    "\n",
    "# --- Get the confounds and select which ones to include in the design\n",
    "confounds = layout.get(\n",
    "    subject=sID, \n",
    "    datatype='func', \n",
    "    desc='confounds', \n",
    "    extension=\".tsv\", \n",
    "    return_type='filename'\n",
    "    )\n",
    "print(\"Found \" + str(len(confounds)) + \" confounds files\")\n",
    "\n",
    "# --- Define which confounds to include in the GLM\n",
    "confounds_of_interest = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z', \n",
    "                         'non_steady_state_outlier00', 'non_steady_state_outlier01']\n",
    "\n",
    "# --- For each run, load the confounds and select the ones of interest\n",
    "confounds_glm = []\n",
    "for conf_file in confounds:\n",
    "    this_conf = pd.read_table(conf_file)\n",
    "    # only include the confounds of interest that are present in the file\n",
    "    confounds_of_interest = [conf for conf in confounds_of_interest if conf in this_conf.columns]\n",
    "    # select the confounds of interest and fill NaN with 0    \n",
    "    conf_subset = this_conf[confounds_of_interest].fillna(0) # replace NaN with 0\n",
    "    confounds_glm.append(conf_subset)\n",
    "\n",
    "# --- Get the TR value\n",
    "TR = layout.get_tr()\n",
    "\n",
    "# --- If slice timing correction was applied, get the slice time reference\n",
    "slice_timing = layout.get_metadata(bold[0])\n",
    "if slice_timing['SliceTimingCorrected']:\n",
    "  slice_time_ref = slice_timing['StartTime'] / TR\n",
    "else:\n",
    "  slice_time_ref = 0\n",
    "\n",
    "# --- Define the GLM model\n",
    "fmri_glm = FirstLevelModel(\n",
    "    t_r = TR,\n",
    "    slice_time_ref = slice_time_ref, \n",
    "    hrf_model = 'glover',\n",
    "    drift_model = 'cosine',\n",
    "    high_pass = 0.01,\n",
    "    noise_model = 'ar1',\n",
    "    smoothing_fwhm = 6\n",
    "    )\n",
    "\n",
    "# --- Fit the model\n",
    "fmri_glm = fmri_glm.fit(bold, events, confounds_glm)\n",
    "\n",
    "# --- Get the design matrices\n",
    "design_matrices = fmri_glm.design_matrices_\n",
    "\n",
    "# --- Create contrasts \n",
    "events_df = pd.read_table(events[0])\n",
    "unique_conditions = events_df['trial_type'].unique()\n",
    "\n",
    "contrast_list = []\n",
    "nruns = len(design_matrices)\n",
    "\n",
    "for design_matrix in design_matrices:\n",
    "    n_columns = design_matrix.shape[1]  # number of predictors in the model\n",
    "    column_names = design_matrix.columns  # get the names of the columns\n",
    "    \n",
    "    # Create an empty dictionary to store contrasts for all conditions\n",
    "    contrasts = {}\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Create a contrast vector for each condition\n",
    "    # ------------------------------------------------------------------\n",
    "    for condition in unique_conditions:\n",
    "        # Initialize the contrast vector with zeros\n",
    "        contrast_vector = np.zeros(n_columns)\n",
    "        \n",
    "        # Assign 1 to the columns that correspond to the current condition and scale by the number of runs\n",
    "        for i, col_name in enumerate(column_names):\n",
    "            if col_name.startswith(condition):\n",
    "                contrast_vector[i] = 1/nruns\n",
    "        \n",
    "        # Store the contrast for the current condition\n",
    "        contrasts[condition] = contrast_vector\n",
    "        \n",
    "    # ------------------------------------------------------------------\n",
    "    # Create contrast for Faces > Scrambled\n",
    "    # ------------------------------------------------------------------\n",
    "    # Calculate the number of \"Faces\" and \"Scrambled\" conditions\n",
    "    num_faces = sum(1 for col_name in column_names if col_name.startswith(('FAMOUS', 'UNFAMILIAR')))\n",
    "    num_scrambled = sum(1 for col_name in column_names if col_name.startswith('SCRAMBLED'))\n",
    "    \n",
    "    # Initialize the contrast vector with zeros\n",
    "    contrast_vector = np.zeros(n_columns)\n",
    "    \n",
    "    # Assign weights to the contrast vector\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        if col_name.startswith(('FAMOUS', 'UNFAMILIAR')):\n",
    "            contrast_vector[i] = 1 / num_faces /nruns # weigh the faces conditions\n",
    "        elif col_name.startswith('SCRAMBLED'):\n",
    "            contrast_vector[i] = -1 / num_scrambled/nruns  # weigh the scrambled conditions\n",
    "    \n",
    "    contrasts['Faces_Scrambled'] = contrast_vector\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Create effects of interest contrast\n",
    "    # ------------------------------------------------------------------\n",
    "    contrasts['EffectsOfInterest'] = np.eye(n_columns)[:len(unique_conditions)]\n",
    "    \n",
    "    # Append all contrasts for this design matrix\n",
    "    contrast_list.append(contrasts)\n",
    "    \n",
    "# --- Compute the contrasts and save the results\n",
    "for contrast_id in contrast_list[0].keys():   \n",
    "    if contrast_id == 'EffectsOfInterest':\n",
    "        stats = 'z_score' \n",
    "    else:\n",
    "        stats = 'effect_size'\n",
    "    stats_map = fmri_glm.compute_contrast(\n",
    "        [c[contrast_id] for c in contrast_list], \n",
    "        output_type = stats)\n",
    "    # Save results following BIDS standart\n",
    "    res_name = os.path.basename(bold[0]).split(\"run\")[0]\n",
    "    # from stats get only the part before _ for the BIDS file name\n",
    "    stats_suffix = stats.split(\"_\")[0]\n",
    "    # in contrast_id remove underscores\n",
    "    contrast_id = contrast_id.replace(\"_\", \"\")\n",
    "    # Save the result\n",
    "    stats_map.to_filename(os.path.join(outdir, res_name + 'desc-' + contrast_id + '_' + stats_suffix + '.nii.gz'))\n",
    "\n",
    "# ======================================================================\n",
    "# CREATE THIS MODEL'S dataset_description.json FILE\n",
    "# This is needed to use the results directory as BIDS data. \n",
    "# We will save our model parameters in the file as well, which is very useful.\n",
    "# ======================================================================\n",
    "\n",
    "jason_file = os.path.join(output, model_name, \"dataset_description.json\")\n",
    "\n",
    "if not os.path.exists(jason_file):\n",
    "    import json\n",
    "    import datetime\n",
    "    from importlib.metadata import version\n",
    "\n",
    "    bids_version = layout.get_dataset_description()['BIDSVersion']\n",
    "    nilearn_version = version('nilearn')\n",
    "    date_created = datetime.datetime.now()\n",
    "    \n",
    "    # Data to be written\n",
    "    content = {\n",
    "        \"Name\": \"First-level GLM analysis\",\n",
    "        \"BIDSVersion\": bids_version,\n",
    "        \"DatasetType\": \"results\",\n",
    "        \"GeneratedBy\": [\n",
    "            {\n",
    "                \"Name\": \"Nilearn\",\n",
    "                \"Version\": nilearn_version,\n",
    "                \"CodeURL\": \"https://nilearn.github.io\"\n",
    "            }\n",
    "        ],    \n",
    "        \"Date\": date_created,\n",
    "        \"ConfoundsIncluded\": confounds_of_interest,\n",
    "        \"FirstLevelModel\": [\n",
    "            fmri_glm.get_params()\n",
    "        ], \n",
    "    }\n",
    "    \n",
    "    # Serializing json\n",
    "    json_object = json.dumps(content, indent=4, default=str)\n",
    "    \n",
    "    # Writing to .json\n",
    "    with open(jason_file, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "# ======================================================================\n",
    "print(\"Finished first-level analysis for subject \" + sID)\n",
    "print(\"Finished at: \" + time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "print(\"Processing time: \" + str(round((time.time() - start_time)/60, 2)) + \" minutes\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I we have `SLURM` available on the system, we can write a project-specific script where we define the paths and use the `sbatch` command to execute the generic script for each subject in parallel using `SLURM`. \n",
    "\n",
    "**Example script to perform first-level analysis on multiple subjects**: [step08_first_level_analysis.sh](code-examples/step08_first_level_analysis.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "874.85px",
    "left": "2183px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
