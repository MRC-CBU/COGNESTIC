{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This notebook is based closely on demo_bootstrap.ipynb from the RSA toolbox.\n",
    "\n",
    "# Getting started exercise for RSA3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "These three exercises will introduce the functionality of the new RSA toolbox for inferring the underlying model representation based on measured data. Generally we assume that there is a true underlying representation, which is captured by our model. The measurement process (like fMRI) will lead to a distorted view of that representation, which we may or may not include in our analysis as an explicit measurement model.  \n",
    "\n",
    "For illustration, these exercises use simulated RDMs from the paper \"[Inferring brain-computational mechanisms with models of activity measurements](https://royalsocietypublishing.org/doi/10.1098/rstb.2016.0278)\" by Kriegeskorte & Diedrichsen (2016). Ground truth RDMs were simulated based on the layers of Alexnet--the deep neural network model, which sparked the interest in deep learning. Simulated data RDMs were generated as follows: First, voxel responses were generated by randomly selecting locations within the layer and modelling their response as a local average of the feature values. Then, noise was added to those voxel responses and RDMs were computed from these noisy responses. As model predictions to compare to, we use noise-free RDMs generated for each layer, by applying different amounts of smoothing and averaging to the layer representation. \n",
    "\n",
    "Our overall aim in this setting is to infer which representation the data RDMs were based on, i.e. which layer was used for generating the data. Towards this aim we will make three steps:\n",
    "\n",
    "In *Exercise 1*, we will load the data, convert them into the formats used in the toolbox and have a first exploratory look at the data.\n",
    "\n",
    "In *Exercise 2*, we will compare the RDMs based on the undistorted representations to the simulated data RDMs. This is the classical and simplest approach and already allows us to perform model comparisons and the general evaluation of model-RDMs. This approach uses *fixed models*, i.e. each model predicts a single, fixed RDM. We will see that this may not allow us to correctly infer the underlying representation though, because the measurement process distorts the RDMs.\n",
    "\n",
    "((DJM note: actually it *is* possible to infer the correct underlying representation based on fixed models, at least in this example, depending on the options chosen.))\n",
    "\n",
    "In *Exercise 3*, we will apply *flexible models*. This means that each model predicts a distribution of RDMs. In the present context this means that the model is flexible in which measurement model is applied to explain the data. To evaluate such flexible models, additional cross-validation is necessary, which we also discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Data and RDM handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import rsatoolbox\n",
    "import time     \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model RDMs\n",
    "Here the models are different layers of Alexnet.\n",
    "For each layer, different models of how the fMRI voxels sample the neurons are being considered.\n",
    "\n",
    "The simulated data were generated in Matlab ([Kriegeskorte & Diedrichsen 2016](https://royalsocietypublishing.org/doi/10.1098/rstb.2016.0278)). Thus, we load the Matlab files in .mat format.\n",
    "\n",
    "For each model-RDM, we obtain:\n",
    "- the RDM itself,\n",
    "- a \"model_layer\" name, which specifies the layer used to generate the RDM.\n",
    "- a \"model_model_measurement_model\" name, which specifies the applied distortions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_data = io.matlab.loadmat('rsatoolbox_demos_2025/rdms_inferring/modelRDMs_A2020.mat')\n",
    "matlab_data = matlab_data['modelRDMs']\n",
    "n_models = len(matlab_data[0])\n",
    "\n",
    "rdms_array     = np.array([matlab_data[0][i][3][0] for i in range(n_models)])\n",
    "model_layer             = [matlab_data[0][i][0][0] for i in range(n_models)]\n",
    "model_measurement_model = [matlab_data[0][i][1][0] for i in range(n_models)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are not specific to the toolbox, but to the format in which the RDMs were originally saved.\n",
    "To load other data, simply transform them such that they are numpy arrays of either the whole RDM or vector format of the upper triangular part of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the model RDMs as an rsatoolbox object\n",
    "We place the RDMs in an rsatoolbox object, which can contain additional descriptors for the RDMs and the experimental conditions.\n",
    "Here we label each RDM with the name of the brain-computational model (AlexNet layer) and the name of the measurement model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_rdms = rsatoolbox.rdm.RDMs(rdms_array,\n",
    "                            rdm_descriptors={'brain_computational_model':model_layer,\n",
    "                                             'model_measurement_model':model_measurement_model},\n",
    "                            dissimilarity_measure='Euclidean'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `model_rdms` is now a custom object, which contains all the RDMs from the .mat file with the additional information.\n",
    "It also has a few methods for forming subsets of the data, saving and loading, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the RDMs from AlexNet layer conv1\n",
    "\n",
    "As a simple example, select the RDMs that correspond to the first convolutional layer. These can then be plotted using the function `rsatoolbox.vis.show_rdm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv1_rdms = model_rdms.subset('brain_computational_model','conv1')\n",
    "rsatoolbox.vis.show_rdm( rsatoolbox.rdm.rank_transform( conv1_rdms ), \n",
    "                         show_colorbar  = 'figure', \n",
    "                         rdm_descriptor = 'model_measurement_model', \n",
    "                                figsize = (10,10), \n",
    "                                   cmap = 'classic');\n",
    "\n",
    "# DJM: I rank transformed the RDMs, because otherwise setting   show_colorbar = 'figure'   would \n",
    "# saturate some of the RDMs, because they have very different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the simulated RDMs that were generated from convolutional layer 1 by different measurement models. Each RDM is labeled with the name of the measurement model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print information about a set of RDMs\n",
    "The rsatoolbox objects can simply be passed to the print function to obtain a short description of their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_rdms = model_rdms.subset('brain_computational_model','conv1')\n",
    "print(conv1_rdms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Of course, you can also show all RDMs or select any other subset. Have a look at the different RDMs!\n",
    "\n",
    "How many RDMs are there for each layer?\n",
    "\n",
    "Generate a plot which shows all RDMs with the 'complete' measurement model.\n",
    "\n",
    "How different do the different measurement models look to you and how different do the different layers look?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_rdms = model_rdms.subset('model_measurement_model','complete')\n",
    "#print(complete_rdms)\n",
    "rsatoolbox.vis.show_rdm( rsatoolbox.rdm.rank_transform( complete_rdms ), \n",
    "                         show_colorbar  = 'figure', \n",
    "                         rdm_descriptor = 'brain_computational_model', \n",
    "                                figsize = (10,10), \n",
    "                                   cmap = 'classic');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Fixed model inference\n",
    "### Load data RDMs\n",
    "Here we use simulated data to demonstrate RSA inference.\n",
    "Since we know the true data-generating model in each case, we can tell when inference fails or succeeds.\n",
    "\n",
    "For each data RDM, we obtain the name of the underlying Layer, a full width at half maximum (FWHM) value and a noise standard deviation. The FWHM value specifies the spatial range the simulated voxels average over. The noise standard deviation specifies how much noise was added to the voxel responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matlab_data = io.matlab.loadmat('rsatoolbox_demos_2025/rdms_inferring/noisyModelRDMs_A2020.mat')\n",
    "repr_names_matlab = matlab_data['reprNames']\n",
    "fwhms_matlab = matlab_data['FWHMs']\n",
    "noise_std_matlab = matlab_data['relNoiseStds']\n",
    "rdms_matlab = matlab_data['noisyModelRDMs']\n",
    "repr_names = [repr_names_matlab[i][0][0] for i in range(repr_names_matlab.shape[0])]\n",
    "fwhms = fwhms_matlab.squeeze().astype('float')\n",
    "noise_std = noise_std_matlab.squeeze().astype('float')\n",
    "rdms_matrix = rdms_matlab.squeeze().astype('float')\n",
    "\n",
    "print('\\nShape of loaded RDM matrix: ', rdms_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each RDM has 4186 entries.\n",
    "There are 9 underlying representations (layers of CNN).\n",
    "For each of these, there are 5 levels of smoothing, and 4 levels of added noise.\n",
    "For each combination, measurements are sampled 12 times.\n",
    "\n",
    "### Choose the data RDMs for inference\n",
    "\n",
    "Here we choose which data RDMs we use for the exercise. You can change the representation, the noise level and the amount of averaging by changing the index values at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# indices choosing brain-computational model, noise level, and the size of the kernel with which each voxel samples the neural activity\n",
    "i_rep   = 2 #np.random.randint(len(repr_names)) \n",
    "i_noise = 1 #np.random.randint(len(noise_std))\n",
    "i_fwhm  = 0 #np.random.randint(len(fwhms))\n",
    "\n",
    "# print the chosen representation definition\n",
    "repr_name = repr_names[i_rep]\n",
    "print('The chosen ground truth model is:')\n",
    "print(repr_name)\n",
    "print('with noise level:')\n",
    "print(noise_std[i_noise])\n",
    "print('with averaging width (full width at half magnitude):')\n",
    "print(fwhms[i_fwhm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We then convert the chosen data RDMs (12 samples drawn from the chosen model) into an rsatoolbox RDMs object, and display them as we did for the model RDMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the rdms into an RDMs object and show it\n",
    "rdms_data = rsatoolbox.rdm.RDMs(rdms_matrix[:, i_rep, i_fwhm, i_noise, :].transpose())\n",
    "\n",
    "rsatoolbox.vis.show_rdm(rdms_data, figsize=(10,10), cmap='classic');\n",
    "#print(rdms_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fixed models\n",
    "An \"RDM model\" is an rsatoolbox object that can predict a data RDM.\n",
    "For example, a \"flexible\" RDM model may contain a set of predictor RDMs, which predict the data RDM as a weighted combination.\n",
    "Here we use \"fixed\" RDM models, which contain just a single RDM with no parameters to be fitted.\n",
    "\n",
    "Models are generated by first choosing the RDM, in this case one for each \"brain_computational_model\" and the \"model_measurement_model\" \"complete\", which corresponds to no distortions added. This object is then passed to the function `rsatoolbox.model.ModelFixed`, which generates a fixed RDM model. These RDM models are then collected in the list `models`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for i_model in np.unique(model_layer):\n",
    "    rdm_m = model_rdms.subset('brain_computational_model', i_model).subset('model_measurement_model','complete')\n",
    "    m = rsatoolbox.model.ModelFixed(i_model, rdm_m)\n",
    "    models.append(m)\n",
    "\n",
    "print('created the following models:')\n",
    "for i in range(len(models)):\n",
    "    print(models[i].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model RDMs to \"measured\" (\"data\") RDMs\n",
    "Evaluate models naively, i.e. simply compute the average correlation to the data RDMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdms_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_1 = rsatoolbox.inference.eval_fixed(models, rdms_data, method='corr')\n",
    "rsatoolbox.vis.plot_model_comparison(results_1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these plots, the errorbars are based on the variability across subjects, which can be handled analytically. The lines above the plot show which pairwise comparisons are significant. The upper bound of the noise ceiling is computed by finding the RDM with the highest possible average similarity to the measured RDMs. This is not 1, because the RDMs for different subjects or measurements differ. The lower bound of the noise ceiling is a leave-one-out crossvalidation of this averaging procedure, i.e. we find the RDM to perform optimally on all but one of the RDMs and evaluate this average RDM on the left-out RDM. Each RDM is left out once and the correlations are averaged.\n",
    "\n",
    "To take the many pairwise model comparisons into account in performing inference, we can choose a correction for multiple comparisons: We can either control the family wise error rate (FWER) or the false discovery rate (FDR). Here we use a Bonferroni correction for FWER and the Benjamini-Hochberg procedure for FDR.\n",
    "\n",
    "The basic information from a results object can also be printed with the simple print command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the results object has a few functions called `get_*` to access results like the mean values, error bars, the noise ceiling etc., and `test_*` functions that return the p-values of the various tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "To estimate our uncertainty about the models' performance, once we want to include the uncertainty due to random stimulus selection, we can perform bootstrapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparison by bootstrapping the subjects\n",
    "We *could* bootstrap resample the subjects, which estimates how variable the model performances would be if we repeted the experiment with the same stimuli but new subjects from the same population. Based on that uncertainty estimate, we can statistically compare model performances.\n",
    "\n",
    "However, in the limit of many bootstrap samples, this estimate converges exactly to the variance we get from the much faster analytical solution used by eval_fixed. Thus, this analysis is usually not used in practice. It is the simplest version of bootstrapping though, and so a good starting point to illustrate the use of the bootstrap. The following should thus produce very similar results as `results_1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2a = rsatoolbox.inference.eval_bootstrap_rdm(models, rdms_data, method = 'corr')\n",
    "# defaults to 1000 samples; takes ~ 16 s\n",
    "rsatoolbox.vis.plot_model_comparison(results_2a); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparison by bootstrapping the stimuli\n",
    "We can alternatively bootstrap resample the stimuli, to estimate how much model performance would vary if we repeated the experiment with the same subjects using a new sample of stimuli from the same population. This analysis is not analytically solvable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2b = rsatoolbox.inference.eval_bootstrap_pattern(models, rdms_data, method='corr')\n",
    "# defaults to 1000 samples; takes ~ 30 s\n",
    "rsatoolbox.vis.plot_model_comparison(results_2b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model comparison by bootstrapping both stimuli and subjects\n",
    "Finally, we can bootstrap resample both stimuli and subjects to estimate how variable the model performances would be if we repeated the experiment with new subjects and new stimuli from their respective populations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2c = rsatoolbox.inference.eval_bootstrap(models, rdms_data, method='corr')\n",
    "# defaults to 1000 samples; takes ~ 22 s\n",
    "rsatoolbox.vis.plot_model_comparison(results_2c);\n",
    "print(results_2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the paper \"[Statistical inference on representational geometries](https://elifesciences.org/articles/82566)\" notes that this estimate is overly conservative (too large errorbars), and suggests a correction based on combining the errorbars from bootstrapping both stimuli and subjects with the separate bootstraps for the two factors. This is implemented in the `eval_dual_bootstrap` method, although we won't run it as it's quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_2d = rsatoolbox.inference.eval_dual_bootstrap(models, rdms_data, method='corr')\n",
    "# # defaults to 1000 samples; takes ~ 1.5 mins\n",
    "# rsatoolbox.vis.plot_model_comparison(results_2d);\n",
    "# print(results_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- Do the mean estimates from bootstrapping differ from the evaluations over the whole dataset?\n",
    "- Does the right model win?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DJM: Whitening the RDMs\n",
    "\n",
    "Notice that here the correct model (\"conv3\", from which the data were generated) did *not* give the best fit to the data (the best fit was \"fc7\"). This is because the data contain smoothing and noise (via the \"measurement model\"), which are not present in the fixed models tested. \n",
    "\n",
    "However, by using a different RDM comparison method, `corr_cov`, which adjusts for autocorrelations within the RDMs  (see [Diedrichsen et al., 2021](https://arxiv.org/abs/2007.02789)), the correct model *does* win.\n",
    "\n",
    "Here, we will also sort the bar chart, which makes the arrow plot more concise and easier to interpret. Notice, too, that the prediction accuracies are now substantially higher, and the error bars are substantially smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1b = rsatoolbox.inference.eval_fixed(models, rdms_data, method='corr_cov')\n",
    "rsatoolbox.vis.plot_model_comparison(results_1b, test_pair_comparisons='arrows', sort='descend');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Crossvalidation for flexible models\n",
    "### Defining flexible models\n",
    "Here we use a type of flexible model called a *selection model*. This type of model specifies that the true RDM is one from a list of RDMs. To evaluate flexible models, they have to be fitted to data, i.e. we need to provide some data, which can be used to adjust the RDM-prediction of the model. For a selection model, the fitting process simply selects the RDM that performs best on the training data. The model will perform better on this data than on independent data. An unbiased performance estimate therefore requires independent test data. Crossvalidation is a data-efficient way of obtaining an unbiased performance estimate.\n",
    "\n",
    "We first have to generate the selection models. This process is the same as for fixed models, but uses `rsatoolbox.model.ModelSelect` and passes multiple RDMs instead of a single one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_flex = []\n",
    "for i_model in np.unique(model_layer):\n",
    "    MeasurementModelsForThisLayer = model_rdms.subset('brain_computational_model', i_model)\n",
    "    models_flex.append( rsatoolbox.model.ModelSelect(i_model, MeasurementModelsForThisLayer) )\n",
    "\n",
    "print('Created the following flexible models:')\n",
    "for i in range(len(models_flex)):\n",
    "    print(models_flex[i].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('There are still ', len(models_flex), ' models...')\n",
    "print('But now each model contains multiple candidate RDMs:')\n",
    "for i in range(len(models_flex)):\n",
    "    print((models_flex[i]).rdm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossvalidation\n",
    "As a first step, we split our data into training and test sets, which should not share either subjects or stimuli. To do so, we split each dimension into k groups and leave one of these groups out as a test set and use all others as training data. Models choose their parameters to maximize performance on the training set and are evaluated on the test set. Additionally a so-called *ceil set* is created, which contains the data from the training subjects for the test stimuli, which is necessary for calculating a noise ceiling.\n",
    "\n",
    "The variables `k_pattern` and `k_rdm` specify how many folds should be formed over stimuli and subjects, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set, test_set, ceil_set = rsatoolbox.inference.sets_k_fold(rdms_data, k_pattern=3, k_rdm=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these sets we can now evaluate our models, as we did without crossvalidaton, and plot the results. The performance estimates will be averaged across folds, to obtain a single performance estimate without errorbars. (The variability over cross-validation folds are not indicative of the variability across independent datasets. Although training and test data are independent of each other in each fold, performance estimates are not independent across folds of crossvalidation, because folds share the same training data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3_cv = rsatoolbox.inference.crossval(models_flex, rdms_data, train_set, test_set,\n",
    "                                             ceil_set = ceil_set, method='corr')\n",
    "# plot results\n",
    "rsatoolbox.vis.plot_model_comparison(results_3_cv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapped crossvalidation\n",
    "\n",
    "We can perform bootstrapping around the crossvalidation to get uncertainty estimates for the evaluation and for model comparison. This is implemented in the eval_dual_bootstrap method and can be activated by simply adding the k_pattern and k_rdm inputs. Here we compute only 100 bootstrap samples to keep the computation time short enough for demonstrations. For accurate inference, please run more samples of course!\n",
    "\n",
    "This method internally applies the crossvalidation variance correction introduced in the paper \"[Statistical inference on representational geometries](https://elifesciences.org/articles/82566)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "results_3_full = rsatoolbox.inference.eval_dual_bootstrap(models_flex, rdms_data, k_pattern=2, k_rdm=2, method='corr', N=100)\n",
    "print('Took ', time.time()-tic,' s')\n",
    "# plot results\n",
    "rsatoolbox.vis.plot_model_comparison(results_3_full);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Questions\n",
    "\n",
    "- Does the right model win?\n",
    "- Try some different settings for the crossvalidation: How do the results change when you make the training and test sets larger or smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "306733df669f86703ca68e4138a64697e4e8bd47d8228a728899ed125c5a4740"
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "af6f0c1be22da210ce14b764d3d407b4e31df46360687c396ac7d1fbf0a9a76f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
