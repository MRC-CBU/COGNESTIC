{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Classification in python using scikit-learn\n",
    "\n",
    "Much of this tutorial is based on, or inspired by, the longer fMRI-pattern-analysis course developed by Lukas Snoek at the University of Amsterdam, which you can find here: https://lukas-snoek.com/NI-edu/index.html \n",
    "\n",
    "In this demo, we will start with simulated data; then, in the next demo, we will move on to real fMRI data. However, most of the principles apply to any source of data patterns. See Mate's sessions tomorrow for examples applied to EEG/MEG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Getting Ready\n",
    "\n",
    "To give python the functionality we need, we typically have to **import** a bunch of packages (See Kshipra's session).\n",
    "Many of these you will have  seen in previous sessions. The important package for today is Scikit-learn: this is a popular and powerful library for machine learning in Python. It has a very useful website: https://scikit-learn.org/stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np              # This lets python process matrices, like Matlab\n",
    "import matplotlib.pyplot as plt # This lets python plot graphs like Matlab\n",
    "import seaborn as sns           # This provides another popular set of plotting functions\n",
    "import pandas as pd             # To use \"dataframes\" (like tables in Matlab)\n",
    "import os                       # To interact with the operating system, including files and paths (e.g. path.join)\n",
    "import time                     # To use \"time\" (like \"tic\" in Matlab)\n",
    "\n",
    "import nilearn as nil           # Many useful functions for MRI, including...\n",
    "from nilearn import image       # to load (load_img), resample (resample_to_img), manipulate (math_img) fMRI data, etc.\n",
    "from nilearn import datasets    # includes e.g. fetch_atlas_harvard_oxford\n",
    "from nilearn import masking\n",
    "from nilearn import input_data  # includes NiftiMasker, NiftiLabelsMasker\n",
    "from nilearn import plotting    # includes plot_roi, plot_stat_map, view_img_on_surf, etc.\n",
    "from nilearn import decoding    # includes Searchlight\n",
    "\n",
    "# scikit-learn is the major library for machine learning in Python:\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing   # includes LabelEncoder, OneHotEncoder, StandardScaler...\n",
    "from sklearn import decomposition   # includes PCA\n",
    "from sklearn import model_selection # includes StratifiedKFold, LeaveOneGroupOut, etc....\n",
    "from sklearn import linear_model    # includes LogisticRegression, RidgeClassifier...\n",
    "from sklearn import svm             # includes SVC, NuSVC & LinearSCV...\n",
    "from sklearn import discriminant_analysis # includes LinearDiscriminantAnalysis\n",
    "from sklearn import metrics         # includes accuray, balanced accuracy, roc_auc_score, etc....\n",
    "from sklearn import pipeline        # includes make_pipeline\n",
    "from sklearn import inspection      # includes DecisionBoundaryDisplay\n",
    "\n",
    "# scipy? # provides basic statistic functions, e.g. t-tests, and allows import of Matlab files\n",
    "# glob? # to search filenames with wildcards\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "First, we'll **set the seed to numpy's random number generator**, to get the same sequence of random values every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3) # (if it's not desireable to set a global seed, a seeded RandomState can be passed to the relevant functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Examples using simulated data, with a true effect\n",
    "\n",
    "### Define data matrix and labels:\n",
    "\n",
    "Now, we'll **simulate some data from a single participant**.\n",
    "Let's define **two conditions**, which could be e.g. button presses using two different fingers.\n",
    "We'll simulate **four runs**, with **three repeats** of each condition per run. We'll assume that each repeat has been separately estimated from a GLM (see Rik's & Dace's sessions), so we would have 4 x 3 x 2 = 24 activation patterns. In an fMRI experiment, these might be whole-brain volumes; in E/MEG these might be activations across electrodes. The values in each pattern could be beta coefficients, contrasts, or t statistics.\n",
    "For now, **we'll just consider two voxels** from each activation map, to make things easier to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nruns = 4     # number of runs \n",
    "n_per_run = 3 # number of repetitions per run\n",
    "mu1 = np.array([2.0, -0.5]) # mean activation for condition 1 (1st voxel activated; 2nd voxel deactivated)\n",
    "mu2 = np.array([-0.5, 2.0]) # mean activation for condition 2 (opposite pattern)\n",
    "voxel_covariance = np.diag([0.0, 3.0])+1 # independent noise per voxel; 2nd voxel is noisier; shared noise is positively correlated\n",
    "# print(\"Noise covariance matrix: \\n\", voxel_covariance )\n",
    "\n",
    "data_per_run = []   # list of pattern matrices (one per run)\n",
    "labels_per_run = [] # list of label vectors (one per run)\n",
    "for r in np.arange(nruns):  # loop over runs\n",
    "    a1 = np.random.multivariate_normal(mu1, voxel_covariance, size=n_per_run) # activations for condition 1\n",
    "    a2 = np.random.multivariate_normal(mu2, voxel_covariance, size=n_per_run) # activations for condition 2 (different mean, same noise covarience)\n",
    "    data_matrix = np.concatenate((a1, a2), axis=0) # stack condition 1 (n rows of observations) then condition 2 (n more observations)\n",
    "    # add some extra noise that is similar across conditions and voxels, but whose mean and variance increases across runs:\n",
    "    data_matrix = data_matrix + np.random.normal( r*10, r, size=(2*n_per_run,2))\n",
    "    \n",
    "    label_vector = n_per_run*[\"Thumb\"] + n_per_run*[\"Pinky\"] # give each pattern a label that indicates from which condition it was drawn\n",
    "    \n",
    "    data_per_run.append( data_matrix )  # append this run's data to list\n",
    "    labels_per_run.append( label_vector ) \n",
    "    \n",
    "print(\"\\nData matrix for one run:\\n\", data_per_run[0])\n",
    "print(\"\\nLabel vector for one run:\\n\", labels_per_run[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We now have everything we need for a decoding analysis. However, functions often require data to be represented numerically, so we will also **convert the labels to integers**. This can be done in various ways, but here we'll use the the `LabelEncoder` class from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_enc = skl.preprocessing.LabelEncoder() # this initialises the LabelEncoder object\n",
    "labels_per_run_int = []\n",
    "for r in np.arange(nruns):\n",
    "    labels_per_run_int.append( lab_enc.fit_transform(labels_per_run[r]) )\n",
    "    \n",
    "print(\"Integer labels for all runs:\\n\", labels_per_run_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Notice that we first initialised a `LableEncoder` object, and then called its `.fit_transform` method. Many objects also have `.fit` and `.transform` methods that can be called separately. This procedure is similar for most objects in scikit-learn.\n",
    "\n",
    "Before doing anything else, let's **plot the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_runs  = np.concatenate(data_per_run,axis=0)       # concatenate data across runs\n",
    "concatenated_labels= np.concatenate(labels_per_run_int,axis=0) # concatenate labels across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(concatenated_runs,yticklabels=concatenated_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,8));  # create a matplotlib figure\n",
    "plt.title('Activation patterns as points in n-dimensional space:');\n",
    "scatter = plt.scatter(concatenated_runs[:,[0]], concatenated_runs[:,[1]], \n",
    "                      s= 90, alpha=0.7, c=concatenated_labels, cmap='bwr');\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(labels_per_run[0])); # \"set\" returns unique values\n",
    "plt.xlabel('Voxel 1 activity');\n",
    "plt.ylabel('Voxel 2 activity');\n",
    "plt.rcParams.update({'font.size': 18});\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Initial classification, without cross-validation:\n",
    "\n",
    "At this stage, it is common to **standardise each feature**, e.g. by rescaling into the range [0 1], or by z-scoring (subtracting mean and dividing by standard deviation across samples). For many classifiers, standardizing each feature across all samples should not change the classification accuracy. (Imagine how these transformations change the distribution of the data in the figure above.) However, putting each feature on the scame scale can help parameters to be estimated efficiently, and may be necessary for some models to converge. \n",
    "\n",
    "Consider whether it would it be a good or bad idea to rescale features for each class separately? \n",
    "\n",
    "Would it be a good or bad idea to rescale features for each run separately?\n",
    "\n",
    "For now, let's z-score each feature across all samples of all runs. Again, we can do this by initialising a scikit-learn object and calling its `fit_transform` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler           = skl.preprocessing.StandardScaler()\n",
    "concatenated_runs= scaler.fit_transform(concatenated_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data again to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,8));  # create a matplotlib figure\n",
    "plt.title('Activation patterns as points in n-dimensional space:');\n",
    "scatter = plt.scatter(concatenated_runs[:,[0]], concatenated_runs[:,[1]], \n",
    "                      s= 90, alpha=0.7, c=concatenated_labels, cmap='bwr');\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(labels_per_run[0])); # \"set\" returns unique values\n",
    "plt.xlabel('Voxel 1 activity');\n",
    "plt.ylabel('Voxel 2 activity');\n",
    "plt.rcParams.update({'font.size': 18});\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now we can proceed to classification. We'll start with a logistic regression classifier, which we'll initialise to a variable `LR`.\n",
    "\n",
    "Each **classifier needs to be \"trained\" or \"fit\" using \"training data\" and corresponding labels.** This uses the training data to optimise the parameters of the model, to best match the labels to some function (often a linear combination) of the features.\n",
    "\n",
    "In scikit-learn, this uses the `.fit` method of the classifier, giving the patterns matrix and labels as inputs. To start with, we'll train the classfier using all the data, concatenated across runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = skl.linear_model.LogisticRegression(solver='lbfgs') # there are more options that could be set here; we won't worry about them\n",
    "LR.fit(concatenated_runs, concatenated_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **estimated (fitted) coefficients are now stored in the classifier object**. Notice that for this linear classifier we have one coefficient per feature (voxel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients:\\n\", LR.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We can also plot the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl.inspection.DecisionBoundaryDisplay.from_estimator(LR, concatenated_runs, alpha=0.5, ax=fig.axes[0], cmap='bwr', response_method='predict');\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "The next step is to **\"test\" the model by giving it some patterns and \"predicting\" their labels**. For now, we'll give it the same data we used to train the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = LR.predict(concatenated_runs)\n",
    "print(\"Predictions for all samples:\\n\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "How accurate are these predictions? We can **\"evaluate\" the predictions by comparing them with the true values. The quality of the predictions is summarised as a \"performance\", \"score\" or \"metric\"**. If we had continuous predictions (a regression model), we might correlate them with the true values. Here we have categorical predictions (a classification model), which we can compare with the true values in various ways...\n",
    "\n",
    "The **simplest score, called \"accuracy,\" is just the proportion of all predictions that are correct**. What is the best possible accuracy? What is the worst possible accuracy? What accuracy would we expect by chance?\n",
    "\n",
    "Calculating accuracy is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(concatenated_labels==predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Scikit-learn also has a function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = skl.metrics.accuracy_score(concatenated_labels, predicted_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Actually scikit-learn has **many metrics to choose from.** The only thing to recommend accuracy as a metric is its simplicity. Otherwise, it is relatively insensitive (because binary guesses discard information about the confidence of a particular prediction) and can be misleading if classes are unbalanced (more about that later).\n",
    "\n",
    "A metric that incorporates the confidence of each prediction, and can handle unbalanced classes, but is rather complicated, is the \"**Area Under the Receiver Operating Characteristic curve**\" (AUROC, or just AUC). We can use a function from scikit-learn to calculate this, but instead of vectors of true and predicted labels, it generally needs matrices of true and predicted probabilities per class. (In this binary case, we could give it the vector labels and the probabilities for the class with the largest label, but this syntax doesn't generalise to more than two classes).\n",
    "\n",
    "Matrices of true class probabilities can be constructed from the true labels using scikit-learn's `one-hot encoder` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = skl.preprocessing.OneHotEncoder(sparse_output=False)  # we don't want a \"sparse\" output\n",
    "true_probabilities = ohe.fit_transform(concatenated_labels.reshape(-1,1)) # the reshape is needed because the function expects a 2D input\n",
    "print(\"True probabilities per class, for first 5 patterns:\\n\", true_probabilities[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Matrices of predicted class probabilities can be returned by many classifier objects, including logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = LR.predict_proba(concatenated_runs)\n",
    "print(\"Predicted probabilities per class, for first 5 patterns:\\n\", predicted_probabilities[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "With these, we can now calculate the AUROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROC = skl.metrics.roc_auc_score(true_probabilities, predicted_probabilities)\n",
    "print(\"AUROC:\")\n",
    "print(AUROC)\n",
    "AUROC = skl.metrics.roc_auc_score(concatenated_labels, predicted_probabilities[:,1]) # alternative syntax for binary classifcation\n",
    "print(AUROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Notice that the performance is a bit better than simple accuracy. (Both metrics are bounded between 0 and 1. For AUROC, chance is 0.5. For balanced binary classification, as here, chance accuracy is also 0.5.)\n",
    "\n",
    "So far, so good. BUT... **these results are biased and cannot be trusted, because we tested the model on the same data that we used for training!** This is like taking an exam after having already seen the correct answers. To emphasise the problem, let's create a **new simulated dataset**, with the same trial structure, but a more realistic number of features/voxels (e.g. 30), and **with no true difference between the conditions** (both are drawn at random fom the same distribution):\n",
    "\n",
    "\n",
    "## Examples using simulated data with NO true effect - the importance of cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox = 30                          # number of voxels\n",
    "mu = np.arange(nvox)               # mean activation per voxel (voxels all have different activation strength) for both conditions 1 & 2\n",
    "voxel_covariance = np.diag(mu) + 1 # independent noise per voxel is proportional to mean, plus some covariance\n",
    "\n",
    "null_data_per_run = []             # list of pattern matrices (one for each run)\n",
    "for r in np.arange(nruns):\n",
    "    data_matrix =  np.random.multivariate_normal(mu, voxel_covariance, size= 2 * n_per_run) # activations are drawn from THE SAME DISTRIBUTION FOR BOTH CONDITIONS!\n",
    "    null_data_per_run.append( data_matrix ) \n",
    "\n",
    "null_concatenated_runs = np.concatenate(null_data_per_run,axis=0)             # concatenate runs\n",
    "null_concatenated_runs = scaler.fit_transform(null_concatenated_runs)         # scale data per voxel\n",
    "LR.fit(null_concatenated_runs, concatenated_labels)                           # fit the classifier       \n",
    "null_predicted_labels = LR.predict(null_concatenated_runs)                    # predict the labels\n",
    "accuracy = skl.metrics.accuracy_score(concatenated_labels, null_predicted_labels) # measure accuracy\n",
    "print('True labels:\\n', concatenated_labels)\n",
    "print('Predicted labels from null data:\\n', null_predicted_labels)\n",
    "print('Accuracy from null data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Here we have got perfect classification, despite both conditions being random samples from the same distribution! \n",
    "\n",
    "Let's just **try using a different classifier**, to show that this is a general problem. Another popular classifier is a **Support Vector Machine**. The procedure is essentially the same:\n",
    " - create the classifier object\n",
    " - train (fit) the classifier (using (scaled) patterns and their labels)\n",
    " - test the classifier (predict labels from patterns)\n",
    " - calculate a performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM=skl.svm.LinearSVC(dual=True)\n",
    "SVM.fit(null_concatenated_runs, concatenated_labels)\n",
    "null_predicted_labels = SVM.predict(null_concatenated_runs)\n",
    "accuracy = skl.metrics.accuracy_score(concatenated_labels, null_predicted_labels)\n",
    "print('Accuracy from null data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "With more voxels than conditions (which is common in neuroimaging), the classifier can easily separate conditions based on noise (chance differences). This is called **overfitting**. **When testing on the same data that was for training**, these chance differences are the same, so the **classification performance is positively biased**. \n",
    "\n",
    "We solve this using **cross-validation**.\n",
    "\n",
    "Cross-validation means that the classifier is **tested on separate data**, that was not used for training. Independent data will have different noise. Therefore, overfitting based on noise in the training samples will still occur, but will not generalise to the testing samples, and classifier performance will be **unbiased**.\n",
    "\n",
    "There are different ways to split data into test and train sets. The simplest case is a **single partition, called a \"hold-out\"** scheme. Here we'll split the data into odd and even runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_concatenated_runs_train_odd = np.concatenate(null_data_per_run[0::2],axis=0) # concatenate training data across odd runs\n",
    "null_concatenated_runs_test_even = np.concatenate(null_data_per_run[1::2],axis=0) # concatenate testing data across even runs\n",
    "\n",
    "concatenated_labels_train_odd = np.concatenate(labels_per_run_int[0::2],axis=0) # concatenate training labels across odd runs\n",
    "concatenated_labels_test_even = np.concatenate(labels_per_run_int[1::2],axis=0) # concatenate testing labels across even runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now we can standardise the training data and use these to train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.fit(scaler.fit_transform(null_concatenated_runs_train_odd), concatenated_labels_train_odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we test the classifier on the independent training data, the accuracy should close be to chance (with some random error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_predicted_labels_test_even = LR.predict(scaler.transform(null_concatenated_runs_test_even))\n",
    "accuracy = skl.metrics.accuracy_score(concatenated_labels_test_even, null_predicted_labels_test_even)\n",
    "print('True labels (from even runs):\\n', concatenated_labels_test_even)\n",
    "print('Predicted labels from null data (even runs):\\n', null_predicted_labels_test_even)\n",
    "print('Accuracy from null data (even runs):', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "(Notice here that we used  `scaler.transform`  rather than  `scaler.fit_transform`. This means that  we use the use the standardisation parameters fit using the train data for standardising the test data. One could instead independently standardise the test set, or standardise the whole dataset before partitioning. Opinions differ on whether it matters, and which, if any, is best.)\n",
    "\n",
    "Okay, accuracy is now close to chance, as it should be. But notice that the classifier is now:\n",
    "- trained on only some (here, half) of the data, which makes it less sensitive\n",
    "- tested on only some (here, half) of the data, which makes its performance more variable (less reliable)\n",
    "\n",
    "For more efficient use of the data, we can use **K-fold cross-validation**. This means that the data are split into K parts, and in each of K folds/splits one part is used for testing and the rest are used for training. This way, each fold uses more training data (so more sensitive), and after combining predictions across folds there are more predictions (so a more reliable accuracy estimate). Generally we want the proportion of classes to be as balanced as possible in each fold (and therefore in the training set). To do this we could use scikit-learn's `StratifiedKFold` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "skf = skl.model_selection.StratifiedKFold(n_splits=3) # create the object\n",
    "folds = tuple(skf.split(null_concatenated_runs, concatenated_labels)) # create the folds\n",
    "# note: the folds are returned as a \"generator\", which means that each element can only be accessed once and then disappears; the conversion to a tuple makes them persist.\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "\n",
    "    # fold  is a tuple, which contains indices to train and test samples\n",
    "    train_idx, test_idx = fold # unpack the tuple\n",
    "    \n",
    "    print(\"Fold %i\" % (i + 1))\n",
    "    print(\"Train samples:\", train_idx)\n",
    "    print(\"Test samples:\", test_idx, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Note two things here. \n",
    "\n",
    "Firstly, this is only one of many ways to split our 24 samples into three parts. We could repeat the partitioning with different random splits, using `.RepeatedStratifiedKFold`. This would further increase the total number of predictions, and therefore the precision of the estimated accuracy. However, it would also take longer. Also, note that the total number of unique ways to split into K parts decreases as K increases. Therefore, the **choice of K involves a trade-off between senstivity and precision**.\n",
    "\n",
    "Secondly, here samples from more than one run are spread across folds. For fMRI, this is NOT what we want, because **for fMRI samples within a run are likely not independent**, and thus the train/test splits will also not be independent. Instead, the best and typical approach is to cross-validate across runs. To do this, we can use the **`LeaveOneGroupOut`** object (see also `GroupKFold`, and `LeavePGroupsOut`), and tell it that we want to treat each fMRI run as a group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_indices =  np.concatenate([[i] * 2 * n_per_run for i in range(nruns)]) # get the indices of each run\n",
    "print(\"Run indices:\",run_indices,\"\\n\")\n",
    "\n",
    "logo = skl.model_selection.LeaveOneGroupOut()\n",
    "folds = tuple(logo.split(null_concatenated_runs, concatenated_labels, run_indices))\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    train_idx, test_idx = fold\n",
    "    print(\"Fold \", i, \" test samples:\", test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "It can be cumbersome to specify every step within a loop across the folds (e.g. scaling the data, training the classifier, testing the classifier, plus any extra steps). This can increase the risk of accidentally using test data when pre-processing or training the classifier, or accidentally applying different pre-processing to train and test sets.\n",
    "\n",
    "Scikit-learn has a mechanism called **\"pipelines\"** to make this more efficient, and reduce the risk of such errors. To make a pipeline, we call the `make_pipeline` function, and give it the objects for each step of the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = skl.pipeline.make_pipeline(scaler, SVM)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Then, in the cross-validation loop, we can just call the pipeline's `.fit` method on the training data and its `.predict` method on the test data, to automatically apply all these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = np.full(nruns,np.nan)\n",
    "for i, fold in enumerate(folds):\n",
    "    train_idx, test_idx = fold # get the train and test indices\n",
    "    pipe.fit(null_concatenated_runs[train_idx,:],concatenated_labels[train_idx]) # do all preprocessing and model fitting on training data\n",
    "    null_predicted_labels = pipe.predict(null_concatenated_runs[test_idx,:])     # do prediction on test data\n",
    "    accuracy[i] = skl.metrics.accuracy_score(concatenated_labels[test_idx], null_predicted_labels) # score the predictions\n",
    "print(\"Accuracy per fold:\", accuracy)\n",
    "print(\"Mean accuracy for random data:\", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "To make things *even easier*, we can pass the pipeline, cross-validation scheme, and performance metric to a single function, to do everything in one go: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = skl.model_selection.cross_val_score(pipe, null_concatenated_runs, concatenated_labels,\n",
    "                                    groups  = run_indices,\n",
    "                                    scoring = 'accuracy',\n",
    "                                    cv      = logo)\n",
    "print(\"Crossvalidated accuracy per fold for random data:\", accuracy)                   \n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned accuraccies per fold should be the same as above.\n",
    "\n",
    ".\n",
    "\n",
    "### Final example using simulated data:  true effect with cross-validation:\n",
    "Now that we have a nice cross-validated pipeline, let's return to the original simulated data that *does* have a difference between conditions, and check that the decoding accuracy is well above chance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = skl.model_selection.cross_val_score(pipe, concatenated_runs, concatenated_labels,\n",
    "                                    groups  = run_indices,\n",
    "                                    scoring = 'accuracy',\n",
    "                                    cv      = logo)\n",
    "print(\"Crossvalidated accuracy per fold for original data:\", accuracy)  \n",
    "print(\"Mean crossvalidated accuracy for original data:    \", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples using real brain data\n",
    "\n",
    "Okay, time to use some real brain data! \n",
    "\n",
    "We'll use the 3T dataset from the fMRI-pattern-analysis course developed by Lukas Snoek at the University of Amsterdam: https://lukas-snoek.com/NI-edu/index.html. This uses stimuli from the publicly available \"Face Research Lab London Set\" (https://figshare.com/articles/dataset/Face_Research_Lab_London_Set/5047666). Participants were shown 480 images of faces for 1.25 seconds each, separated by 3.75 seconds. Faces counterbalanced sex (male, female) and ethnicity (Caucasian, East-Asian, Black) across 40 identities. Each identity was shown six times with a neutral expression, and six times smiling. Stimuli were divided into 12 runs, across two days, with 40 stimuli per run. 1 in 8 faces were followed by a judgement of attractiveness, dominance or trustworthiness. Functional scans were acquired at a TR of 0.7 s and an isotropic resolution of 2.7 mm. Response patterns, in MNI space, were estimated for individual trials.\n",
    "\n",
    "These activation patterns have already been downloaded. Let's look at the data from one session of one participant:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get directory with data:\n",
    "#data_dir=\"/group/duncan-lab/users/dm01/COGNESTIC24/NI-edu-data-minimal-djm\";\n",
    "data_dir=\"NI-edu-data-minimal-djm\";\n",
    "pattern_dir = os.path.join(data_dir, 'derivatives', 'pattern_estimation', 'sub-03', 'ses-1', 'patterns')\n",
    "\n",
    "# Check what's in it:\n",
    "print('\\n'.join(sorted(os.listdir(pattern_dir))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of six runs, there is a .tsv file that contains the definition of each trial in the run, plus a 4D nifti file ending in \"_beta.nii.gz\", which conatins the activation patterns we will use.\n",
    "First, from the events file of run 1, we'll **extract class labels** that indicate whether each face was \"smiling\" or \"neutral\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events-file corresponding to run 1\n",
    "events_file = os.path.join(pattern_dir, 'sub-03_ses-1_task-face_run-1_events.tsv') \n",
    "events = pd.read_csv(events_file, sep='\\t') # read into pandas dataframe\n",
    "pd.options.display.width=250\n",
    "print(events.head(15))\n",
    "\n",
    "# Filter out 'response' and 'rating' events\n",
    "events = events.loc[events['trial_type'].str.contains('STIM'), :]\n",
    "\n",
    "# Encode the string labels into integers\n",
    "labels = lab_enc.fit_transform(events['expression']) # Encode the \"expression\" column (containing either \"smiling\" or \"neutral\") as integers\n",
    "print('Labels:\\n', labels)  # smiling = 1, neutral = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now let's **load the fMRI data** from run 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_path = os.path.join(pattern_dir, 'sub-03_ses-1_task-face_run-1_space-MNI152NLin2009cAsym_desc-trial_beta.nii.gz')\n",
    "patterns_4D = nil.image.load_img(betas_path)\n",
    "\n",
    "print(f\"Shape of betas: {patterns_4D.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "The first three dimensions are the x, y, and z coordinates.\n",
    "The 4th dimension is trials. We can again see that there were 40 trials (faces) presented in this run. \n",
    "\n",
    "We don't expect all of these voxels to contain signal (some are not even in the brain!). So we will **select voxels from a Region Of Interest (ROI).** This is a type of **feature selection**, which restricts the analysis to brain regions we are interested in, and/or brain regions where we expect to find signal. To avoid biasing the results, any data used for feature selection must be independent of the data being classified. Since these data are in MNI space, we can use any mask independently defined in MNI space. Here, we'll use Nilearn to load anatomical ROIs from the Harvard-Oxford subcortical atlas. These are returned as a dictionary that stores the `filename`, `description` and, importantly, `labels` (ROI names) and `maps` (a 3D volume of integers that index the ROI names):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a subcortical, maximum-probability, anatomical atlas, thresholded at 25%, with 2 mm resolution\n",
    "HO_atlas = nil.datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm',data_dir) \n",
    "print(\"File name of atlas:\", HO_atlas['filename'])\n",
    "# print(HO_atlas['description'])\n",
    "print(\"\\nROI names:\\n\", HO_atlas['labels'])\n",
    "# print(\"ROI mask:\\n\", HO_atlas['maps'])\n",
    "print(\"\\nShape of ROI map:\\n\", HO_atlas['maps'].shape)\n",
    "nil.plotting.plot_roi(HO_atlas['filename']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, although the functional data and the ROI masks are both in MNI space, they have different dimensions. We can use nilearn to **resample** the ROIs to match the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_ROI_map = nil.image.resample_to_img(HO_atlas['maps'], patterns_4D, interpolation='nearest')\n",
    "# note that for continuous-valued data we would want continuous interpolation, but here nearest-neighboour interpolation preserves the integer ROI values \n",
    "print(\"Shape of resampled ROI map:\", resampled_ROI_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now we'll select the amygdalae, and use nilearn's `math_img` function to turn them into a binary mask. It's sensible to plot this, to check that it looks correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l_amyg_id = HO_atlas['labels'].index('Left Amygdala')\n",
    "r_amyg_id = HO_atlas['labels'].index('Right Amygdala')\n",
    "print(\"Indices of left and right amygdala:\", l_amyg_id, r_amyg_id)\n",
    "\n",
    "amyg_ROI = nil.image.math_img(f'(map == {l_amyg_id}) | (map == {r_amyg_id})', map=resampled_ROI_map)\n",
    "nil.plotting.plot_roi(amyg_ROI);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass this ROI to a `masker` object, and apply it to the activation patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = nil.input_data.NiftiMasker(amyg_ROI)\n",
    "patterns_ROI = masker.fit_transform(patterns_4D) \n",
    "# (if we hadn't resampled the ROI, the masker could also do this; we would provide \"target_affine\" as the space of the data)\n",
    "\n",
    "print('Shape of pattern matrix from chosen ROI:', patterns_ROI.shape)\n",
    "\n",
    "ax = sns.heatmap(patterns_ROI, cmap='coolwarm', center=0, robust=1, xticklabels=50, yticklabels=10)\n",
    "ax.set(xlabel=\"voxels\", ylabel=\"samples / patterns / faces\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We can see that some of these voxels are correlated, and some will be noisier than others. Classifiers are often able to handle this, but we may choose to apply **dimension reduction**. This is sometimes called **feature extraction**, and a common method is principle component analysis (**PCA**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_ROI = scaler.fit_transform (patterns_ROI)\n",
    "\n",
    "pca = skl.decomposition.PCA(n_components=10)\n",
    "pca.fit(patterns_ROI)\n",
    "patterns_ROI_PCA10 = pca.transform(patterns_ROI)\n",
    "\n",
    "print(\"Shape of pattern matrix after PCA:\", patterns_ROI_PCA10.shape)\n",
    "\n",
    "ax = sns.heatmap(patterns_ROI_PCA10, cmap='coolwarm', center=0, robust=1, xticklabels=1, yticklabels=10)\n",
    "ax.set(xlabel=\"Principal components\", ylabel=\"samples / patterns / faces\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that features are typically standardized before PCA, otherwise the principal components will be dominated by features with larger scales. However, standardization means that features that originally had low variance will be upweighted, under the assumption that all features are expected to be equally predictive. In fMRI, noisy/non-predictive features may have low variance, so upweighting them may be detrimental to performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now let's put everything together: load data from all six runs of this day..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_data_per_run   = [] # list of ROI's pattern matrices (one for each run)\n",
    "brain_data_per_run = [] # also save the whole brain data for running a searchlight later\n",
    "labels_per_run     = [] # list of label vectors (one for each run)\n",
    "for day in [1]:\n",
    "    pattern_dir = os.path.join(data_dir, 'derivatives', 'pattern_estimation', 'sub-03', f'ses-{day}', 'patterns')\n",
    "\n",
    "    for run in (np.arange(6)+1):\n",
    "        betas_file = os.path.join(pattern_dir, f'sub-03_ses-{day}_task-face_run-{run}_space-MNI152NLin2009cAsym_desc-trial_beta.nii.gz')\n",
    "        print(betas_file)\n",
    "        patterns_4D = nil.image.load_img(betas_file) \n",
    "\n",
    "        patterns_4D = nil.image.clean_img(patterns_4D, standardize=True, detrend=False)\n",
    "        \n",
    "        patterns_ROI = masker.fit_transform(patterns_4D) \n",
    "\n",
    "        events_file = os.path.join(pattern_dir, f'sub-03_ses-{day}_task-face_run-{run}_events.tsv')\n",
    "        events = pd.read_csv(events_file, sep='\\t') # read into pandas dataframe\n",
    "        events = events.loc[events['trial_type'].str.contains('STIM'), :] # Filter out 'response' and 'rating' events\n",
    "        label_vector = lab_enc.fit_transform(events['expression']) # Encode the \"expression\" column (containing either \"smiling\" or \"neutral\") as integers\n",
    "\n",
    "        # patterns_ROI = scaler.fit_transform (patterns_ROI)\n",
    "        \n",
    "        ROI_data_per_run.append( patterns_ROI )  # append this run's data to list\n",
    "        brain_data_per_run.append( patterns_4D )\n",
    "        labels_per_run.append( label_vector ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "...and try to decode expression within the amygdala ROI, using leave-one-run-out cross-validatation. This time we'll use yet another classifier - a Fisher's linear discriminant classifer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nruns=(len(ROI_data_per_run))\n",
    "npatterns=(np.size(ROI_data_per_run[0],axis=0))\n",
    "run_indices = np.concatenate([[i] * npatterns for i in range(nruns)])\n",
    "all_data    = np.concatenate(ROI_data_per_run, axis=0) # concatenate data across runs\n",
    "all_labels  = np.concatenate(labels_per_run, axis=0) # concatenate labels across runs\n",
    "\n",
    "LDA  = skl.discriminant_analysis.LinearDiscriminantAnalysis();\n",
    "pipe = pipeline.make_pipeline(LDA)\n",
    "\n",
    "fold_score = skl.model_selection.cross_val_score(pipe, all_data, all_labels, groups = run_indices,\n",
    "                                    scoring = 'roc_auc', cv = logo)\n",
    "observed_score = np.mean(fold_score)\n",
    "print(\"Cross-validated score per fold:\", fold_score)  \n",
    "print(\"Mean cross-validated score:  \", observed_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance testing\n",
    "\n",
    "This is only just above chance. Is it significantly above chance? (See Rik's lectures.) We cannot test this using a t-test across folds, because the folds are not independent (they share training data). If we had multiple participants we could use a **one-sample t-test across participants**, versus chance. This is valid (assuming the specified chance level is correct) because participants are independent. For \"within-context\" cross-validation, below-chance accuracy is not meaningful (the true value can't be negative), which means we can use a one-tailed test; it also means that the test only provides fixed-effects rather than random-effects inference (see Allefeld et al., 2016). For cross-validation that generalises *across* contexts (e.g. train the classifer to decode the expression of younger faces, then test decoding of the expression of older faces) negative performance *could* be meaningful, so random-effects inference is justified. Similarly, when comparing classfication performance across conditions (e.g. is the performance of expression decoding for younger faces different from that for older faces?), paired-tests or repeated-measures ANOVA allow random-effects inference.\n",
    "\n",
    "If we want **to assess significance for a single participant, we would need a permutation test**. (Actually this applies to testing the significance of any single *classification* that can't be repeated across independent samples, e.g. a single classification of the disease status of multiple partarticipants.) The permutation test involves shuffling the labels many times (e.g. 10,000) to generate a null distribution of classifier performance that would be expected under the null hypothesis of no difference between classes.\n",
    "\n",
    "In some situations, permutation tests may be necessary, but they do have disadvantages:\n",
    " - the large number of required permutations makes them slow\n",
    " - low numbers of samples limit the number of unique permutations, which limits robustness of the p-value\n",
    " - there's some complexity in ensuring the labels are actually \"exchangeable\"\n",
    "\n",
    "First we'll code this explicitly within the cross-validation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = tuple(logo.split(all_data, all_labels, run_indices))\n",
    "\n",
    "n_permutations = 200 # this is not nearly enough, but it will still take a while\n",
    "permuted_scores = np.zeros(n_permutations)\n",
    "print(f'Permuting {n_permutations} times:')\n",
    "tic = time.time()\n",
    "for p in range(n_permutations):\n",
    "    \n",
    "    fold_score=np.full(nruns,np.nan)\n",
    "    for i, fold in enumerate(folds):\n",
    "        train_idx, test_idx = fold   \n",
    "\n",
    "        #### previously we trained the classifier like this:\n",
    "        # pipe.fit( all_data[train_idx,:], all_labels[train_idx] )\n",
    "        #### now we want to do something like this:\n",
    "        # pipe.fit( all_data[train_idx,:], np.random.shuffle(all_labels[train_idx]) )\n",
    "        #### but confusingly, we need to break it up like this:\n",
    "        train_labels = all_labels[train_idx] \n",
    "        np.random.shuffle(train_labels)\n",
    "        pipe.fit(all_data[train_idx,:], train_labels)\n",
    "        #### because indexing makes a temporary copy, and shuffle operates in-place\n",
    "\n",
    "        predicted_probabilities = pipe.predict_proba(all_data[test_idx,:]) # do prediction on test data\n",
    "        fold_score[i] = skl.metrics.roc_auc_score(all_labels[test_idx], predicted_probabilities[:,1]) # syntax for binary classifcation\n",
    "    permuted_scores[p] = np.mean(fold_score)\n",
    "    print('.',end='')\n",
    "print('Done.')\n",
    "print('Took ', time.time()-tic,' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the observed score and compare it to the null distribution of scores from permuted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(permuted_scores, element='step', alpha=0.5)\n",
    "ax.set(xlabel = 'AUROC of permuted data');\n",
    "lh = ax.axvline(np.percentile(permuted_scores, 95), color='b',label='95% threshold')\n",
    "mh = ax.plot(observed_score, 0, marker='o', color='r', markersize=10, label='observed score')\n",
    "ax.legend();\n",
    "\n",
    "p = (sum(permuted_scores > observed_score) + 1) / (n_permutations + 1)\n",
    "print('p vlaue = ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there is no evidence for significant classification of expression within this ROI.\n",
    "\n",
    "\\\n",
    "Just as scikit-learn has a function (`cross_val_score`) to simplify the cross-validation loop, it also has a similar function (`permutation_test_score`) to simplify the permutation process. This also allows the permutation to be run in parallel, which can be much faster! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "actual_score, permuted_scores, p = skl.model_selection.permutation_test_score(pipe, all_data, all_labels, groups = run_indices,\n",
    "                                    scoring = 'roc_auc', cv = logo, n_permutations = n_permutations, n_jobs = -1, random_state = None)\n",
    "\n",
    "print('Took ', time.time()-tic,' s')\n",
    "print(\"p value = \", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This p-value should, on average, be similar to above, but will differ because of the randomness of the permutations. As the number of unique permutations increases, the p-values from different repetitions should become more similar, giving an idea of the robustness of the permutation-p-value.\n",
    "\n",
    "\\\n",
    "So far we have considered a single ROI. If we want to know where in the brain a representation is strongest, we can compare multiple ROIs. Sometimes we might be interested in discrete regions that cover the brain (e.g. an atlas/parcellation), but if we want a spatially continuous output, without making assumptions about area borders, we can use a \"searchlight\". \n",
    "\n",
    "## Searchlight analysis\n",
    "\n",
    "A searchlight is not really anything special. It's just a set of overlapping ROIs, often spherical, that cover the brain (or analysis mask of interest). Each searchlight's classification performance is typically assigned to its central voxel. The inferences one would make are very similar to a voxel-wise, mass-univariate analysis of smoothed data (see Dace's sessions). Increasing the size of the searchlight reduces the spatial specificity of the inference, while tending to increase sensitivity, in a very similar way to increasing the smoothing kernal in univariate analysis. Similarly to the matched-filter theorem for univariate analyses, the optimal searchlight size will depend on the spatial scale of the signal and the spatial scale of the noise. It is sometimes assumed that searchlights need to be spatially contiguous, because they usually are, but the definition of a searchlight is just as flexible as the definition of any ROI. \n",
    "\n",
    "A searchlight analysis can be set up using nilearn's `.searchlight` object. This *is* restricted to contiguous spherical searchlights. When the searchlight object is created, it needs to be given a mask specifying which voxels will be included in the searchlights. So first we'll get an analysis mask that covers the whole brain. As before, we can load a template-space mask from nilearn, and resample it to match the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_mask = nil.datasets.load_mni152_brain_mask()\n",
    "brain_mask = nil.image.resample_to_img(brain_mask, patterns_4D, interpolation='nearest')\n",
    "\n",
    "has_data =   nil.image.math_img('(img.prod(axis=3) !=0)', img=patterns_4D); # not zero for any sample\n",
    "brain_mask = nil.image.math_img('(in_mask & has_data)', in_mask=brain_mask, has_data=has_data);\n",
    "\n",
    "display = nil.plotting.plot_roi(brain_mask,alpha=0.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We can also provide a second, optional mask that specifies where the searchlights are centred. To save some time, we'll constrain these to be in cortical grey matter of the left hemisphere. We could also load a grey-matter mask from nilearn, but here we'll use the 'Left Cerebral Cortex' region of the atlas we loaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gm_mask = nil.datasets.load_mni152_gm_mask()\n",
    "# gm_mask = nil.image.resample_to_img(gm_mask, patterns_4D, interpolation='nearest')\n",
    "\n",
    "l_cortex_id = HO_atlas['labels'].index('Left Cerebral Cortex')                                # get the ID of the Left Cerebral Cortex\n",
    "gm_mask     = nil.image.math_img(f'(map == {l_cortex_id} )', map=resampled_ROI_map)           # create binary mask of this ROI\n",
    "gm_mask     = nil.image.math_img('(in_mask & has_data)', in_mask=gm_mask, has_data=has_data); # restrict mask to voxels that have data\n",
    "\n",
    "display = nil.plotting.plot_roi(nil.image.math_img('a+2*b', a=gm_mask, b=brain_mask),alpha=0.6,cmap='summer');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the `searchlight` object. We will also specify some other optional inputs, explained in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = skl.pipeline.make_pipeline(LDA) # we won't do PCA, because this would fail for some searchlights (think about why)\n",
    "\n",
    "sl = nil.decoding.SearchLight(\n",
    "    mask_img = brain_mask,       # only include these voxels within searchlights\n",
    "    process_mask_img = gm_mask,  # only centre searchlights on these voxels \n",
    "    radius = 5,                  # in mm\n",
    "    estimator = pipe,            # a classifier or pipeline object\n",
    "    n_jobs = -1,                 # how many CPUs to use (-1 means the maximum available)\n",
    "    scoring = 'roc_auc',         # choice of scoring metric\n",
    "    cv = logo,                   # cross-validation object\n",
    "    verbose = False              # this is slow, so if running locally (1 job) set it to true to know how far it's got; when running in parallel it produces too much output, so set to false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "To launch the searchlight analysis, call the `searchlight` object's `.fit` method, passing it the patterns, the labels, and optionally the groups for cross-validation.\n",
    "This will take a while (probably about 5 minutes on the virtual machine, with 4 cores)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = nil.image.concat_imgs(brain_data_per_run) # concatenate 4D nifti files from each run (along the 4th dimension)\n",
    "\n",
    "print('Shape of 4D nifti data:', all_data.shape)\n",
    "print('Shape of mask:', sl.mask_img.shape)\n",
    "print('Shape of process_mask:', sl.process_mask_img.shape)\n",
    "\n",
    "print('Running searchlight...')\n",
    "tic = time.time()\n",
    "sl.fit(all_data,all_labels, groups=run_indices)\n",
    "print('Took ', time.time()-tic,' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "The output (here, mean AUROC) per voxel is stored in the  `scores_` property of the searchlight object (as a 3D numpy array). If we convert this to a nifti volume, we can plot it using nilearn. We'll also subtract chance (here, 0.5), so that positive and negative values will indicate above- and below-chance decoding performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of output numpy array: ', sl.scores_.shape)\n",
    "scores = sl.scores_ - 0.5 # subtract chance\n",
    "score_img = nil.image.new_img_like(brain_mask, sl.scores_ - 0.5); # save in samme format as the brain_mask\n",
    "score_img = nil.image.math_img('score_img * processing_mask', score_img=score_img,processing_mask=gm_mask); # set voxels outside the processing mask to zero\n",
    "nil.plotting.plot_stat_map(score_img, threshold=0, cmap='bwr', cut_coords=(-18, -31, -16));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nil.plotting.view_img_on_surf(score_img, threshold=0,cmap='bwr',symmetric_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
