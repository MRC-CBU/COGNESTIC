{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Created: [Danny Mitchell](https://www.mrc-cbu.cam.ac.uk/people/Daniel.Mitchell) with thanks to [Rik Henson](https://www.mrc-cbu.cam.ac.uk/people/rik.henson/)\n",
    "- Last updated: July 2025\n",
    "- Conda environment: This uses the `mri` environment\n",
    "\n",
    "# Classification in python using scikit-learn\n",
    "\n",
    "Much of this tutorial is based on, or inspired by, the longer fMRI-pattern-analysis course developed by Lukas Snoek at the University of Amsterdam, which you can find here: https://lukas-snoek.com/NI-edu/index.html \n",
    "\n",
    "In these demos, we will start with simulated data; then we will move on to real fMRI data. However, most of the principles apply to any source of data patterns. See Mate's sessions tomorrow for examples applied to EEG/MEG.\n",
    "\n",
    "# Part I: Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Getting Ready\n",
    "\n",
    "To give python the functionality we need, we have to **import** a bunch of packages (See Kshipra's session).\n",
    "Many of these you will have  seen in previous sessions. The important package for today is Scikit-learn: this is a popular and powerful library for machine learning in Python. It has a very useful website: https://scikit-learn.org/stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np               # Lets python process matrices, like Matlab\n",
    "np.set_printoptions(precision=3) # Tells numpy to display 3 decimal places\n",
    "\n",
    "import warnings                 # Allows harmless warnings to be hidden \n",
    "\n",
    "import matplotlib.pyplot as plt # Lets python plot graphs like Matlab\n",
    "import seaborn as sns           # Another popular set of plotting functions\n",
    "import pandas as pd             # To use \"dataframes\" (like tables in Matlab)\n",
    "import os                       # To interact with the operating system, including files and paths (e.g. path.join)\n",
    "import time                     # To use \"time\" (like \"tic\" in Matlab)\n",
    "\n",
    "import nibabel as nib           # Basic nifti image utilities\n",
    "\n",
    "import nilearn as nil           # Many useful functions for MRI, including...\n",
    "from nilearn import image       # to load (load_img), resample (resample_to_img), manipulate (math_img) fMRI data, etc.\n",
    "from nilearn import datasets    # includes e.g. fetch_atlas_harvard_oxford\n",
    "from nilearn import masking\n",
    "from nilearn import maskers     # includes NiftiMasker, NiftiLabelsMasker\n",
    "from nilearn import plotting    # includes plot_roi, plot_stat_map, view_img_on_surf, etc.\n",
    "from nilearn import decoding    # includes Searchlight\n",
    "\n",
    "# scikit-learn is the major library for machine learning in Python:\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing         # includes LabelEncoder, OneHotEncoder, StandardScaler...\n",
    "from sklearn import decomposition         # includes PCA\n",
    "from sklearn import model_selection       # includes StratifiedKFold, LeaveOneGroupOut, etc....\n",
    "from sklearn import linear_model          # includes LogisticRegression, RidgeClassifier...\n",
    "from sklearn import svm                   # includes SVC, NuSVC & LinearSCV...\n",
    "from sklearn import discriminant_analysis # includes LinearDiscriminantAnalysis\n",
    "from sklearn import metrics               # includes accuray, balanced accuracy, roc_auc_score, etc....\n",
    "from sklearn import pipeline              # includes make_pipeline\n",
    "from sklearn import inspection            # includes DecisionBoundaryDisplay\n",
    "\n",
    "# scipy? # provides basic statistic functions, e.g. t-tests, and allows import of Matlab files\n",
    "# glob?  # to search filenames with wildcards\n",
    "# copy?  # contains deepcopy (to ACTUALLY copy, rather than making a new pointer to same object)\n",
    "\n",
    "%matplotlib inline \n",
    "# (to show plots in cell)\n",
    "\n",
    "import bids.layout  # To fetch data from BIDS-compliant datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "First, we'll **set the seed to numpy's random number generator**, to get the same sequence of random values every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3) # (if it's not desireable to set a global seed, a seeded RandomState can be passed to the relevant functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Examples using simulated data, with a true effect\n",
    "\n",
    "### Define data matrix and labels:\n",
    "\n",
    "Now, we'll **simulate some data from a single participant**.\n",
    "Let's define **two conditions**, for eample button presses using two different fingers.\n",
    "We'll simulate **four runs**, with **three repeats** of each condition per run. We'll assume that each repeat has been separately estimated from a GLM (see Rik's & Dace's sessions), so we would have 4 x 3 x 2 = 24 activation patterns. In an fMRI experiment, these might be whole-brain volumes; in E/MEG these might be activations across electrodes. The values in each pattern could be beta coefficients, contrasts, or t statistics.\n",
    "For now, **we'll just consider two voxels** from each activation map, to make things easier to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nruns = 4     # number of runs \n",
    "n_per_run = 3 # number of repetitions per run\n",
    "mu1 = np.array([ 2.0, -0.5]) # mean activation for condition 1 (1st voxel activated; 2nd voxel deactivated)\n",
    "mu2 = np.array([-0.5,  2.0]) # mean activation for condition 2 (opposite pattern)\n",
    "voxel_covariance = np.diag([0.0, 3.0])+1 # independent noise per voxel; 2nd voxel is noisier; shared noise is positively correlated\n",
    "# print(\"Noise covariance matrix: \\n\", voxel_covariance )\n",
    "\n",
    "data_per_run = []   # list of pattern matrices (one per run)\n",
    "labels_per_run = [] # list of label vectors (one per run)\n",
    "for r in np.arange(nruns):  # loop over runs\n",
    "    a1 = np.random.multivariate_normal(mu1, voxel_covariance, size = n_per_run) # activations for condition 1\n",
    "    a2 = np.random.multivariate_normal(mu2, voxel_covariance, size = n_per_run) # activations for condition 2 (different mean, same noise covarience)\n",
    "    data_matrix = np.concatenate((a1, a2), axis = 0) # stack condition 1 (n rows of observations) then condition 2 (n more observations)\n",
    "    # add some extra noise that is similar across conditions and voxels, but whose mean and variance increases across runs:\n",
    "    data_matrix = data_matrix + np.random.normal( r*10, r, size = (2*n_per_run, 2))\n",
    "    \n",
    "    label_vector = n_per_run*[\"Thumb\"] + n_per_run*[\"Index\"] # give each pattern a label that indicates from which condition it was drawn\n",
    "    \n",
    "    data_per_run.append(   data_matrix  )  # append this run's data to list\n",
    "    labels_per_run.append( label_vector ) \n",
    "    \n",
    "print(\"\\nData matrix for first run:\\n\", data_per_run[0])\n",
    "print(\"\\nLabel vector for first run:\\n\", labels_per_run[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We now have everything we need for a decoding analysis. However, functions often require data to be represented numerically, so we will also **convert the labels to integers**. This can be done in various ways, but here we'll use the the `LabelEncoder` class from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_enc = skl.preprocessing.LabelEncoder() # this initialises the LabelEncoder object\n",
    "labels_per_run_int = []\n",
    "for r in np.arange(nruns):\n",
    "    labels_per_run_int.append( lab_enc.fit_transform(labels_per_run[r]) )\n",
    "    \n",
    "print(\"Integer labels for all runs:\\n\", labels_per_run_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Notice that we first initialised a `LableEncoder` object, and then called its `.fit_transform` method. Many objects also have `.fit` and `.transform` methods that can be called separately. This procedure is similar for most objects in scikit-learn.\n",
    "\n",
    "Before doing anything else, let's **plot the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_runs  = np.concatenate(data_per_run,axis=0)       # concatenate data across runs\n",
    "concatenated_labels= np.concatenate(labels_per_run_int,axis=0) # concatenate labels across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(concatenated_runs, yticklabels = concatenated_labels);\n",
    "ax.set_xlabel( \"voxels\", fontsize = 14)\n",
    "ax.set_ylabel(\"labelled patterns\", fontsize = 14)\n",
    "ax.tick_params(axis='x', labelsize = 12)\n",
    "ax.tick_params(axis='y', labelsize = 12)\n",
    "ax.collections[0].colorbar.set_label(\"activation\", fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8));  # create a matplotlib figure\n",
    "plt.rcParams.update({'font.size': 15});\n",
    "plt.title('Activation patterns as points in n-dimensional space:');\n",
    "scatter = plt.scatter(concatenated_runs[:,[0]], concatenated_runs[:,[1]], \n",
    "                      s = 90, alpha = 0.7, c = concatenated_labels, cmap = 'bwr');\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(labels_per_run[0])); # \"set\" returns unique values\n",
    "plt.xlabel('Voxel 1 activity');\n",
    "plt.ylabel('Voxel 2 activity');\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that, by drawing a diagonal line y=x, one could separate most of the red dots from most of the blue dots. This would be a \"decision boundary\" that a classifier needs to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Initial classification, without cross-validation:\n",
    "\n",
    "At this stage, it is common to **standardise each feature**, e.g. by rescaling into the range [0 1], or by z-scoring (subtracting the mean and dividing by the standard deviation across samples). For many classifiers, standardizing each feature across all samples should not change the classification accuracy. (Imagine how these transformations change the distribution of the data in the figure above.) However, putting each feature on the scame scale can help parameters to be estimated efficiently, and may be necessary for some models to converge. \n",
    "\n",
    "Consider whether it would it be a good or bad idea to rescale features for each class separately? \n",
    "\n",
    "Would it be a good or bad idea to rescale features for each run separately?\n",
    "\n",
    "For now, let's z-score each feature across all samples of all runs. Again, we can do this by initialising a scikit-learn object and calling its `fit_transform` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler            = skl.preprocessing.StandardScaler()\n",
    "concatenated_runs = scaler.fit_transform(concatenated_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data again to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize = (8,8));  # create a matplotlib figure\n",
    "plt.rcParams.update({'font.size': 15});\n",
    "plt.title('Activation patterns as points in n-dimensional space (after scaling):');\n",
    "scatter = plt.scatter(concatenated_runs[:,[0]], concatenated_runs[:,[1]], \n",
    "                      s = 90, alpha = 0.7, c = concatenated_labels, cmap = 'bwr');\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels = set(labels_per_run[0])); # \"set\" returns unique values\n",
    "plt.xlabel('Voxel 1 activity');\n",
    "plt.ylabel('Voxel 2 activity');\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now we can proceed to classification. We'll start with a logistic regression classifier, which we'll initialise to a variable called `LR`. (There are lots of different types of classifier, and we'll look at some others later. For some discussion of their pro sand cons, see [this blog](https://medium.com/analytics-vidhya/pros-and-cons-of-popular-supervised-learning-algorithms-d5b3b75d9218).)\n",
    "\n",
    "Each **classifier needs to be \"trained\" or \"fit\" using \"training data\" and corresponding labels.** This uses the training data to optimise the parameters of the model, to best match the labels to some function (often a linear combination) of the features.\n",
    "\n",
    "In scikit-learn, this uses the `.fit` method of the classifier, which needs to be given the patterns matrix and labels as inputs. To start with, we'll train the classfier using all the data, concatenated across runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = skl.linear_model.LogisticRegression(solver = 'lbfgs') # there are more options that could be set here; we won't worry about them\n",
    "LR.fit(concatenated_runs, concatenated_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **estimated (fitted) coefficients are now stored in the classifier object**. Notice that for this linear classifier we have one coefficient per feature (voxel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients:\\n\", LR.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "These coefficients define the linear decision boundary, so we can now plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl.inspection.DecisionBoundaryDisplay.from_estimator(LR, concatenated_runs, \n",
    "                                                      alpha = 0.5, ax = fig.axes[0], cmap = 'bwr', response_method = 'predict');\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "The next step is to **\"test\" the model, by giving it some patterns and \"predicting\" their labels**. For now, we'll give it the same data we used to train the model. (Some of you may have a question at this point - if so, hold that thought...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = LR.predict(concatenated_runs)\n",
    "print(\"Predictions for all samples:\\n\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "How accurate are these predictions? We can **\"evaluate\" the predictions by comparing them with the true values. The quality of the predictions is summarised as a \"performance\", \"score\" or \"metric\"**. If we had continuous predictions (a regression model), we might correlate them with the true values. Here we have categorical predictions (a classification model), which we can compare with the true values in various ways...\n",
    "\n",
    "\n",
    "### Measures of accuracy\n",
    "\n",
    "The **simplest score, called \"accuracy,\" is just the proportion of all predictions that are correct**. What is the best possible accuracy? What is the worst possible accuracy? What accuracy would we expect by chance?\n",
    "\n",
    "Calculating accuracy is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = \", np.mean(concatenated_labels == predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Scikit-learn also has a function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = skl.metrics.accuracy_score(concatenated_labels, predicted_labels)\n",
    "print(\"Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Actually scikit-learn has **many metrics to choose from.** The only thing to recommend accuracy as a metric is its simplicity. Otherwise, it is relatively insensitive (because binary guesses discard information about the confidence of a particular prediction) and can be misleading if classes are unbalanced (more about that later).\n",
    "\n",
    "A metric that incorporates the confidence of each prediction, and can handle unbalanced classes, but is rather complicated, is the \"**Area Under the Receiver Operating Characteristic curve**\" (AUROC, or just AUC). We can use a function from scikit-learn to calculate this, but instead of vectors of true and predicted labels, it generally needs matrices of true and predicted probabilities per class. (In this binary case, we could give it the vector labels and the probabilities for the class with the largest label, but this syntax doesn't generalise to more than two classes).\n",
    "\n",
    "Matrices of *true* class probabilities can be constructed from the true labels using scikit-learn's `one-hot encoder` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = skl.preprocessing.OneHotEncoder(sparse_output = False)              # we don't want a \"sparse\" output\n",
    "true_probabilities = ohe.fit_transform(concatenated_labels.reshape(-1,1)) # the reshape is needed because the function expects a 2D input\n",
    "print(\"True probabilities per class, for first 5 patterns:\\n\", true_probabilities[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Matrices of predicted class probabilities can be returned by many classifier objects, including logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = LR.predict_proba(concatenated_runs) # notice we have just swapped the \"predict\" method for \"predict_proba\"\n",
    "print(\"Predicted probabilities per class, for first 5 patterns:\\n\", predicted_probabilities[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "With these, we can now calculate the AUROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROC = skl.metrics.roc_auc_score(true_probabilities, predicted_probabilities)\n",
    "print(\"AUROC = \", AUROC)\n",
    "AUROC = skl.metrics.roc_auc_score(concatenated_labels, predicted_probabilities[:,1]) # alternative syntax for binary classifcation\n",
    "print(\"AUROC = \", AUROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Notice that the performance is a bit better than simple accuracy. (Both metrics are bounded between 0 and 1. For AUROC, chance is 0.5. For balanced binary classification, as here, chance accuracy is also 0.5.)\n",
    "\n",
    "So far, so good. BUT... **these results are biased and cannot be trusted, because we tested the model on the same data that we used for training!** This is like taking an exam after having already seen the correct answers. To emphasise the problem, let's create a **new simulated dataset**, with the same trial structure, but a more realistic number of features/voxels (e.g. 30), and **with no true difference between the conditions** (both are drawn at random fom the *same* distribution):\n",
    "\n",
    "\n",
    "## Examples using simulated data with NO true effect - the importance of cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox = 30                          # number of voxels\n",
    "mu = np.arange(nvox)               # mean activation per voxel (voxels all have different activation strength), which will be the SAME for both conditions 1 & 2\n",
    "voxel_covariance = np.diag(mu) + 1 # independent noise per voxel is proportional to mean, plus some covariance\n",
    "\n",
    "null_data_per_run = []             # list of pattern matrices (one for each run)\n",
    "for r in np.arange(nruns):\n",
    "    data_matrix =  np.random.multivariate_normal(mu, voxel_covariance, size= 2 * n_per_run) # activations are drawn from THE SAME DISTRIBUTION FOR BOTH CONDITIONS!\n",
    "    null_data_per_run.append( data_matrix ) \n",
    "\n",
    "null_concatenated_runs = np.concatenate(null_data_per_run,axis = 0)               # concatenate runs\n",
    "null_concatenated_runs = scaler.fit_transform(null_concatenated_runs)             # scale data per voxel\n",
    "LR.fit(null_concatenated_runs, concatenated_labels)                               # fit the classifier       \n",
    "null_predicted_labels = LR.predict(null_concatenated_runs)                        # predict the labels\n",
    "accuracy = skl.metrics.accuracy_score(concatenated_labels, null_predicted_labels) # measure accuracy\n",
    "\n",
    "print('True labels:\\n', concatenated_labels)\n",
    "print('Predicted labels from null data:\\n', null_predicted_labels)\n",
    "print('Accuracy from null data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Here we have got perfect classification, despite both conditions being random samples from the same distribution! \n",
    "\n",
    "Let's just **try using a different classifier**, to show that this is a general problem. Another popular classifier is a **Support Vector Machine**. The procedure is essentially the same:\n",
    " - create the classifier object\n",
    " - train (fit) the classifier (using (typically scaled) patterns and their labels)\n",
    " - test the classifier (predict labels from patterns)\n",
    " - calculate a performance measure (compare predicted labels with their true values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = skl.svm.LinearSVC(dual = True) # (dual specifies the algorithm, which should affect convergence but not output; duel is usually fastest)\n",
    "SVM.fit(null_concatenated_runs, concatenated_labels)\n",
    "null_predicted_labels = SVM.predict(null_concatenated_runs)\n",
    "accuracy = skl.metrics.accuracy_score(concatenated_labels, null_predicted_labels)\n",
    "print('Accuracy from null data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "### Cross-validation\n",
    "With more voxels than conditions (which is common in neuroimaging), the classifier can easily separate conditions based on noise (chance differences). This is called **overfitting**. **When testing on the same data that was used for training**, these chance differences are the same, so the **classification performance is positively biased**. \n",
    "\n",
    "We solve this using **cross-validation**.\n",
    "\n",
    "Cross-validation means that the classifier is **tested on separate data**, that was not used for training. Independent data will have different noise. Therefore, overfitting based on noise in the training samples will still occur, but will not generalise to the testing samples, and classifier performance will be **unbiased**.\n",
    "\n",
    "There are different ways to split data into test and train sets. The simplest case is a **single partition, called a \"hold-out\"** scheme. Here we'll split the data into odd and even runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_concatenated_runs_train_odd = np.concatenate(null_data_per_run[0::2], axis = 0) # concatenate training data across odd runs\n",
    "null_concatenated_runs_test_even = np.concatenate(null_data_per_run[1::2], axis = 0) # concatenate testing data across even runs\n",
    "\n",
    "concatenated_labels_train_odd = np.concatenate(labels_per_run_int[0::2], axis = 0) # concatenate training labels across odd runs\n",
    "concatenated_labels_test_even = np.concatenate(labels_per_run_int[1::2], axis = 0) # concatenate testing labels across even runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now we can standardise the training data and use these to train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.fit(scaler.fit_transform(null_concatenated_runs_train_odd), concatenated_labels_train_odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we test the classifier on the independent data (held-out from training), the accuracy should close be to chance (with some random error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_predicted_labels_test_even = LR.predict(scaler.transform(null_concatenated_runs_test_even))\n",
    "accuracy = skl.metrics.accuracy_score(concatenated_labels_test_even, null_predicted_labels_test_even)\n",
    "print('True labels (from even runs):\\n', concatenated_labels_test_even)\n",
    "print('Predicted labels from null data (even runs):\\n', null_predicted_labels_test_even)\n",
    "print('Accuracy from null data (even runs):', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "(Notice that here we used  `scaler.transform`  rather than  `scaler.fit_transform`. This means that the standardisation parameters fit using the training data are also used for standardising the test data. One could instead independently standardise the test set, or standardise the whole dataset before partitioning. Opinions differ on whether it matters, and which, if any, is best.)\n",
    "\n",
    "Okay, accuracy is now close to chance, as it should be. But notice that the classifier is now:\n",
    "- trained on only some (here, half) of the data, which makes it less sensitive\n",
    "- tested on only some (here, half) of the data, which makes its performance more variable (less reliable)\n",
    "\n",
    "For more efficient use of the data, we can use **K-fold cross-validation**. This means that the data are split into K parts, and in each of K folds/splits one part is used for testing and the rest are used for training. This way, each fold uses more training data (so more sensitive), and after combining predictions across folds there are more predictions (so a more reliable accuracy estimate).\n",
    "\n",
    "Generally, we want the proportion of classes to be as balanced as possible in each fold (and therefore in the training set). To do this we could use scikit-learn's `StratifiedKFold` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "skf = skl.model_selection.StratifiedKFold(n_splits = 3) # create the object\n",
    "folds = skf.split(null_concatenated_runs, concatenated_labels) # create the folds\n",
    "# note: the folds are returned as a \"generator\", which means that each element can only be accessed once and then disappears! We can make them persist by converting then to a tuple:\n",
    "folds = tuple(folds)\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "\n",
    "    # fold  is a tuple, which contains indices to train and test samples\n",
    "    train_idx, test_idx = fold # unpack the tuple\n",
    "    \n",
    "    print(\"Fold %i\" % (i + 1))\n",
    "    print(\"Train samples:\", train_idx)\n",
    "    print(\"Test samples:\",  test_idx, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Note two things here. \n",
    "\n",
    "Firstly, this is only one of many ways to split our 24 samples into three parts. We could repeat the partitioning with different random splits, using `RepeatedStratifiedKFold`. This would further increase the total number of predictions, and therefore the precision of the estimated accuracy (at the expense of taking longer). Also, note that the total number of unique ways to split into K parts decreases as K increases. Therefore, the **choice of K involves a trade-off between senstivity and precision**. In the extreme case of K=N, where N is the number of samples per class, this becomes \"Leave-One-Out Cross-Validation\" (LOOCV). This produces the most accurate performance because it uses the maximum possible amount of data for training (without bias). However, it can be less precise, because it can only average over N folds; when K<N, one can repeat the random partitioning many times, giving more test performances that can be averaged to improve the precision of the estimate of overall performance.\n",
    "\n",
    "Secondly, here samples from more than one run are spread across folds. For fMRI, this is NOT what we want, because **fMRI samples within a run are likely not independent**, and so the train/test splits will also not be independent. Instead, the best and typical approach is to cross-validate across runs. To do this, we can swap the `StratifiedKFold` object for a **`LeaveOneGroupOut`** object (see also `GroupKFold`, and `LeavePGroupsOut`), and tell it that we want to treat each fMRI run as a group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_indices =  np.concatenate([[i] * 2 * n_per_run for i in range(nruns)]) # get the indices of each run\n",
    "print(\"Run indices:\",run_indices,\"\\n\")\n",
    "\n",
    "logo = skl.model_selection.LeaveOneGroupOut()\n",
    "folds = tuple(logo.split(null_concatenated_runs, concatenated_labels, run_indices))\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    train_idx, test_idx = fold\n",
    "    print(\"Fold \", i, \" test samples:\", test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "### Pipelines\n",
    "\n",
    "It can be cumbersome to specify every step within a loop across the folds (e.g. scaling the data, training the classifier, testing the classifier, plus any extra steps). This can increase the risk of accidentally using test data when pre-processing or training the classifier, or accidentally applying different pre-processing to train and test sets.\n",
    "\n",
    "Scikit-learn has a mechanism called **\"pipelines\"** to make this more efficient, and reduce the risk of such errors. To make a pipeline, we call the `make_pipeline` function, and give it the objects for each step of the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = skl.pipeline.make_pipeline(scaler, SVM)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Then, in the cross-validation loop, we can just call the pipeline's `.fit` method on the training data and its `.predict` method on the test data, to automatically apply all these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = np.full(nruns,np.nan) # initialise a vector to store the test accuracy of each fold\n",
    "for i, fold in enumerate(folds):\n",
    "    train_idx, test_idx = fold # get the train and test indices\n",
    "    pipe.fit(null_concatenated_runs[train_idx,:], concatenated_labels[train_idx]) # do all preprocessing and model fitting on TRAINING data\n",
    "    null_predicted_labels = pipe.predict(null_concatenated_runs[test_idx,:])      # do prediction on TEST data\n",
    "    accuracy[i] = skl.metrics.accuracy_score(concatenated_labels[test_idx], null_predicted_labels) # score the predictions\n",
    "print(\"Accuracy per fold:\", accuracy)\n",
    "print(\"Mean accuracy for random data:\", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Again, this is not consistently above chance, as expected for random data.\n",
    "\n",
    "To make things *even easier*, we can pass the pipeline, cross-validation scheme, and performance metric to a single function, **`cross_val_score`**, to do everything in one go: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = skl.model_selection.cross_val_score(pipe, null_concatenated_runs, concatenated_labels,\n",
    "                                    groups  = run_indices,\n",
    "                                    scoring = 'accuracy',\n",
    "                                    cv      = logo)\n",
    "print(\"Crossvalidated accuracy per fold for random data:\", accuracy)                   \n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned accuraccies per fold should be the same as above.\n",
    "\n",
    ".\n",
    "\n",
    "### Final example using simulated data:  true effect with cross-validation:\n",
    "Now that we have a nice cross-validated pipeline, let's return to the original simulated data that *does* have a difference between conditions, and check that the decoding accuracy is well above chance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = skl.model_selection.cross_val_score(pipe, concatenated_runs, concatenated_labels,\n",
    "                                    groups  = run_indices,\n",
    "                                    scoring = 'accuracy',\n",
    "                                    cv      = logo)\n",
    "print(\"Crossvalidated accuracy per fold for original data:\", accuracy)  \n",
    "print(\"Mean crossvalidated accuracy for original data:    \", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    ".\n",
    "# Classification in python using scikit-learn - Part II: Brain data\n",
    "\n",
    "Okay, time to try decoding using real fMRI data! (Thanks to [Rik Henson](https://www.mrc-cbu.cam.ac.uk/people/rik.henson/) for sharing some of this code.)\n",
    "\n",
    "We will use one participant (subject 15) from the [Face Recognition dataset](https://www.nature.com/articles/sdata20151) that you have been working with already.\n",
    "\n",
    "In this experiment, there were **nine runs**. In each run, participants saw Famous faces (FF), Unfamilar faces (UF), and Scrambled faces (SF). Here we will try to **decode Famous from Scrambled faces**. Each face had an Initial presentation (Ini), an Immediate repetition (Imm) and a Delayed repetition (Del).\n",
    "\n",
    "In Dace's fMRI sessions, you should have already:\n",
    "- run `/home/cognestic/COGNESTIC/06_fMRI/code-examples/step02_dicom_to_bids.sh` (in the 'mri' environment) to convert the raw DICOM data to .nii files in a BIDS-compatible folder structure (takes about two minutes)\n",
    "- run `python /home/cognestic/COGNESTIC/06_fMRI/code-examples/step03_get_events.py` to download the event files for this subject (from OpenNeuro dataset ds000117; takes just a few seconds)\n",
    "- preprocessed the data, with the output in `/home/cognestic/COGNESTIC/06_fMRI/FaceRecognition/data`\n",
    "\n",
    "In Rik's fMRI Connectivity session, you should have already run:\n",
    "- `/home/cognestic/COGNESTIC/07_fMRI_Connectivity/task_based_fcon.ipynb`, with an example of how to estimate per-trial activations (betas)\n",
    "\n",
    "The examples below require the first two of these steps.\n",
    "\n",
    "## Estimate the patterns to classify\n",
    "\n",
    "First, let's set up the input and output directories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input folder and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_data_dir = '/home/cognestic/COGNESTIC/06_fMRI/FaceRecognition/data' # path to raw and preprocessed BIDS-formatted data\n",
    "sID = '15' # the same subject you used before\n",
    "print(\"Using data from subject\", sID, \", from\" ,fmri_data_dir)\n",
    "\n",
    "layout = bids.layout.BIDSLayout(fmri_data_dir, derivatives = True) # use PyBIDS to create a BIDSLayout object and automatically look for a derivatives/ folder\n",
    "bold = layout.get(subject = sID, datatype = 'func', desc = 'preproc', extension = '.nii.gz', return_type = 'filename') # find the preprocessed BOLD data\n",
    "nruns=len(bold)\n",
    "print(\"Found \", nruns, \" preprocessed functional files:\")\n",
    "print(*bold, sep='\\n')\n",
    "\n",
    "events_files = layout.get(subject = sID, datatype = 'func', suffix = 'events', extension = \".tsv\", return_type = 'filename') # find the events files\n",
    "print(\"Found \", len(events_files), \" event files:\")\n",
    "print(*events_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "For each of the nine runs, there is a .tsv file that contains the definition of each trial (\"event\") in the run, plus a 4D nifti file containing the preprocessed fMRI time series from Dace's sessions. \n",
    "\n",
    "We want to classify famous faces versus scrambled faces, with cross-validation across runs, so we need to estimate their activation patterns per run. We could estimate a separate pattern for every individual trial (see the \"LSA\" model used for beta series regression in Rik's functional connectivity lecture); however, here we will estimate one pattern per condition (the average over trials) per run. One advantage of using run-wise patterns is that we can assume runs are independent, which means that labels are exchangeable under the null hypothesis and a permutation test can be used. Trials within a run are not necessarily independent (due to temporal autocorrelation), so trial-wise classification would typically need either a between-participant significance test, or careful permutation that preserves the autocorrelation structure. (If the trial sequence is fully random, then label randomization can still be valid, but only on average across randomized designs.)  \n",
    "\n",
    "The procedure to define and estimate the GLM should be familiar from both Dace's and Rik's sessions.\n",
    "\n",
    "### Define and create an output folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_dir = '/home/cognestic/COGNESTIC/09_MVPA_MRI/Decoding/' \n",
    "beta_dir = os.path.join(demo_dir, 'FaceRecognitionData') # define an output subfolder\n",
    "if not os.path.exists(beta_dir):                         # if it doesn't exist...\n",
    "    os.makedirs(beta_dir)                                # ...make it\n",
    "os.chdir(beta_dir)                                       # change to that folder\n",
    "print(f\"Output directory currently: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the onsets, durations, and condition names of the events of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "for run, events_file in enumerate(events_files):\n",
    "    events_df = pd.read_table(events_file)\n",
    "    conditions_to_model = list(set(events_df['trial_type'])) # get a list of unique trial types\n",
    "    conditions_to_model.sort(reverse=True)                   # conditions_to_model = ['IniUF', 'IniSF', 'IniFF', 'ImmUF', 'ImmSF', 'ImmFF', 'DelUF', 'DelSF', 'DelFF']\n",
    "    \n",
    "    events_df['trial_type'] = events_df['trial_type'] + '_run' + str(run) # add a suffix to their names to identify the run\n",
    "    events_df = events_df.drop(columns = ['button_pushed', 'stim_file', 'trigger', 'circle_duration', 'response_time']) #  remove the columns we don't need\n",
    "    events.append( events_df )\n",
    "\n",
    "print('\\nEvents for first run:\\n', events[0])\n",
    "print('\\nEvents for final run:\\n', events[8])\n",
    "\n",
    "print('\\nConditions to model:\\n', conditions_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Note that we have added a suffix to each condition name to identify which run it is in. This will let us separate the beta patterns by run later on.\n",
    "\n",
    "### Estimate activation patterns per run and per condition\n",
    "\n",
    "First, we'll plot the design matrix for one example run, to check it looks correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference slice\n",
    "TR = layout.get_tr()\n",
    "slice_timing = layout.get_metadata(bold[0])\n",
    "if slice_timing['SliceTimingCorrected']:\n",
    "  slice_time_ref = slice_timing['StartTime'] / TR\n",
    "  #print('Slice timing reference:', slice_time_ref)\n",
    "\n",
    "for run in [1]:  # range(nruns)\n",
    "    nvols = nib.load(bold[run]).shape[-1]\n",
    "    frame_times = np.linspace(0, (nvols - 1) * TR, nvols) + slice_time_ref # adjust for slice-timing\n",
    "\n",
    "    design_matrix = nil.glm.first_level.make_first_level_design_matrix(frame_times, \n",
    "                                                                       events = events[run], \n",
    "                                                                       hrf_model = 'spm', \n",
    "                                                                       drift_model = 'cosine', \n",
    "                                                                       high_pass = 0, \n",
    "                                                                       drift_order = None)\n",
    "    plt.rcParams.update({'font.size': 12}); \n",
    "    nil.plotting.plot_design_matrix(design_matrix, output_file=None)\n",
    "    fig = plt.gcf(); plt.title('Design matrix for run ' + str(run)); fig.set_size_inches(8,4); plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now we'll load the movement parameters as covariates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confound_files = layout.get(subject = sID, datatype = 'func', desc = 'confounds', extension = \".tsv\", return_type = 'filename')\n",
    "relevant_confounds = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "confounds_for_glm = []\n",
    "for conf_file in confound_files:\n",
    "    this_conf = pd.read_table(conf_file)\n",
    "    conf_subset = this_conf[relevant_confounds].fillna(0) # replace NaN with 0\n",
    "    confounds_for_glm.append( conf_subset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "...load an analysis mask (the whole brain)...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_mask_file = layout.get(return_type = 'file', datatype = 'func', suffix = 'mask', desc = 'brain', space = 'MNI152NLin6Asym', extension = 'nii.gz')[0] \n",
    "# this should be the same space as the preprocessed data\n",
    "brain_mask = nib.load(brain_mask_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "...and define the other settings for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = nil.glm.first_level.FirstLevelModel(t_r = TR,\n",
    "                                               slice_time_ref = slice_time_ref,\n",
    "                                               hrf_model = 'spm',\n",
    "                                               drift_model = 'cosine',\n",
    "                                               drift_order = 1,\n",
    "                                               high_pass = 0.01,\n",
    "                                               smoothing_fwhm = None,\n",
    "                                               signal_scaling = (0, 1), # grand mean scaling only\n",
    "                                               noise_model = 'ols', # no need for 'ar1' if only care about Betas, should speed up\n",
    "                                               mask_img = brain_mask,\n",
    "                                               memory = 'scratch', memory_level=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Note that:\n",
    "- we are not adding spatial smoothing, because we do not necessarily want to lose fine-scale information.\n",
    "- we can use the quicker \"ols\" rather than \"ar1\" model of the error autocorrelation, because we do not need statistical tests, only the beta estimates.\n",
    "\n",
    "Now we can estimate the model for all runs. This could take around a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\") # nilearn would warn about us specifying a mask\n",
    "    fmri_glm.fit(bold, events, confounds_for_glm); # this takes about 16-73 s ? \n",
    "print('Took ', time.time()-tic,' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Next, we define which of the estimated beta patterns we want to classify, and to which classes they belong. \n",
    "Recall that we'll classify famous faces (FF) from scrambed faces (SF), while ignoring unfamiliar faces (UF). \n",
    "For each of these, we have three beta estimates: one for the initial presentation of each stimulus (Ini), one for immediate repeats (Imm) and one for delayed repeats (Del).\n",
    "Thus we are interested in six of nine conditions, each of which belong to one of two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions_to_decode = ['DelFF', 'DelSF', 'DelUF', 'ImmFF', 'ImmSF', 'ImmUF', 'IniFF', 'IniSF', 'IniUF']\n",
    "conditions_to_decode   = ['DelFF', 'DelSF',          'ImmFF', 'ImmSF',          'IniFF', 'IniSF'         ]  \n",
    "print('\\nConditions to decode:\\n', conditions_to_decode)\n",
    "\n",
    "class_labels = [x[3:] for x in conditions_to_decode]\n",
    "print('\\nClass labels:\\n', class_labels)\n",
    "\n",
    "labels_per_run = [ lab_enc.fit_transform(class_labels) for r in range(nruns) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "To extract the beta patterns for each condition of each run, we define \"contrasts\". Here, each contrast will be a vector of zeros for all conditions of all runs, except for a single 1 that targets a single condition of a single run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = nib.load(bold[0]).affine # needed for writing images below\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "betas_per_run_nii = []   # for list of nifti objects\n",
    "\n",
    "for run in range(nruns): # for each run...\n",
    "\n",
    "    Xcols=(fmri_glm.design_matrices_[run].columns) # regressor names for this run\n",
    "       \n",
    "    beta_maps  = [] # to store the betas for each condition of interest in this run\n",
    "    \n",
    "    print(f'Run {run}: Creating {len(conditions_to_decode)} beta volumes and storing them as a 4D array...')\n",
    "\n",
    "    for condition in conditions_to_decode: # for each condition type that we want to classify...\n",
    "\n",
    "        # initialise contrast vectors of zeros for all runs (nilearn would accept NaNs, but then set all outputs to NaN):\n",
    "        contrast_list = [ np.zeros((1, len(fmri_glm.design_matrices_[r].columns))) for r in range(nruns) ]\n",
    "       \n",
    "        # set one-hot vector for this condition of this run:\n",
    "        reg_name = condition + '_run' + str(run) # add the run suffix to get the regressor name\n",
    "        contrast_list[run][0, Xcols.get_loc(reg_name)] = 1\n",
    "   \n",
    "        # compute the betas\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\") # nilearn would warn for every run with an all-zero vector \n",
    "            beta_map = fmri_glm.compute_contrast(contrast_list, output_type = 'effect_size'); # returned as a nifti object\n",
    "        beta_maps.append( beta_map )\n",
    "\n",
    "        # nil.plotting.plot_stat_map(beta_map, threshold=0, cmap='bwr', cut_coords=(-18, -31, -16));\n",
    "\n",
    "    betas_per_run_nii.append( nil.image.concat_imgs(beta_maps) ) # Stack the maps along the 4th dimension, and add to per-run list\n",
    "    \n",
    "    # fname = os.path.join(beta_dir, os.path.basename(bold[run]).split(\"space\")[0] + 'SixConditions_Betas.nii.gz'); # file name if we wanted to save it  \n",
    "    # nib.save(betas_per_run_nii[run], fname);\n",
    "nib.Nifti1Image\n",
    "print('Took ', time.time()-tic,' s')\n",
    "\n",
    "# Remove the cached directory\n",
    "!rm -rf scratch/joblib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Let's just check the shape of the data array from one run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of betas for first run: {betas_per_run_nii[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "The first three dimensions are the x, y, and z coordinates. The 4th dimension is conditions. \n",
    "\n",
    "As intended, we have extracted beta patterns for six conditions for this run.\n",
    "\n",
    "## ROI-based classification\n",
    "\n",
    "We don't expect all of these voxels to contain signal (some are not even in the brain!). So we will **select voxels from a Region Of Interest (ROI)**. This is a type of **feature selection**, which restricts the analysis to brain regions we are interested in. \n",
    "\n",
    "### Define an ROI\n",
    "\n",
    "To avoid biasing the results, any data used for feature selection must be independent of the data being classified. Since these data are in MNI space, we can use any mask independently defined in MNI space. Here, we'll use Nilearn to load anatomical ROIs from the Harvard-Oxford atlas. These are returned as a dictionary that stores the `filename`, `description` and, importantly, `labels` (ROI names) and `maps` (a 3D volume of integers that index the ROI names):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HO_atlas = nil.datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm', demo_dir) # this is a maximum-probability, anatomical atlas, thresholded at 25%, with 2 mm resolution\n",
    "print(\"\\nFile name of atlas:\", HO_atlas['filename'])\n",
    "# print(HO_atlas['description'])\n",
    "print(\"\\nROI names:\\n\", HO_atlas['labels'])\n",
    "# print(\"ROI mask:\\n\", HO_atlas['maps'])\n",
    "nil.plotting.plot_roi(HO_atlas['filename'], cut_coords = (-40, -52, -18), colorbar = False); # we'll put the cross-hairs near the left FFA\n",
    "plt.show();\n",
    "print(\"\\nShape of ROI map:\\n\", HO_atlas['maps'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, although the functional data and the ROI masks are both in MNI space, they have different dimensions. We can use nilearn to **resample** the ROIs to match the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_ROI_map = nil.image.resample_to_img(HO_atlas['maps'], betas_per_run_nii[0],\n",
    "                                              interpolation = 'nearest',\n",
    "                                              force_resample = True,\n",
    "                                              copy_header = True)\n",
    "# note that for continuous-valued data we would want continuous interpolation, but here nearest-neighboour interpolation preserves the integer ROI values \n",
    "print(\"Shape of resampled ROI map:\", resampled_ROI_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now we'll select the bilateral \"Temporal Occipital Fusiform Cortex\" region, which is close to the fusiform face area (FFA) that is expected to have a face-selective response.\n",
    "\n",
    "We'll use nilearn's `math_img` function to turn this region into a binary mask. It's sensible to plot this, to check that it looks correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusi_id = HO_atlas['labels'].index('Temporal Occipital Fusiform Cortex')\n",
    "print(\"\\nIndices of fusiform ROI:\", fusi_id)\n",
    "\n",
    "fusi_ROI = nil.image.math_img(f'(map == {fusi_id})', map = resampled_ROI_map)\n",
    "nil.plotting.plot_roi(fusi_ROI, cut_coords=(-40, -52, -18), colorbar = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from ROI\n",
    "\n",
    "Now we can pass this ROI to a `masker` object, and apply it to the activation maps to extract the patterns we are interested in. \n",
    "\n",
    "We'll start by looking at the first run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You could instead use the ROI defined by the univariate contrast of faces versus scrambled faces, estimated in a previous session: \n",
    "# fusi_ROI = nib.load('/home/cognestic/COGNESTIC/06_fMRI/FaceRecognition/results/FFA_sphere_and_faces-scrambled_fwe.nii.gz')\n",
    "### (This should already be in the same space as the data, so it needn't be resampled.)\n",
    "### However, this would NOT BE INDEPENDENT of the data we will classify, so the results would be biased.\n",
    "\n",
    "fusi_masker = nil.maskers.NiftiMasker(fusi_ROI)\n",
    "\n",
    "beta_patterns = fusi_masker.fit_transform(betas_per_run_nii[0]) # just look at the first run for now\n",
    "# (if we hadn't resampled the ROI, the masker could also do this; we would provide \"target_affine\" as the space of the data)\n",
    "\n",
    "print('Shape of pattern matrix from chosen ROI:', beta_patterns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 4))\n",
    "sns.heatmap(beta_patterns, cmap='coolwarm', center = 0, robust = 1, xticklabels = 50, yticklabels = class_labels)\n",
    "ax.set_title('Data from fusiform ROI, in first run' , fontsize = 14)\n",
    "ax.set_xlabel( \"voxels\" , fontsize = 14)\n",
    "ax.set_ylabel(\"labelled samples / patterns / faces\" , fontsize = 14)\n",
    "ax.tick_params(axis='x', labelsize = 12)\n",
    "ax.tick_params(axis='y', labelsize = 12)\n",
    "ax.collections[0].colorbar.set_label(\"activation\", fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "We can see that some of these voxels are correlated, and some will be noisier than others. Classifiers are often able to handle this, but we may choose to apply **dimension reduction**. This is sometimes called **feature extraction**, and a common method is principle component analysis (**PCA**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_patterns = scaler.fit_transform(beta_patterns)\n",
    "\n",
    "PCA = skl.decomposition.PCA(n_components = 5) # the number of components should be smaller than the smallest dimension of the input\n",
    "PCA.fit(beta_patterns)\n",
    "beta_patterns_PCA5 = PCA.transform(beta_patterns)\n",
    "\n",
    "print(\"\\nShape of pattern matrix after PCA:\", beta_patterns_PCA5.shape)\n",
    "\n",
    "ax = sns.heatmap(beta_patterns_PCA5, cmap = 'coolwarm', center = 0, xticklabels = 1, yticklabels = class_labels)\n",
    "ax.set_title('Data from fusiform ROI, in first run' , fontsize = 14)\n",
    "ax.set_xlabel( \"Principal components\" , fontsize = 14)\n",
    "ax.set_ylabel(\"labelled samples / patterns / faces\" , fontsize = 14)\n",
    "ax.tick_params(axis='x', labelsize = 12)\n",
    "ax.tick_params(axis='y', labelsize = 12)\n",
    "ax.collections[0].colorbar.set_label(\"activation\", fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that features are typically standardized before PCA, otherwise the principal components will be dominated by features with larger scales. However, standardization means that features that originally had low variance will be upweighted, under the assumption that all features are expected to be equally predictive. In fMRI, noisy/non-predictive features may have low variance, so upweighting them may be detrimental to performance.\n",
    "\n",
    "Also, pattern variance might be dominated by aspects of the stimulus or mental state other than the distinction we are tying to classify. In that case, PCA might discard higher-numbered components that could have been informative. Thus, similar to spatial smoothing, PCA can either help or hinder, depending on the relative suppression/enhancement of signal versus noise.\n",
    "\n",
    "### Classify using leave-one-run-out cross-validation\n",
    "\n",
    "Now, let's put everything together: \n",
    "Load ROI data from all runs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (If you wanted to remove the mean across voxels for each condition, you could use the \"demeaner\" transform below.\n",
    "# However, typically the mean signal will carry useful information that helps to distinguish the classes.)\n",
    "# demeaner = skl.preprocessing.StandardScaler(with_mean=True, with_std=False)\n",
    "\n",
    "ROI_patterns_per_run = [] # list of ROI's pattern matrices (one for each run)\n",
    "for run in range(nruns):\n",
    "    print('Extracting ROI pattern from run ',run)\n",
    "    ROI_patterns = fusi_masker.fit_transform(betas_per_run_nii[run])\n",
    "    # ROI_patterns = demeaner.fit_transform(ROI_patterns.transpose()).transpose() # remove mean across voxels\n",
    "    ROI_patterns_per_run.append( ROI_patterns ) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "...concatenate the data, labels, and run indices, across runs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data    = np.concatenate(ROI_patterns_per_run, axis = 0) # concatenate data across runs\n",
    "all_labels  = np.concatenate(labels_per_run,       axis = 0) # concatenate labels across runs\n",
    "run_indices = np.concatenate([[i] * np.size(ROI_patterns_per_run[i], axis = 0) for i in range(nruns)]) # label each run number\n",
    "print('Run indices: ', run_indices)\n",
    "\n",
    "print(f\"Total of {all_data.shape[0]} patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "...and try to decode faces within this ROI, using leave-one-run-out cross-validatation.\n",
    "This time we'll use yet another classifier - a **Fisher's linear discriminant (LDA)** classifer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA  = skl.discriminant_analysis.LinearDiscriminantAnalysis();\n",
    "pipe = pipeline.make_pipeline(PCA, LDA)\n",
    "\n",
    "fold_score = skl.model_selection.cross_val_score(pipe, all_data, all_labels, groups = run_indices,\n",
    "                                    scoring = 'roc_auc', cv = logo)\n",
    "observed_score = np.mean(fold_score)\n",
    "print(\"Cross-validated score per fold:\", fold_score)  \n",
    "print(\"Mean cross-validated score:  \", observed_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test significance\n",
    "\n",
    "This looks very accurate! But we should still confirm that it is *significantly* above chance (See Rik's session on statistics.) \n",
    "\n",
    "We cannot test this using a t-test across folds, because the folds are not independent (they share training data). \n",
    "\n",
    "If we had multiple participants, we could use a **one-sample t-test across participants**, versus chance. This is valid (assuming the specified chance level is correct) because participants are independent. \n",
    "\n",
    "For \"within-context\" cross-validation, below-chance accuracy is not meaningful (the true value can't be negative), which means we can use a one-tailed test; it also means that the test only provides fixed-effects rather than random-effects inference (see [Allefeld et al., 2016](https://pubmed.ncbi.nlm.nih.gov/27450073/)). For cross-validation that generalises *across* contexts (e.g. train the classifer to decode famous versus scrambled faces on initial presentations, then test decoding on repeated presentation) negative performance *could* be meaningful, so random-effects inference is justified, and a two-tailed test is appropriate. Similarly, when comparing classfication performance across conditions (e.g. is the performance of face decoding at initial presentation different from at repeat presentation?), paired-tests or repeated-measures ANOVA allow random-effects inference.\n",
    "\n",
    "However, if we want **to assess significance for a single participant, we would need a permutation test**. (Actually this applies to testing the significance of any single *classification* that can't be repeated across independent samples, e.g. a single classification of the disease status of multiple participants.) The permutation test involves shuffling the labels many times (e.g. 10,000) to generate a null distribution of classifier performance that would be expected under the null hypothesis of no difference between classes.\n",
    "\n",
    "In some situations, permutation tests may be necessary, but they do have disadvantages:\n",
    " - the large number of required permutations makes them slow\n",
    " - low numbers of samples limit the number of unique permutations, which limits robustness of the p-value\n",
    " - there's some complexity in ensuring the labels are actually \"exchangeable\", and hence risk of error.\n",
    "\n",
    "First, we'll code the permutations explicitly. It is important that this is done outside the cross-validation loop ([Valente et al., 2021](https://www.sciencedirect.com/science/article/pii/S1053811921004225#bib0022)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = tuple(logo.split(all_data, all_labels, run_indices))\n",
    "\n",
    "n_permutations = 200 # this is not nearly enough, but it will still take a while\n",
    "permuted_scores = np.zeros(n_permutations)\n",
    "null_labels = np.concatenate(labels_per_run, axis = 0) # re-concatenate labels across runs; to be shuffled below\n",
    "\n",
    "print(f'Permuting {n_permutations} times:')\n",
    "tic = time.time()\n",
    "for p in range(n_permutations):\n",
    "\n",
    "    okay = False\n",
    "\n",
    "    while not okay:\n",
    "        np.random.shuffle(null_labels) # (shuffle operates in-place)\n",
    "\n",
    "        try:\n",
    "            fold_score=np.full(nruns,np.nan)\n",
    "            for i, fold in enumerate(folds):\n",
    "                train_idx, test_idx = fold   \n",
    "\n",
    "                pipe.fit(all_data[train_idx,:], null_labels[train_idx])\n",
    "                predicted_probabilities = pipe.predict_proba(all_data[test_idx,:]) # do prediction on test data\n",
    "                fold_score[i] = skl.metrics.roc_auc_score(null_labels[test_idx], predicted_probabilities[:,1]) # syntax for binary classifcation\n",
    "            okay = True\n",
    "        except:\n",
    "            okay = False\n",
    "            \n",
    "    permuted_scores[p] = np.mean(fold_score)\n",
    "    print('.', end='')\n",
    "print('Done.')\n",
    "print('Took ', time.time()-tic,'  s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "Now let's plot the observed score and compare it to the null distribution of scores from permuted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(permuted_scores, element = 'step', alpha = 0.5)\n",
    "ax.set(xlabel = 'AUROC of permuted data');\n",
    "lh = ax.axvline(np.percentile(permuted_scores, 95), color = 'b',label = '95% threshold')\n",
    "mh = ax.plot(observed_score, 0, marker = 'o', color = 'r', markersize = 10, label = 'observed score')\n",
    "ax.legend();\n",
    "\n",
    "p = (sum(permuted_scores > observed_score) + 1) / (n_permutations + 1)\n",
    "print('p vlaue = ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the null distribution is centred around chance (0.5). The observed score is more extreme than the 95th percentile, so the classification is significantly above chance.\n",
    "\n",
    "\\\n",
    "Just as scikit-learn has a function (`cross_val_score`) to simplify the cross-validation loop, it also has a similar function (`permutation_test_score`) to simplify the permutation process. This also allows the permutation to be run in parallel, which can be much faster! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutations = 1000 # this is more like it\n",
    "\n",
    "tic = time.time()\n",
    "actual_score, permuted_scores, p = skl.model_selection.permutation_test_score(pipe, all_data, all_labels, groups = run_indices,\n",
    "                                    scoring = 'roc_auc', cv = logo, n_permutations = n_permutations, n_jobs = -1, random_state = None)\n",
    "\n",
    "print('Took ', time.time()-tic, ' s')\n",
    "print(\"p value = \", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "This p-value should, on average, be similar to above, but will differ because of the randomness of the permutations. As the number of unique permutations increases, the p-values from different repetitions should become more similar, giving an idea of the robustness of the permutation-p-value.\n",
    "\n",
    "## Searchlight analysis\n",
    "\n",
    "So far, we have considered a single ROI. If we want to know where in the brain a representation is strongest, we can compare multiple ROIs. Sometimes, we might be interested in discrete regions that cover the brain (e.g. an atlas/parcellation), but if we want a spatially continuous output, without making assumptions about area borders, we can use a \"searchlight\". \n",
    "\n",
    "A searchlight is just a set of overlapping ROIs, often spherical, that cover the brain (or analysis mask of interest). Each searchlight's classification performance is typically assigned to its central voxel. The inferences one would make are very similar to a voxel-wise, mass-univariate analysis of smoothed data (see Dace's sessions). Increasing the size of the searchlight reduces the spatial specificity of the inference, while tending to increase sensitivity, in a very similar way to increasing the smoothing kernel in univariate analysis. Similarly to the matched-filter theorem for univariate analyses, the optimal searchlight size will depend on the spatial scale of the signal and the spatial scale of the noise. It is sometimes assumed that searchlights need to be spatially contiguous, because they usually are, but the definition of a searchlight is just as flexible as the definition of any ROI. \n",
    "\n",
    "### Define the search space\n",
    "\n",
    "A searchlight analysis can be set up using nilearn's `.searchlight` object. This *is* restricted to contiguous spherical searchlights. When the searchlight object is created, it needs a \"voxel\" mask that specifies which voxels will be included in the searchlights.\n",
    "\n",
    "We'll use a voxel mask that covers the whole brain. As before, we can load a template-space mask from nilearn, and resample it to match the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_mask = nil.datasets.load_mni152_brain_mask()\n",
    "brain_mask = nil.image.resample_to_img(brain_mask, betas_per_run_nii[0], \n",
    "                                       interpolation = 'nearest',\n",
    "                                       force_resample = True,\n",
    "                                       copy_header = True)\n",
    "\n",
    "# we'll also restrict this mask to voxels that have data\n",
    "has_data =   nil.image.math_img('(img.prod(axis=3) !=0)', img = betas_per_run_nii[0]); # not zero for any sample\n",
    "brain_mask = nil.image.math_img('(in_mask & has_data)', in_mask = brain_mask, has_data = has_data);\n",
    "\n",
    "display = nil.plotting.plot_roi(brain_mask,\n",
    "                                alpha=0.5,\n",
    "                                cmap='summer', \n",
    "                                cut_coords=(-40, -52, -18),\n",
    "                                colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide a second, optional, \"process mask\" that constrains where the spheres are centred. To save some time, we'll restrict this to cortical grey matter of the left hemisphere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HO_atlas_subcort = nil.datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm', demo_dir) \n",
    "resampled_atlas  = nil.image.resample_to_img(HO_atlas_subcort['maps'], betas_per_run_nii[0],\n",
    "                                              interpolation = 'nearest',\n",
    "                                              force_resample = True,\n",
    "                                              copy_header = True)\n",
    "l_cortex_id = HO_atlas_subcort['labels'].index('Left Cerebral Cortex')  # get the ID of the Left Cerebral Cortex\n",
    "\n",
    "gm_mask = nil.image.math_img(f'(map == {l_cortex_id} )', map = resampled_atlas)               # create binary mask of this ROI\n",
    "gm_mask = nil.image.math_img('(in_mask & has_data)', in_mask = gm_mask, has_data = has_data); # restrict mask to voxels that have data\n",
    "\n",
    "display = nil.plotting.plot_roi(nil.image.math_img('a+b', a = gm_mask, b = brain_mask),\n",
    "                                alpha=0.5,\n",
    "                                cmap='summer', \n",
    "                                cut_coords=(-40, -52, -18),\n",
    "                                colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, green voxels could be part of each spherical searchlight (depending on its radius), but the centre of the spheres will only ever be placed in the yellow voxels.\n",
    "\n",
    "### Run the searchlight\n",
    "\n",
    "Now we can create the `searchlight` object. We will also specify some other optional inputs, explained in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = skl.pipeline.make_pipeline(LDA) # we won't do PCA, because this would fail for some searchlights (think about why)\n",
    "\n",
    "sl = nil.decoding.SearchLight(\n",
    "    mask_img = brain_mask,       # only include these voxels within searchlights\n",
    "    process_mask_img = gm_mask,  # only centre searchlights on these voxels \n",
    "    radius = 5,                  # in mm\n",
    "    estimator = pipe,            # a classifier or pipeline object\n",
    "    n_jobs = -1,                 # how many CPUs to use (-1 means the maximum available)\n",
    "    scoring = 'roc_auc',         # choice of scoring metric\n",
    "    cv = logo,                   # cross-validation object\n",
    "    verbose = False              # this is slow, so if running locally (1 job) set it to true to know how far it's got; when running in parallel it produces too much output, so set to false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "To launch the searchlight analysis, call the `searchlight` object's `.fit` method, passing it the patterns, the labels, and the groups for cross-validation.\n",
    "This will take a while (probably about 4 minutes on the virtual machine, running in parallel with 4 cores)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = nil.image.concat_imgs(betas_per_run_nii) # concatenate 4D nifti objects from each run (along the 4th dimension)\n",
    "\n",
    "print('Shape of 4D nifti data:', all_data.shape)\n",
    "print('Shape of mask:', sl.mask_img.shape)\n",
    "print('Shape of process_mask:', sl.process_mask_img.shape)\n",
    "\n",
    "print('Running searchlight...')\n",
    "tic = time.time()\n",
    "sl.fit(all_data, all_labels, groups = run_indices) # expect this to take ~ 3-5 minutes\n",
    "print('Took ', time.time()-tic, ' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "The output (here, mean AUROC) per voxel is stored in the  `scores_` property of the searchlight object (as a 3D numpy array). If we convert this to a nifti volume, we can plot it using nilearn. We'll also subtract chance (here, 0.5), so that positive and negative values will indicate above- and below-chance decoding performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of output numpy array: ', sl.scores_.shape)\n",
    "scores_minus_chance = sl.scores_ - 0.5 # subtract chance\n",
    "score_img = nil.image.new_img_like( brain_mask, scores_minus_chance); # save in samme format as the brain_mask\n",
    "score_img = nil.image.math_img('score_img * processing_mask', score_img = score_img, processing_mask = gm_mask); # set voxels outside the processing mask to zero\n",
    "nil.plotting.plot_stat_map(score_img, threshold = 0, cmap = 'bwr', cut_coords = (-40, -52, -18));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see reasonable classification in the fusiform gyrus, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nil.plotting.view_img_on_surf(score_img, threshold = 0, cmap = 'bwr', symmetric_cmap = True, darkness = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of this MVPA demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
