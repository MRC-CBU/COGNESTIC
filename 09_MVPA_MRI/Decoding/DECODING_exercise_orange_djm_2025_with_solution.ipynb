{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcac860",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h1><span style=\"color:orange\">The ORANGE exercise</span></h1>\n",
    "\n",
    "Patterns from each of ***two classes have been drawn from different distributions***. I.e. there is a true difference between them. However, a simple classification has been run, and the ***classifier has returned an accuracy quite close to chance***. \n",
    "\n",
    "***Is there a problem?***\n",
    "\n",
    "***Is there a simple way to improve the analysis to increase the classification score?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e394300-dd0b-4210-aec7-7b830adac6bc",
   "metadata": {},
   "source": [
    "## Getting ready\n",
    "\n",
    "Import the packages we might need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a25811-9c9a-4b40-85b5-b5ce42aa25bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np              # Lets python process matrices, like Matlab\n",
    "import matplotlib.pyplot as plt # Lets python plot graphs like Matlab\n",
    "\n",
    "# scikit-learn is the major library for machine learning in Python:\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing         # includes LabelEncoder, OneHotEncoder, StandardScaler...\n",
    "from sklearn import model_selection       # includes StratifiedKFold, LeaveOneGroupOut...\n",
    "from sklearn import linear_model          # includes LogisticRegression, RidgeClassifier...\n",
    "from sklearn import svm                   # includes SVC, NuSVC & LinearSCV...\n",
    "from sklearn import discriminant_analysis # includes LinearDiscriminantAnalysis\n",
    "from sklearn import metrics               # includes roc_auc_score...\n",
    "from sklearn import pipeline              # includes make_pipeline\n",
    "from sklearn import inspection            # includes DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d3071-e841-415b-a8d4-d8b989157b89",
   "metadata": {},
   "source": [
    "Set the random number generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51954c-bbe6-4bdb-b8c7-77f7e0a34f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f7243-78fd-41d7-a680-ab434f353fd4",
   "metadata": {},
   "source": [
    "## Simulate some data with a true difference between conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2561c1-80b3-46ca-98cc-3cccb7eb91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox = 2 # number of voxels\n",
    "nruns= 6 # number of runs\n",
    "n_samples_per_run = 20 # number of samples/patterns per run; these will be divided into conditions/classes \"0\" and \"1\"\n",
    "proportion_of_samples_from_condition_0 = 0.9\n",
    "\n",
    "mu0 = np.arange(nvox) # mean activation for condition 0 (voxels all have different activation strength)\n",
    "mu1 = np.roll(mu0, 1) # mean activation for condition 1 (same overall activity, but distributed differently across voxels)\n",
    "voxel_covariance0 = np.diag(mu0) + 1 # voxel covariance: independent noise per voxel is proportional to mean, plus some covariance\n",
    "voxel_covariance1 = np.diag(mu1) + 1 # voxel covariance: independent noise per voxel is proportional to mean, plus some covariance\n",
    "\n",
    "data_per_run =   []  # list of pattern matrices (one for each run)\n",
    "labels_per_run = []  # list of label vectors (one for each run)\n",
    "for r in np.arange(nruns):\n",
    "    # label each sample as condition 0 or 1 (the +0 is a trick to convert the logical values to integers):\n",
    "    label_vector= ((np.arange(n_samples_per_run)/n_samples_per_run) >= proportion_of_samples_from_condition_0) + 0;\n",
    "    \n",
    "    # sample activations from each distribution:\n",
    "    data_matrix0 =  np.random.multivariate_normal(mu0, voxel_covariance0, size=np.sum(label_vector == 0))\n",
    "    data_matrix1 =  np.random.multivariate_normal(mu1, voxel_covariance1, size=np.sum(label_vector == 1))\n",
    "    data_matrix  =  np.concatenate((data_matrix0, data_matrix1))\n",
    "    \n",
    "    data_per_run.append( data_matrix ) \n",
    "    labels_per_run.append( label_vector ) \n",
    "\n",
    "# concatenate runs\n",
    "data        = np.concatenate(data_per_run,   axis = 0)             \n",
    "labels      = np.concatenate(labels_per_run, axis = 0)\n",
    "run_indices = np.concatenate([[i] * n_samples_per_run for i in range(nruns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828b4e4-600d-4c36-8edd-255b38b2e570",
   "metadata": {},
   "source": [
    "\\\n",
    "Plot the data for the first two voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695b6ca-7c5c-4d04-a499-c04b8133e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))  # create a matplotlib figure\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.title('Activation patterns for first two voxels')\n",
    "scatter = plt.scatter(data[:,[0]], data[:,[1]], \n",
    "                      s = 90, alpha = 0.7, c = labels, cmap = 'bwr')\n",
    "plt.legend(handles = scatter.legend_elements()[0], labels = set(np.unique(labels))) # \"set\" returns unique values\n",
    "plt.xlabel('Voxel 1 activity')\n",
    "plt.ylabel('Voxel 2 activity')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90082002-2b19-4651-8068-4cf4a31498b6",
   "metadata": {},
   "source": [
    "\\\n",
    "Specify the pre-processing, classification pipeline, and a leave-one-run-out cross-validation scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f81459-4298-4f44-b043-acd2cd9e7b22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = skl.preprocessing.MinMaxScaler()\n",
    "SVM    = skl.svm.LinearSVC(dual = True)\n",
    "pipe   = skl.pipeline.make_pipeline(scaler, SVM)\n",
    "logo   = skl.model_selection.LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162544b-2451-44b1-87b3-c4b4f584ab9d",
   "metadata": {},
   "source": [
    "\\\n",
    "Run the classification analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a93dd-588d-4816-a599-3352d008a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = skl.model_selection.cross_val_score(pipe, data, labels, groups  = run_indices, cv = logo, scoring = 'balanced_accuracy')\n",
    "\n",
    "print(\"Accuracy per fold:\", accuracy)\n",
    "print(\"Mean accuracy for data with a true difference between conditions:\", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a0406-8231-405a-b857-266119dbce03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The classifier has returned an accuracy quite close to chance. \n",
    "\n",
    "Is there a problem? Is there a simple way to improve the analysis to increase the classification score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66aad9-215f-42c3-b830-1d49c493428f",
   "metadata": {},
   "source": [
    "# Explanation and possible solution\n",
    "\n",
    "The dataset is very unbalanced, i.e. there are many more samples in one class than the other. This means that a naive classifier will be biased to select the over-represented class, more often than would be optimal.\n",
    "\n",
    "We can see this by plotting the classification boundaries per run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0b9a7-0968-4bb2-b5e0-f6dcaed7a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(nruns):\n",
    "    SVM.fit(data_per_run[r], labels_per_run[r])\n",
    "    skl.inspection.DecisionBoundaryDisplay.from_estimator(SVM, data, \n",
    "                                                      alpha = 0.2, ax = fig.axes[0], cmap = 'bwr', response_method = 'predict');\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f2cc6-6491-449d-ac82-e681f51db560",
   "metadata": {},
   "source": [
    "On average, the classification boundaries seem too far to the bottom right. \n",
    "\n",
    "The performance metric used here (balanced accuracy) is unbiased, but is unable to counteract the reduced sensitivity caused by the initial bias in the classifier. To force the classifier to learn the *relevant* information (voxel activation pattern differences) and to ignore the irrelevant information (sample proportions), set `class_weight = 'balanced'` when creating the classifier object. A  more involved solution could be to sub-sample the over-represented class to make the classes balanced (see the RED exercise for a way this might be implemented). \n",
    "\n",
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974177f-ab12-4faf-90b5-d91158c4b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_balanced  = skl.svm.LinearSVC(dual = True, class_weight = 'balanced')\n",
    "pipe_balanced = skl.pipeline.make_pipeline(scaler, SVM_balanced)\n",
    "accuracy = skl.model_selection.cross_val_score(pipe_balanced, data, labels, groups  = run_indices, cv = logo, scoring = 'balanced_accuracy')\n",
    "\n",
    "print(\"Accuracy per fold:\", accuracy)\n",
    "print(\"Mean accuracy for data with a true difference between conditions:\", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0385684-ee98-44ed-95a0-113aa1e77f1f",
   "metadata": {},
   "source": [
    "\\\n",
    "Now we get much higher accuracies. If we replot the decision boundaries per run, we will see they are less biased: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ecb6e2-cc95-4d96-b4f1-9ae432060573",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))  # create a matplotlib figure\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.title('Activation patterns for first two voxels')\n",
    "scatter = plt.scatter(data[:,[0]], data[:,[1]], \n",
    "                      s = 90, alpha = 0.7, c = labels, cmap = 'bwr')\n",
    "plt.legend(handles = scatter.legend_elements()[0], labels = set(np.unique(labels))) # \"set\" returns unique values\n",
    "plt.xlabel('Voxel 1 activity')\n",
    "plt.ylabel('Voxel 2 activity')\n",
    "plt.axis('equal')\n",
    "\n",
    "for r in range(nruns):\n",
    "    SVM_balanced.fit(data_per_run[r], labels_per_run[r])\n",
    "    skl.inspection.DecisionBoundaryDisplay.from_estimator(SVM_balanced, data, \n",
    "                                                      alpha = 0.2, ax = fig.axes[0], cmap = 'bwr', response_method = 'predict');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e39898-11fa-4fb4-a29d-e6ba6d23f879",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Extension: \n",
    "Imbalance tends to be more problematic when spuriously inflating classification performance (see the RED exercise), than when reducing senstitivity to a true effect, because imbalance typically needs to be quite extreme to swamp the true information in the patterns. Explore this by ensuring that `class_weight = 'balanced'` is NOT set, and change the proportion of samples that come from class \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72ddd6-2ae5-47ca-82c5-9838655191f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
