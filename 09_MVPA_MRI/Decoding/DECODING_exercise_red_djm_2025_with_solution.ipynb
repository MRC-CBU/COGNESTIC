{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c4723d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h1><span style=\"color:red\">The RED exercise</span></h1>\n",
    "\n",
    "Simulated patterns from each of ***two classes have been drawn from an identical distribution***. I.e. there is no true difference between them. A simple classification has been run for one participant, and the ***classifier returns a suspiciously high accuracy***. \n",
    "\n",
    "***What is the issue?***\n",
    "\n",
    "***Improve the analysis so that the classification score is not misleadingly high.*** There are multiple ways that this could be achieved - how many can you think of? How many can you implement?\n",
    "\n",
    "Extension: run your improved analysis for 35 subjects, and confirm that your classification score is not significantly higher than a chance level equivalent to 50% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ecea4a-358a-4fe3-96b4-7fc9c127c06b",
   "metadata": {},
   "source": [
    "## Getting ready\n",
    "\n",
    "Import the packages we might need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40e4f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np              # Lets python process matrices, like Matlab\n",
    "import matplotlib.pyplot as plt # Lets python plot graphs like Matlab\n",
    "\n",
    "# scikit-learn is the major library for machine learning in Python:\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing         # includes LabelEncoder, OneHotEncoder, StandardScaler...\n",
    "from sklearn import model_selection       # includes StratifiedKFold, LeaveOneGroupOut...\n",
    "from sklearn import linear_model          # includes LogisticRegression, RidgeClassifier...\n",
    "from sklearn import svm                   # includes SVC, NuSVC & LinearSCV...\n",
    "from sklearn import discriminant_analysis # includes LinearDiscriminantAnalysis\n",
    "from sklearn import metrics               # includes roc_auc_score...\n",
    "from sklearn import pipeline              # includes make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c40a7-aba9-438c-b5f4-2945ac167df5",
   "metadata": {},
   "source": [
    "Set the random number generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee7287-2678-4ce8-9443-685edd8e37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4317c-cd67-4585-81d7-df9fc7ae16c4",
   "metadata": {},
   "source": [
    "## Simulate the data, with no true difference between conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvox  = 2 # number of voxels\n",
    "nruns = 4 # number of runs\n",
    "n_samples_per_run = 10 # number of samples/patterns per run; these will be divided into conditions/classes \"0\" and \"1\"\n",
    "proportion_of_samples_from_condition_0 = 0.8\n",
    "\n",
    "mu = np.arange(nvox)               # mean activation for both conditions (voxels all have different activation strength)\n",
    "voxel_covariance = np.diag(mu) + 1 # voxel covariance for both conditions: independent noise per voxel is proportional to mean, plus some covariance\n",
    "\n",
    "null_data_per_run = [] # list of pattern matrices (one for each run)\n",
    "labels_per_run    = [] # list of label vectors    (one for each run)\n",
    "for r in np.arange(nruns):\n",
    "    # activations are drawn from THE SAME DISTRIBUTION FOR BOTH CONDITIONS!\n",
    "    data_matrix =  np.random.multivariate_normal(mu, voxel_covariance, size=n_samples_per_run) \n",
    "\n",
    "    # label each sample as condition 0 or 1 (the +0 is a trick to convert the logical values to integers):\n",
    "    label_vector= ((np.arange(n_samples_per_run)/n_samples_per_run) >= proportion_of_samples_from_condition_0) + 0;\n",
    "    \n",
    "    null_data_per_run.append( data_matrix ) \n",
    "    labels_per_run.append  ( label_vector ) \n",
    "\n",
    "# concatenate runs\n",
    "null_data   = np.concatenate(null_data_per_run, axis = 0)             \n",
    "labels      = np.concatenate(labels_per_run, axis = 0)\n",
    "run_indices = np.concatenate([[i] * n_samples_per_run for i in range(nruns)]) # make vector of run indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4dff5-678d-4071-a283-41dc82d41c65",
   "metadata": {},
   "source": [
    "\\\n",
    "Plot the data for the first two voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f808abc-71a3-499c-b225-529049f0fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))  # create a matplotlib figure\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.title('Activation patterns for first two voxels')\n",
    "scatter = plt.scatter(null_data[:,[0]], null_data[:,[1]], \n",
    "                      s = 90, alpha = 0.7, c = labels, cmap = 'bwr')\n",
    "plt.legend(handles = scatter.legend_elements()[0], labels = set(np.unique(labels))) # \"set\" returns unique values\n",
    "plt.xlabel('Voxel 1 activity')\n",
    "plt.ylabel('Voxel 2 activity')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33402af8-ccb4-4728-a5f7-8b1e48aa2777",
   "metadata": {},
   "source": [
    "\\\n",
    "Specify the pre-processing, classification pipeline, and a leave-one-run-out cross-validation scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d0ec7-5607-42da-bb32-34931bcb24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = skl.preprocessing.MinMaxScaler()\n",
    "SVM    = skl.svm.LinearSVC(dual = True)\n",
    "pipe   = skl.pipeline.make_pipeline(scaler, SVM)\n",
    "logo   = skl.model_selection.LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f5a351-ca49-489b-a47e-f141c7b58e5d",
   "metadata": {},
   "source": [
    "\\\n",
    "Run the classification analysis, with a fair bit of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca38afd-3fa1-45e8-9b95-5052e20ef49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = tuple(logo.split(null_data, labels, run_indices))\n",
    "accuracy = np.full(nruns, np.nan) # initialise vector to store accuracies\n",
    "for i, fold in enumerate(folds):\n",
    "    train_idx, test_idx = fold # get the train and test indices\n",
    "    pipe.fit(null_data[train_idx,:], labels[train_idx])    # do all preprocessing and model fitting on training data\n",
    "    predicted_labels = pipe.predict(null_data[test_idx,:]) # do prediction on test data\n",
    "    accuracy[i] = skl.metrics.accuracy_score(labels[test_idx], predicted_labels) # score the predictions\n",
    "print(\"Accuracy per fold:\", accuracy)\n",
    "print(\"Mean accuracy for random data:\", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95984d2-17cc-46e1-b981-3a3ffd5d270d",
   "metadata": {},
   "source": [
    "\\\n",
    "...or run it the quick way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc7479-e0f8-4784-976c-289048410bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = skl.model_selection.cross_val_score(pipe, null_data, labels, groups = run_indices, cv = logo, scoring = 'accuracy')\n",
    "\n",
    "print(\"Accuracy per fold:\", accuracy)\n",
    "print(\"Mean accuracy for random data:\", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761987b0-6c28-4e86-a735-c1f41ccb5f9d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Questions:\n",
    " - We have two classes, drawn from the same distribution, so why is accuracy not close to 0.5?\n",
    " - Is the classifier performing as it should?\n",
    " - Can you change the analysis to get a classification score closer to a level equivalent to 50% accuracy? There are multiple ways that this could be achieved - how many can you think of? How many can you implement?\n",
    "\n",
    "## Hints:\n",
    "- Consider the data that are being provided to the classifier per class.\n",
    "- Think about the choice of performance metric.\n",
    "- Consider other options when specifying the classifier.\n",
    "\n",
    "## Extension: \n",
    "- Run your improved analysis for 35 subjects and confirm that your classification score is not significantly higher than a chance level equivalent to 50% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88ffee-006a-4e6d-8c9d-71a3194c570b",
   "metadata": {},
   "source": [
    "# Explanation and possible solutions\n",
    "\n",
    "The problem here is that the dataset is **unbalanced**, i.e. there are more samples in one class than the other. This means that a naive classifier can always guess that samples come from the over-represented class, even without looking at the data, and still be correct most of the time. Note that the classifier is doing exactly what it is supposed to: maximizing its average classification performance. The issue is that the classifier can use information in the class proportions, whereas we are only interested in information it can get from the patterns.\n",
    "\n",
    "The easiest solution is to **use a performance metric that adjusts for any bias due to imbalanced classes**. The area under the ROC curve, and other signal-detection-theory-based metrics (e.g. d') do this. There is also a variant of \"accuracy\" called \"balanced accuracy\", which scores the accuracy per class and then averages these over classes. These metrics ensure that the performance estimate is not biased by any imbalance, but they do not prevent the classifier from learning the class imbalance in the first place. Therefore sensitivity can still be reduced in the presence of severe imbalance (see the ORANGE exercise). \n",
    "\n",
    "Another set of solutions is to **ensure that the classes are balanced within each training set**. Ideally this would be done when designing the experiment, but sometimes it is not possible: some conditions may necessarily be rarer than others; assignment of samples to classes might depend on something that can't be controlled, such as behaviour; samples may be missing at random due to technical faults or noisy data. Another option is to randomly sub-sample the over-represented class until the classes are balanced. This is not ideal, because (a) it is not straightforward, (b) it reduces the amount of training data (and therefore classifier accuracy), and (c) repeated subsampling is stochastic and time-consuming. A related option could be to create additional \"pseudo-samples\" for the under-represented class, by randomly averaging the available samples. This is also time-consuming, tricky to set up, and may not make sense depending on the context.\n",
    "\n",
    "Yet another solution is to **tell the classifier about the imbalance** when training it (so that it can emphasise information in the patterns). This requires the particular classifier implementation to have this option. Typically it works by proportionally up-weighting samples from the under-represented class. \n",
    "\n",
    "Let's try these three approaches:\n",
    "\n",
    "### 1) Use a performance metric that is not biased by unbalanced classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372ff8d-89ed-46ca-842f-523c07d6d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy = skl.model_selection.cross_val_score(pipe, null_data, labels, groups  = run_indices, cv = logo, scoring = 'balanced_accuracy')\n",
    "\n",
    "print(\"Balanced accuracy per fold:\", balanced_accuracy)\n",
    "print(\"Mean balanced accuracy for random data:\", np.mean(balanced_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5cea0-44d4-4254-a700-45b3319718e7",
   "metadata": {},
   "source": [
    "\"Chance level\" is now 0.5.\n",
    "\n",
    "### 2) Sub-sample the over-represented class:\n",
    "This is a bit more involved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f814be-be13-4dac-9f83-2aa33c108885",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.full(nruns, np.nan)\n",
    "for i, fold in enumerate(folds):\n",
    "    train_idx, test_idx = fold # get the train and test indices\n",
    "    ############# the new bit:\n",
    "    min_sample_per_class= np.min(np.bincount(labels[train_idx]))\n",
    "    sort_vec = np.argsort(labels[train_idx])\n",
    "    train_idx= np.concatenate((train_idx[sort_vec[0:min_sample_per_class]], train_idx[sort_vec[-min_sample_per_class:]]))\n",
    "    #############\n",
    "    pipe.fit(null_data[train_idx,:], labels[train_idx])    # do all preprocessing and model fitting on training data\n",
    "    predicted_labels = pipe.predict(null_data[test_idx,:]) # do prediction on test data\n",
    "    accuracy[i] = skl.metrics.accuracy_score(labels[test_idx], predicted_labels) # score the predictions\n",
    "print(\"Accuracy per fold:\", accuracy)\n",
    "print(\"Mean accuracy for random data:\", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad113f-2667-48c7-a405-d25cf8a5bbe8",
   "metadata": {},
   "source": [
    "Accuracy estimates are now centred on 0.5 (even using simple \"accuracy\"), but notice that they are also more variable.\n",
    "\n",
    "### 3) Tell the classifier to adjust for imbalance by re-weighting samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235a846-444e-45da-936f-1a6af0fe3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### the new bit:\n",
    "SVM    = skl.svm.LinearSVC(dual = True, class_weight = 'balanced')\n",
    "########################\n",
    "pipe   = skl.pipeline.make_pipeline(scaler, SVM)\n",
    "\n",
    "accuracy = skl.model_selection.cross_val_score(pipe, null_data, labels, groups = run_indices, cv = logo, scoring = 'accuracy')\n",
    "\n",
    "print(\"Accuracy per fold:\", accuracy)\n",
    "print(\"Mean accuracy for random data:\", np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3857f7-57d7-43b1-908d-b9ae7ff44987",
   "metadata": {},
   "source": [
    "As above, accuracy estimates are now close to 0.5 (even using simple \"accuracy\"), but again notice that they are also more variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30426b-602c-479e-be77-8821cfdd06e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
