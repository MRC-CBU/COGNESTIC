{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Ap≈°valka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) with edits by [Rik Henson](https://www.mrc-cbu.cam.ac.uk/people/rik.henson/)\n",
    "- **Date:** August 2025  \n",
    "- **conda environment**: [mri conda environment](../mri_environment.yml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI Data Analysis: Group-Level Analysis\n",
    "\n",
    "Once you have beta (or contrast) maps for conditions (or contrasts) from all subjects, you can perform group-level statistics. Importantly, all subject first-level results need to be in common space, e.g., MNI, to perform voxel-wise group analyses. Group-level analysis allows you to make inferences about the population, rather than individual subjects, by assessing common activations across participants. Common statistical methods for group-level analysis include one-sample or paired t-tests, as well as more complex ANOVA models, depending on your study design.\n",
    "\n",
    "In this tutorial, we illustrate an ANOVA in which we include 9 contrasts, one for each of the 9 experimental conditions (averaged across runs), as well as 16 additional regressors to remove between-subject variance. This corresponds to a *repeated-measures ANOVA*, though assumes that the (pooled) error is spherical *(see [Rik's Stats tutorial](../03_Statistics/nb03_general_stats.ipynb) section on ANOVA for the importance of this)*. \n",
    "\n",
    "Here is a recommended viewing to help better understand the principles of the group-level analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('''\n",
    "<div style=\"width: 1200px; margin: 0; display: flex; justify-content: space-around;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <h3><a href=\"https://www.youtube.com/embed/__cOYPifDWk\">Group-Level Analysis (7 min)</a></h3>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/__cOYPifDWk\" frameborder=\"0\" allow=\"picture-in-picture\" allowfullscreen></iframe>\n",
    "    </div>\n",
    "</div>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**    \n",
    "1. Import required packages and set up some stuff    \n",
    "2. Retrieve First-Level results    \n",
    "3. Displaying subject Effects-Of-Interest z-maps    \n",
    "4. Specify the second-level model    \n",
    "4.1. Design matrix for 3x3 repeated-measures ANOVA   \n",
    "4.2. Model specification and fit Contrasts  \n",
    "4.3. Contrasts   \n",
    "5. Computing contrasts and plotting result maps    \n",
    "5.1. Faces > Scrambled \n",
    "5.2. Famous > Unfamiliar  \n",
    "5.3. Stimulus x Immediate Repetition interaction  \n",
    "5.4. FWE correction using non-parametric permutation testing    \n",
    "6. Summary results  \n",
    "6.1. Using atlasreader package    \n",
    "6.2. Nilearn's report   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Import required packages and set up some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob # to search for files using regex\n",
    "\n",
    "import pandas as pd # for data manipulation\n",
    "import numpy as np # for numerical operations\n",
    "\n",
    "from bids import BIDSLayout # to fetch data from BIDS-compliant datasets\n",
    "\n",
    "import matplotlib.pyplot as plt # for basic plotting\n",
    "\n",
    "import nibabel as nib # NiBabel, to read and write neuroimaging data, https://nipy.org/nibabel/\n",
    "\n",
    "# Nilearn modules, for the analysis of brain volumes, plotting, etc., https://nilearn.github.io/\n",
    "from nilearn.plotting import plot_glass_brain, plot_design_matrix, plot_contrast_matrix, plot_stat_map, view_img, view_img_on_surf\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.glm.thresholding import threshold_stats_img\n",
    "from nilearn.datasets import load_mni152_template\n",
    "from nilearn.glm.second_level import non_parametric_inference\n",
    "\n",
    "from atlasreader import create_output # For generating result tables https://github.com/miykael/atlasreader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNI152 template will be used as a backgound for plotting results\n",
    "mni152_template = load_mni152_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve First-Level results\n",
    "\n",
    "For the group analysis, we will use the single-condition contrast estimate (beta estimate) maps for all nine conditions. Because we saved the results in BIDS format, we can us PyBIDS to retrieve the subject-level results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up the paths to the data and results folders\n",
    "fmri_data_dir = 'FaceRecognition/data' # data in BIDS format\n",
    "fmri_results_dir = 'FaceRecognition/results' # results in BIDS format\n",
    "\n",
    "fmri_group_dir = os.path.join(fmri_results_dir, 'group-level') # where group results will go\n",
    "os.makedirs(fmri_group_dir, exist_ok=True)\n",
    "\n",
    "# --- Set up the BIDS layout\n",
    "layout = BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "\n",
    "# Attach the results folder to the layout. It must complay with BIDS standards. \n",
    "# And must include dataset_description.json file!\n",
    "layout.add_derivatives(os.path.join(fmri_results_dir, \"first-level\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying subject Effects-Of-Interest z-maps\n",
    "\n",
    "To check how the first-level results look overall, it is helpful to display effects-of-interest for all subjects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoi_maps = layout.get(desc='EffectsOfInterest', extension='.nii.gz', return_type='file')\n",
    "print(*eoi_maps, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = layout.get_subjects()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(14, 14))\n",
    "\n",
    "for i, stat_map in enumerate(eoi_maps):\n",
    "    plot_glass_brain(\n",
    "      stat_map,\n",
    "      title = 'sub-' + subjects[i],\n",
    "      axes = axes[int(i / 4), int(i % 4)],\n",
    "      plot_abs = False, \n",
    "      display_mode='x',\n",
    "      colorbar=False\n",
    "    )\n",
    "fig.suptitle('Effects of interest' + ', unthresholded z-maps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the second-level model\n",
    "\n",
    "At the group-level analysis, we also use a GLM. The outcome variable is the beta/contrast estimate from each subject, and the predictor variables typically include group-level factors such as experimental conditions, subject-specific regressors (in repeated-measures designs), group-specific regressors (in between-subject designs), and subject-specific covariates (e.g., age, gender, or behavioural scores).\n",
    "\n",
    "One approach is to obtain the contrast of interest maps (e.g. Faces > Scrambled) at the subject-level, and take these to the group-level one-sample T-test, i.e, testing whether those contrasts are significantly above or below zero when averaged across participants. This is called the **\"partitioned error\" or \"summary statistic\" approach**. With this approach, one has to estimate a new second-level model for each contrast of interest.\n",
    "\n",
    "Alternatively, you can compute beta maps for each condition (e.g., Faces, Scrambled, etc.), average across runs, and take these to the group level. This allows you to run a full ANOVA model across all conditions (e.g., 3 √ó 3 design with our 9 conditions). This is called the **\"pooled error\" approach**. It can be more powerful, and simpler, because all contrasts can be tested within a single model. However, it makes stronger assumptions about the sphericity of the error, which may be violated, which is why the above \"partitioned error\" approach (one-sample t-test) is generally safer. See [Rik's stats notebook](../03_Statistics/nb03_general_stats.ipynb) for more information.\n",
    "\n",
    "Below we will perform the second approach - 3x3 ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design matrix for 3x3 repeated-measures ANOVA\n",
    "\n",
    "In this example, the design matrix we generate will incorporate both **within-subject (conditions)** and **between-subject (subjects)** factors. Each row in the design matrix corresponds to a specific observation, which in this case is a beta estimate from a given condition and subject, while each column represents a predictor variable.\n",
    "\n",
    "The number of rows in the design matrix must match the number of first-level result files that will be entered into the second-level model. The order of the rows in the design matrix must match the order of the provided files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which conditions to include in the analysis and retrieve their effect files from the first-level results.\n",
    "conditions = ['IniFF', 'ImmFF', 'DelFF', 'IniUF', 'ImmUF', 'DelUF', 'IniSF', 'ImmSF', 'DelSF']\n",
    "\n",
    "effect_files = layout.get(desc=conditions, suffix='effect', extension='.nii.gz', return_type='filename')\n",
    "\n",
    "# print to see if it found what we expexted\n",
    "print(f\"Found {len(effect_files)} effect files:\")\n",
    "print(*effect_files[0:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we have reordered conditions so that the \"stimulus\" factor (Famous Face, Unfamiliar Face, Scrambled Face) rotates slowest, and \"presentation\" factor (Initial, Immediate Repeat, Delayed Repeat) rotates fastest in this 3x3 design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame\n",
    "df = pd.DataFrame(columns=conditions + subjects)\n",
    "\n",
    "# Populate the DataFrame with 0s and 1s\n",
    "for i, condition in enumerate(conditions):\n",
    "    # Filter files based on condition\n",
    "    condition_files = [1 if condition in file else 0 for file in effect_files]\n",
    "    # Add a column for the condition\n",
    "    df[condition] = condition_files\n",
    "\n",
    "# Populate the DataFrame with 0s and 1s for subjects\n",
    "for i, subject in enumerate(subjects):\n",
    "    # Filter files based on subject\n",
    "    subject_files = [1 if f\"sub-{subject}\" in file else 0 for file in effect_files]\n",
    "    # Add a column for the subject\n",
    "    df[subject] = subject_files\n",
    "\n",
    "# Display the design matrix\n",
    "design_matrix = df\n",
    "ax = plot_design_matrix(design_matrix)\n",
    "ax.set_title(\"Second level design matrix\", fontsize=12)\n",
    "ax.set_ylabel(\"stat maps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 9 columns capture the mean of each condition, while the remaining 16 columns capture the mean of each subject (which we do not care about, and removing this variance can improve statistics for contrasts over the first 9 columns by reducing the residual error). \n",
    "\n",
    "### Model specification and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify\n",
    "second_level_model = SecondLevelModel() \n",
    "\n",
    "# fit\n",
    "second_level_model = second_level_model.fit(\n",
    "  effect_files, \n",
    "  design_matrix = design_matrix\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrasts\n",
    "\n",
    "We can specify some contrasts that might be of interest. The first two are T-contrasts (one row), the last is an F-contrast (more than one row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_columns = design_matrix.shape[1]\n",
    "n_conditions = len(conditions)\n",
    "\n",
    "contrasts = {\n",
    "  'FacesScrambled':   np.pad([1/6,  1/6,  1/6,  1/6,  1/6,  1/6, -1/3, -1/3, -1/3], (0, n_columns - n_conditions), 'constant'),\n",
    "  'FamousUnfamiliar': np.pad([1/3,  1/3,  1/3, -1/3, -1/3, -1/3,    0,    0,    0], (0, n_columns - n_conditions), 'constant'),\n",
    "  'Stimulus X Immediate Repetition': np.array([np.pad(np.kron([1/2, 1/2, -1], [1, -1, 0]), (0, n_columns - n_conditions), 'constant'),\n",
    "                                     np.pad(np.kron([1,   -1,   0], [1, -1, 0]), (0, n_columns - n_conditions), 'constant')])\n",
    "}\n",
    "\n",
    "for contrast_id, contrast_val in contrasts.items():\n",
    "    plot_contrast_matrix(contrast_val, design_matrix=design_matrix)\n",
    "    plt.suptitle(contrast_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no particular scientific reason for the specific F-contrast interaction above, other than to demonstrate how to construct such contrasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing contrasts and plotting result maps\n",
    "\n",
    "We could then loop through every contrast like below, using the same threshold:\n",
    "```python\n",
    "for contrast_id, contrast_val in contrasts.items():\n",
    "    print(f\"\\tcontrast id: {contrast_id}\")\n",
    "    z_map = second_level_model.compute_contrast(contrast_val, output_type=\"z_score\")\n",
    "\n",
    "    thresholded_map, threshold = threshold_stats_img(\n",
    "        z_map, \n",
    "        alpha = 0.001, \n",
    "        height_control = None, \n",
    "        two_sided = True)\n",
    "    \n",
    "    plot_stat_map(\n",
    "        thresholded_map, \n",
    "        threshold = threshold, \n",
    "        bg_img = mni152_template,\n",
    "        display_mode=\"ortho\",\n",
    "        black_bg = True,\n",
    "        cmap = 'hot', \n",
    "        title = contrast_id)  \n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10,3)\n",
    "    plt.show()\n",
    "```\n",
    "but let's look at each contrast separately. \n",
    "\n",
    "### Faces > Scrambled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the z-maps for the contrast\n",
    "z_map = second_level_model.compute_contrast(\n",
    "  contrasts['FacesScrambled'], \n",
    "  output_type='z_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_map_bonf, threshold_bonf = threshold_stats_img(\n",
    "  z_map, \n",
    "  alpha=0.05, \n",
    "  height_control='bonferroni', \n",
    "  two_sided=False)\n",
    "\n",
    "print('Bonferroni p<.05 threshold: %.3f' % threshold_bonf)\n",
    "\n",
    "plot_stat_map(\n",
    "    thresholded_map_bonf, \n",
    "    bg_img = mni152_template,\n",
    "    threshold=threshold_bonf,   \n",
    "    display_mode = 'ortho',\n",
    "    black_bg = True,\n",
    "    cmap = 'hot',    \n",
    "    title = 'Faces > Scrambled  (Bonf. p<.05)'\n",
    "    )\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plotting\n",
    "plot = view_img(\n",
    "  thresholded_map_bonf, \n",
    "  bg_img=mni152_template, \n",
    "  threshold=threshold_bonf, \n",
    "  colorbar=True, \n",
    "  cmap = 'cold_hot',\n",
    "  title='Faces > Scrambled  (Bonf. p<.05)'\n",
    "  )\n",
    "\n",
    "plot.open_in_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Famous > Unfamiliar\n",
    "\n",
    "Next, what about the famous vs unfamiliar contrast. This time we will use a one-tailed FDR correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map = second_level_model.compute_contrast(contrasts['FamousUnfamiliar'], output_type=\"z_score\")\n",
    "thresholded_map, threshold = threshold_stats_img(\n",
    "        z_map, \n",
    "        alpha = 0.05, \n",
    "        height_control = 'fdr', \n",
    "        two_sided = False)\n",
    "plot_stat_map(\n",
    "        thresholded_map, \n",
    "        threshold = threshold, \n",
    "        bg_img = mni152_template,\n",
    "        display_mode=\"ortho\",\n",
    "        black_bg = True,\n",
    "        cmap = 'hot', \n",
    "        title = 'Famous > Unfamiliar',\n",
    "    )       \n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe increased activity in medial prefrontal regions for famous faces, which participants likely recognised. This may reflect engagement of semantic memory or emotional processing.\n",
    "\n",
    "### Stimulus x Immediate Repetition interaction\n",
    "\n",
    "Finally, what about the interaction F-contrast we created, which asks whether the effect of repetition depends on whether the repeated stimulus is a face or scrambled face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map = second_level_model.compute_contrast(contrasts['Stimulus X Immediate Repetition'], output_type=\"z_score\") # Don't need stat_type=\"F\" option?\n",
    "thresholded_map, threshold = threshold_stats_img(\n",
    "        z_map, \n",
    "        alpha = 0.001, \n",
    "        height_control = None,\n",
    "        two_sided = True)\n",
    "plot_stat_map(\n",
    "        thresholded_map, \n",
    "        threshold = threshold, \n",
    "        bg_img = mni152_template,\n",
    "        display_mode=\"ortho\",\n",
    "        black_bg = True,\n",
    "        cmap = 'hot', \n",
    "        title = 'Stimulus X Immediate Repetition',\n",
    "    )       \n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing particularly convincing for this last contrast: unlikely that anything would survive correction for multiple comparisons, but some might be reliable when using more focused Region-of-Interest (ROI) analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWE correction using non-parametric permutation testing\n",
    "\n",
    "Nilearn's FWE correction using the Bonferroni approach (`height_control='bonferroni'`) is applied to the number of voxels. However, this method is not well-suited for fMRI data because neuroimaging data typically exhibit spatially correlated data points, which violate the Bonferroni assumption of independent tests.\n",
    "\n",
    "As an alternative, neuroscientists have developed **Random Field Theory** (RFT), which accounts for spatial correlations by applying multiple comparison corrections that consider the smoothness of the data (it can be thought of as a Bonferroni correction based on the number of **'resels'** - RESolution ELements - rather than the raw number of voxels). However, it makes various parametric assumptions about the data, including a minimal smoothness in terms of voxels, that can be violated. More practically, it is not implemented in Nilearn (to our knowledge).\n",
    "\n",
    "Another alternative, which is implemented in Nilearn, is to use non-parametric inference, specifically permutation testing. This makes fewer assumptions when correcting for multiple comparisons (though note that is assumes exchangeability, which can also be violated if the error is nonspherical under the null; see [Rik's Stats](../03_Statistics/nb03_general_stats.ipynb)). The only downside is that permutation testing can take a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = non_parametric_inference(\n",
    "    effect_files,\n",
    "    design_matrix = design_matrix,\n",
    "    second_level_contrast = contrasts['FacesScrambled'],\n",
    "    n_perm = 1000, # ideally at least 10000\n",
    "    two_sided_test = False,\n",
    "    n_jobs = -1, # Use all available cores\n",
    "    threshold = 0.001 # cluster level threshold; enables cluster-level inference\n",
    ")\n",
    "\n",
    "# Print the keys of the output dictionary\n",
    "print(out_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is multiple images (maps), organised in a dictionary. \n",
    "* Voxel-level inference\n",
    "  * **t**: t-statistics\n",
    "  * **logp_max_t**: Negative log10 family-wise error rate-corrected p-values corrected based on the distribution of maximum t-statistics from permutations.\n",
    "* Cluster-level inference\n",
    "  * **size**: Cluster size values associated with the significance test \n",
    "  * **logp_max_size**: Negative log10 family-wise error rate-corrected p-values corrected based on the distribution of maximum cluster sizes from permutations.\n",
    "  * **mass**: Cluster mass values associated with the significance test \n",
    "  * **logp_max_mass**: Negative log10 family-wise error rate-corrected p-values corrected based on the distribution of maximum cluster masses from permutations. \n",
    "\n",
    "**We will focus only on the voxel-level inference.**\n",
    "\n",
    "To report the FWE-corrected maps, we could display the *logp_max_t*; however, these values can be difficult to interpret if you're not familiar with them. It might be better to plot and report a t-map, masked to exclude the voxels that did not survive the FWE correction.\n",
    "\n",
    "Let's create a new image displaying t-values for the voxels with a p-value < 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "masked = out_dict['logp_max_t'].get_fdata() > -np.log10(alpha)\n",
    "masked_t_map = out_dict['t'].get_fdata() * masked\n",
    "\n",
    "# save the masked t-map as a nifti image\n",
    "masked_t_map_img = nib.Nifti1Image(masked_t_map, out_dict['t'].affine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the smallest t-value that is above the threshold (for the colorbar; the maps themselves are thresholded already)\n",
    "threshold_fwe = masked_t_map[masked_t_map > 0].min()\n",
    "print('FWE (perm.) p<.05 threshold: %.3f' % threshold_fwe)\n",
    "\n",
    "plot_stat_map(\n",
    "    masked_t_map_img, \n",
    "    threshold = threshold_fwe,       \n",
    "    display_mode = 'ortho',\n",
    "    black_bg = True,\n",
    "    bg_img = mni152_template,\n",
    "    cmap = 'hot',\n",
    "    title = f\"Faces > Scrambled (FWE p <.{alpha})\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we observe that the non-parametric FWE correction is slightly less conservative than the Bonferroni correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the thresholded image for later use in our [Region of Interest (ROI) tutorial](nb06_ROI_analysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.save(masked_t_map_img, os.path.join(fmri_group_dir, 'FacesScrambled_fwe_t-map.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary results\n",
    "\n",
    "### Using atlasreader package\n",
    "\n",
    "We can use ['atlasreader'](https://github.com/miykael/atlasreader) package to get summary results (peak table, cluster table, .png images of each cluster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and save atlasreader output\n",
    "outdir = os.path.join(fmri_results_dir, 'group-level', 'permutation', 'FacesScrambled')\n",
    "\n",
    "create_output(\n",
    "    masked_t_map_img, \n",
    "    cluster_extent = 20, \n",
    "    voxel_thresh = threshold_fwe,\n",
    "    direction = 'pos',\n",
    "    outdir = outdir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the peak table\n",
    "peaks = glob.glob(os.path.join(outdir, '*_peaks.csv'))\n",
    "display(pd.read_csv(peaks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the cluster table\n",
    "clusters = glob.glob(os.path.join(outdir, '*_clusters.csv'))\n",
    "display(pd.read_csv(clusters[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some more plotting options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5 peaks' x values\n",
    "x = pd.read_csv(peaks[0])['peak_x'][:5]\n",
    "# sort the x values\n",
    "x = x.sort_values()\n",
    "\n",
    "# plot these peaks\n",
    "plot_stat_map(\n",
    "    masked_t_map_img, \n",
    "    threshold = threshold_fwe,       \n",
    "    display_mode = 'x',\n",
    "    cut_coords = x,\n",
    "    black_bg = True,\n",
    "    bg_img = mni152_template,\n",
    "    cmap = 'hot',\n",
    "    title = f\"Faces > Scrambled (FWE p <.{alpha})\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at a 3D brain using `plotly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = view_img_on_surf(masked_t_map_img, \n",
    "    threshold = threshold_fwe)\n",
    "#view.open_in_browser()\n",
    "view.resize(1600, 800)\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use, for example, FSLeyes to plot and explore the result maps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nilearn's report\n",
    "\n",
    "Nilearn has a built-in report generator that can create reports for all defined contrasts. However, a limitation is that it cannot generate reports for results obtained using non-parametric inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_report = second_level_model.generate_report(\n",
    "  contrasts, \n",
    "  title = \"Results of the second-level analysis\", \n",
    "  bg_img = mni152_template, \n",
    "  alpha = 0.001, \n",
    "  cluster_threshold = 20, \n",
    "  height_control = 'fpr', \n",
    "  min_distance = 8.0, \n",
    "  plot_type = 'slice', \n",
    "  display_mode = 'x', \n",
    "  report_dims = (1600, 800))\n",
    "\n",
    "second_level_report.open_in_browser()\n",
    "\n",
    "#second_level_report.save_as_html(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† EXERCISE\n",
    "\n",
    "Perform a one-sample t-test on the first-level results of *Faces > Scrambled* contrast. How do the results compare to our ANOVA model approach above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the first-level effect maps for the contrast\n",
    "\n",
    "\n",
    "# Display subject level maps\n",
    "\n",
    "\n",
    "# Create a design matrix \n",
    "\n",
    "\n",
    "# Specify and fit the second-level model\n",
    "\n",
    "\n",
    "# Compute the contrast and get the z-map\n",
    "\n",
    "\n",
    "# Threshold the z-map\n",
    "\n",
    "\n",
    "# Display the thresholded map\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "337px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "874.85px",
    "left": "2183px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
