{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTI Probabilistic Tractography\n",
    "\n",
    "In the previous tutorial we covered a simple example of determinisc tractography. The approach described is fast and very straightforward, however there are several disadvantages. In particular, deterministic DTI tractography can be overly simplistic, as it does not account for crossing, diverging, or converging fibers within a voxel. It is also highly sensitive to noise and partial volume effects, which can lead to erroneous tracking results.\n",
    "\n",
    "Probabilistic tractography models the uncertainty in fiber direction estimation and generates multiple possible pathways for each seed point, effectively sampling from a distribution of possible directions. The output is a distribution of possible pathways from each seed point, resulting in a density map or connectivity map that represents the probability of connection between regions. This approach can better capture the complexity of the brain's white matter, including crossing fibers and regions with uncertain fiber direction. It is also more robust to noise and partial volume effects. \n",
    "\n",
    "However, probabilistic tractography is computationally more intensive than deterministic tractography and the results can be harder to interpret due to the probabilistic nature of the output. This technique will also generate a high rate of false-positive connections, requiring careful analysis, including thresholding.\n",
    "\n",
    "In this tutorial we will cover how to perform probabilistic fibre tracking from the orientation distribution estimated from DTI, and it will include examples from https://workshop.dipy.org/documentation/1.7.0/examples_built/. \n",
    "\n",
    "We will start by loading the pre-processed diffusion MRI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load general modules\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "#load dipy modules\n",
    "from dipy.core.gradients import gradient_table\n",
    "from dipy.io.gradients import read_bvals_bvecs\n",
    "from dipy.io.image import load_nifti, save_nifti\n",
    "\n",
    "#load modules for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#define the paths to the data\n",
    "scripts_dir = os.getcwd()\n",
    "bids_dir = \"../data/bids\"\n",
    "out_dir = \"../data/derivatives\"\n",
    "\n",
    "#subject code to be used in this example\n",
    "sub='01'\n",
    "\n",
    "#load the pre-processed data\n",
    "data_preproc, affine, data_preproc_img = load_nifti(f\"{out_dir}/preprocessing/sub-{sub}/eddy_unwarped_images.nii.gz\", return_img=True)\n",
    "\n",
    "#load the bvals and bvecs\n",
    "bvals, bvecs = read_bvals_bvecs(f\"{bids_dir}/sub-{sub}/dwi/sub-{sub}_acq-AP_dwi.bval\", f\"{out_dir}/preprocessing/sub-{sub}/eddy_unwarped_images.eddy_rotated_bvecs\")\n",
    "\n",
    "#select data for DTI model fitting\n",
    "#create a mask bo b-values less than 1300 s/mm2\n",
    "bval_mask = bvals < 1300\n",
    "\n",
    "#select the data for DTI model fitting\n",
    "data_for_dti = data_preproc[..., bval_mask]\n",
    "\n",
    "bvals_for_dti = bvals[bval_mask]\n",
    "bvecs_for_dti = bvecs[bval_mask, :]\n",
    "\n",
    "#load the gradient table\n",
    "gtab_for_dti = gradient_table(bvals_for_dti, bvecs_for_dti)\n",
    "\n",
    "\n",
    "#load the binary brain mask\n",
    "brain_mask, affine_mask = load_nifti(f\"{out_dir}/preprocessing/sub-{sub}/hifi_nodif_brain_mask.nii.gz\")\n",
    "masked_data = data_for_dti * brain_mask[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Fit the DTI model and generate the orientation distribution function\n",
    "\n",
    "For this step we need to generate the orientation distribution function (ODF) from the DTI model. Firstly we fit the DTI model to the data, and then we calculate the ODF using the dtifit atribute odf. For this we need to input directions evenly sampling a 3D sphere, which can be loaded from dipy.data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the TensorModel from dipy\n",
    "from dipy.reconst.dti import TensorModel\n",
    "from dipy.data import get_sphere\n",
    "\n",
    "#fit the DTI model\n",
    "dtimodel = TensorModel(gtab_for_dti)\n",
    "dtifit = dtimodel.fit(data_for_dti, mask=brain_mask) \n",
    "\n",
    "#load the directions evenly distributed on a sphere\n",
    "sphere = get_sphere('symmetric724')\n",
    "\n",
    "#generate the ODF\n",
    "tensor_odfs = dtifit.odf(sphere)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quality assurance, we can visualise the ODF for each voxel using the fury package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving illustration as tensor_odfs.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cognestic/miniconda3/envs/mri/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577: UserWarning: We'll no longer accept the way you call the record function in future versions of FURY.\n",
      "\n",
      "Here's how to call the Function record: record(scene='value', cam_pos='value', cam_focal='value', cam_view='value', out_path='value', path_numbering='value', n_frames='value', az_ang='value', magnification='value', size='value', reset_camera='value', screen_clip='value', stereo='value', verbose='value')\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from dipy.viz import window, actor\n",
    "\n",
    "scene = window.Scene()\n",
    "\n",
    "odf_actor = actor.odf_slicer(tensor_odfs, sphere=sphere, scale=0.5,\n",
    "                             colormap='plasma')\n",
    "scene.add(odf_actor)\n",
    "print('Saving illustration as tensor_odfs.png')\n",
    "window.record(scene, n_frames=1, out_path=f\"{out_dir}/DTI/sub-{sub}/tensor_odfs.png\", size=(600, 600))\n",
    "window.show(scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2 & 3: Define the seeds and stopping criteria\n",
    "\n",
    "We will use the same seed mask and stopping criteria as for the deterministic tracking example. We seed the voxels in a corpus callosum sagittal slice with a grid density of 2 × 2 × 2. Streamlines will stop propagating when entering voxels with FA values lower than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking import utils\n",
    "from dipy.tracking.stopping_criterion import ThresholdStoppingCriterion\n",
    "\n",
    "#load corpus callosum mask\n",
    "cc_mask, affine_cc = load_nifti(f\"{out_dir}/DTI/sub-{sub}/CC_mask.nii.gz\")\n",
    "\n",
    "#create the seed mask\n",
    "seed_mask = (cc_mask == 1)\n",
    "seeds = utils.seeds_from_mask(seed_mask, affine_cc, density=[2, 2, 2])\n",
    "\n",
    "#calculate fa map and define stopping criterion\n",
    "fa = dtifit.fa\n",
    "stopping_criterion = ThresholdStoppingCriterion(fa, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Propagating the streamlines\n",
    "\n",
    "The probabilistic streamlines will be generated using LocalTracking, similarly to the deterministic tractography example. However, there is one additional step: we need to instantiate a \"ProbabilisticDirectionGetter\" class object in order to use DTI's ODFs for probabilistic tracking.\n",
    "\n",
    "Note: Input variable \"max_angle\" gives the maximum allowed angle between the incoming direction and the new direction in tractography propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.direction import ProbabilisticDirectionGetter\n",
    "\n",
    "#instanciate the probabilistic direction getter\n",
    "pmf = tensor_odfs.clip(min=0)\n",
    "prob_dg = ProbabilisticDirectionGetter.from_pmf(pmf, max_angle=30,\n",
    "                                                sphere=sphere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initalise the streamlines generator, create the streamlines and save them as a Trackvis file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.local_tracking import LocalTracking\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "from dipy.io.stateful_tractogram import Space, StatefulTractogram\n",
    "from dipy.io.streamline import save_trk\n",
    "\n",
    "#initialise the streamlines generator\n",
    "streamlines_generator = LocalTracking(prob_dg, stopping_criterion, seeds,\n",
    "                                      affine=affine, step_size=0.5)\n",
    "\n",
    "#generate the streamlines\n",
    "streamlines = Streamlines(streamlines_generator)\n",
    "\n",
    "#save the streamlines\n",
    "sft = StatefulTractogram(streamlines, data_preproc_img, Space.RASMM)\n",
    "save_trk(sft, f\"{out_dir}/DTI/sub-{sub}/dti_CC_probabilistic_tract.trk\", streamlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output can be visualised with the fury package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cognestic/miniconda3/envs/mri/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577: UserWarning: We'll no longer accept the way you call the line function in future versions of FURY.\n",
      "\n",
      "Here's how to call the Function line: line(lines_value, colors='value', opacity='value', linewidth='value', spline_subdiv='value', lod='value', lod_points='value', lod_points_size='value', lookup_colormap='value', depth_cue='value', fake_tube='value')\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from dipy.viz import window, actor, has_fury\n",
    "from dipy.viz import colormap\n",
    "\n",
    "if has_fury:\n",
    "    # Prepare the display objects.\n",
    "    color = colormap.line_colors(streamlines)\n",
    "\n",
    "    streamlines_actor = actor.line(streamlines,\n",
    "                                   colormap.line_colors(streamlines))\n",
    "\n",
    "    # Create the 3D display.\n",
    "    scene = window.Scene()\n",
    "    scene.add(streamlines_actor)\n",
    "\n",
    "    # Save still images for this static example. Or for interactivity use\n",
    "    window.record(scene, out_path=f\"{out_dir}/DTI/sub-{sub}/dti_tractogram_prob.png\", size=(800, 800))\n",
    "    window.show(scene)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
